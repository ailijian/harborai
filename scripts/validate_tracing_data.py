#!/usr/bin/env python3
"""
HarborAI è¿½è¸ªç³»ç»Ÿæ•°æ®éªŒè¯è„šæœ¬

æ­¤è„šæœ¬è´Ÿè´£éªŒè¯è¿½è¸ªç³»ç»Ÿçš„æ•°æ®å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®åº“è¡¨ç»“æ„éªŒè¯
- æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
- æ€§èƒ½æŒ‡æ ‡éªŒè¯
- è¿½è¸ªé“¾è·¯å®Œæ•´æ€§éªŒè¯
- æ•°æ®è´¨é‡æŠ¥å‘Šç”Ÿæˆ

åŠŸèƒ½ç‰¹æ€§ï¼š
- å…¨é¢çš„æ•°æ®éªŒè¯
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- æ•°æ®è´¨é‡è¯„åˆ†
- è¯¦ç»†çš„éªŒè¯æŠ¥å‘Š
- è‡ªåŠ¨ä¿®å¤å»ºè®®

ä½œè€…: HarborAIå›¢é˜Ÿ
åˆ›å»ºæ—¶é—´: 2025-01-15
ç‰ˆæœ¬: v1.0.0
"""

import asyncio
import asyncpg
import argparse
import json
import sys
import os
import time
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple, Any, Set
from dataclasses import dataclass, asdict
from pathlib import Path
from collections import defaultdict
import statistics

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from harborai.utils.exceptions import DatabaseError, ValidationError


@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    category: str
    test_name: str
    status: str  # "pass", "fail", "warning"
    message: str
    details: Dict[str, Any] = None
    score: float = 0.0  # 0-100åˆ†
    
    def __post_init__(self):
        if self.details is None:
            self.details = {}


@dataclass
class DataQualityMetrics:
    """æ•°æ®è´¨é‡æŒ‡æ ‡"""
    total_records: int = 0
    valid_records: int = 0
    invalid_records: int = 0
    duplicate_records: int = 0
    orphaned_records: int = 0
    data_completeness: float = 0.0
    data_accuracy: float = 0.0
    data_consistency: float = 0.0
    overall_score: float = 0.0


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    avg_query_time_ms: float = 0.0
    max_query_time_ms: float = 0.0
    min_query_time_ms: float = 0.0
    p95_query_time_ms: float = 0.0
    throughput_qps: float = 0.0
    index_efficiency: float = 0.0
    storage_efficiency: float = 0.0


@dataclass
class ValidationReport:
    """éªŒè¯æŠ¥å‘Š"""
    timestamp: datetime
    database_info: Dict[str, Any]
    schema_validation: List[ValidationResult]
    data_validation: List[ValidationResult]
    performance_validation: List[ValidationResult]
    data_quality: DataQualityMetrics
    performance_metrics: PerformanceMetrics
    overall_score: float = 0.0
    recommendations: List[str] = None
    
    def __post_init__(self):
        if self.recommendations is None:
            self.recommendations = []


class TracingDataValidator:
    """è¿½è¸ªæ•°æ®éªŒè¯å™¨"""
    
    def __init__(self, db_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–éªŒè¯å™¨
        
        Args:
            db_config: æ•°æ®åº“è¿æ¥é…ç½®
        """
        self.db_config = db_config
        self.connection_pool: Optional[asyncpg.Pool] = None
        self.validation_results: List[ValidationResult] = []
        
    async def connect(self) -> None:
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        try:
            self.connection_pool = await asyncpg.create_pool(
                host=self.db_config["host"],
                port=self.db_config["port"],
                database=self.db_config["database"],
                user=self.db_config["user"],
                password=self.db_config["password"],
                min_size=2,
                max_size=10,
                command_timeout=60
            )
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}")
        except Exception as e:
            raise DatabaseError(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
    
    async def disconnect(self) -> None:
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection_pool:
            await self.connection_pool.close()
            print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    async def validate_schema(self) -> List[ValidationResult]:
        """éªŒè¯æ•°æ®åº“æ¨¡å¼"""
        results = []
        
        if not self.connection_pool:
            raise DatabaseError("æ•°æ®åº“è¿æ¥æœªå»ºç«‹")
        
        async with self.connection_pool.acquire() as connection:
            # 1. éªŒè¯å¿…éœ€çš„è¡¨å­˜åœ¨
            required_tables = ["migration_history", "tracing_info"]
            
            for table_name in required_tables:
                exists = await connection.fetchval("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = $1
                    )
                """, table_name)
                
                if exists:
                    results.append(ValidationResult(
                        category="schema",
                        test_name=f"table_{table_name}_exists",
                        status="pass",
                        message=f"è¡¨ {table_name} å­˜åœ¨",
                        score=100.0
                    ))
                else:
                    results.append(ValidationResult(
                        category="schema",
                        test_name=f"table_{table_name}_exists",
                        status="fail",
                        message=f"è¡¨ {table_name} ä¸å­˜åœ¨",
                        score=0.0
                    ))
            
            # 2. éªŒè¯tracing_infoè¡¨ç»“æ„
            if await self._table_exists(connection, "tracing_info"):
                columns = await connection.fetch("""
                    SELECT column_name, data_type, is_nullable, column_default
                    FROM information_schema.columns
                    WHERE table_schema = 'public' AND table_name = 'tracing_info'
                    ORDER BY ordinal_position
                """)
                
                required_columns = {
                    "id": "bigint",
                    "hb_trace_id": "character varying",
                    "otel_trace_id": "character varying",
                    "span_id": "character varying",
                    "operation_name": "character varying",
                    "service_name": "character varying",
                    "start_time": "timestamp with time zone",
                    "status": "character varying",
                    "tags": "jsonb",
                    "logs": "jsonb"
                }
                
                existing_columns = {col["column_name"]: col["data_type"] for col in columns}
                
                for col_name, expected_type in required_columns.items():
                    if col_name in existing_columns:
                        actual_type = existing_columns[col_name]
                        if expected_type in actual_type or actual_type in expected_type:
                            results.append(ValidationResult(
                                category="schema",
                                test_name=f"column_{col_name}_type",
                                status="pass",
                                message=f"åˆ— {col_name} ç±»å‹æ­£ç¡® ({actual_type})",
                                score=100.0
                            ))
                        else:
                            results.append(ValidationResult(
                                category="schema",
                                test_name=f"column_{col_name}_type",
                                status="fail",
                                message=f"åˆ— {col_name} ç±»å‹é”™è¯¯: æœŸæœ› {expected_type}, å®é™… {actual_type}",
                                score=0.0
                            ))
                    else:
                        results.append(ValidationResult(
                            category="schema",
                            test_name=f"column_{col_name}_exists",
                            status="fail",
                            message=f"ç¼ºå°‘å¿…éœ€åˆ— {col_name}",
                            score=0.0
                        ))
            
            # 3. éªŒè¯ç´¢å¼•
            indexes = await connection.fetch("""
                SELECT indexname, tablename
                FROM pg_indexes
                WHERE schemaname = 'public' AND tablename = 'tracing_info'
            """)
            
            required_indexes = [
                "idx_tracing_info_hb_trace_id",
                "idx_tracing_info_otel_trace_id",
                "idx_tracing_info_start_time",
                "idx_tracing_info_status"
            ]
            
            existing_indexes = [idx["indexname"] for idx in indexes]
            
            for idx_name in required_indexes:
                if idx_name in existing_indexes:
                    results.append(ValidationResult(
                        category="schema",
                        test_name=f"index_{idx_name}_exists",
                        status="pass",
                        message=f"ç´¢å¼• {idx_name} å­˜åœ¨",
                        score=100.0
                    ))
                else:
                    results.append(ValidationResult(
                        category="schema",
                        test_name=f"index_{idx_name}_exists",
                        status="warning",
                        message=f"å»ºè®®çš„ç´¢å¼• {idx_name} ä¸å­˜åœ¨",
                        score=70.0
                    ))
            
            # 4. éªŒè¯çº¦æŸ
            constraints = await connection.fetch("""
                SELECT constraint_name, constraint_type
                FROM information_schema.table_constraints
                WHERE table_schema = 'public' AND table_name = 'tracing_info'
            """)
            
            constraint_types = [c["constraint_type"] for c in constraints]
            
            if "PRIMARY KEY" in constraint_types:
                results.append(ValidationResult(
                    category="schema",
                    test_name="primary_key_exists",
                    status="pass",
                    message="ä¸»é”®çº¦æŸå­˜åœ¨",
                    score=100.0
                ))
            else:
                results.append(ValidationResult(
                    category="schema",
                    test_name="primary_key_exists",
                    status="fail",
                    message="ç¼ºå°‘ä¸»é”®çº¦æŸ",
                    score=0.0
                ))
            
            # 5. éªŒè¯è§†å›¾
            views = await connection.fetch("""
                SELECT table_name
                FROM information_schema.views
                WHERE table_schema = 'public' AND table_name LIKE 'v_%'
            """)
            
            expected_views = ["v_active_traces", "v_tracing_stats", "v_error_traces"]
            existing_views = [v["table_name"] for v in views]
            
            for view_name in expected_views:
                if view_name in existing_views:
                    results.append(ValidationResult(
                        category="schema",
                        test_name=f"view_{view_name}_exists",
                        status="pass",
                        message=f"è§†å›¾ {view_name} å­˜åœ¨",
                        score=100.0
                    ))
                else:
                    results.append(ValidationResult(
                        category="schema",
                        test_name=f"view_{view_name}_exists",
                        status="warning",
                        message=f"å»ºè®®çš„è§†å›¾ {view_name} ä¸å­˜åœ¨",
                        score=80.0
                    ))
        
        return results
    
    async def validate_data_integrity(self) -> List[ValidationResult]:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        results = []
        
        if not self.connection_pool:
            raise DatabaseError("æ•°æ®åº“è¿æ¥æœªå»ºç«‹")
        
        async with self.connection_pool.acquire() as connection:
            # 1. æ£€æŸ¥åŸºæœ¬æ•°æ®ç»Ÿè®¡
            total_records = await connection.fetchval(
                "SELECT COUNT(*) FROM tracing_info"
            )
            
            results.append(ValidationResult(
                category="data",
                test_name="total_records_count",
                status="pass" if total_records >= 0 else "fail",
                message=f"æ€»è®°å½•æ•°: {total_records}",
                details={"count": total_records},
                score=100.0 if total_records >= 0 else 0.0
            ))
            
            if total_records == 0:
                results.append(ValidationResult(
                    category="data",
                    test_name="data_exists",
                    status="warning",
                    message="æ•°æ®åº“ä¸­æ²¡æœ‰è¿½è¸ªæ•°æ®",
                    score=50.0
                ))
                return results
            
            # 2. æ£€æŸ¥å¿…éœ€å­—æ®µçš„å®Œæ•´æ€§
            required_fields = ["hb_trace_id", "otel_trace_id", "operation_name", "service_name", "start_time", "status"]
            
            for field in required_fields:
                null_count = await connection.fetchval(
                    f"SELECT COUNT(*) FROM tracing_info WHERE {field} IS NULL"
                )
                
                completeness = ((total_records - null_count) / total_records * 100) if total_records > 0 else 0
                
                if completeness == 100:
                    status = "pass"
                    score = 100.0
                elif completeness >= 95:
                    status = "warning"
                    score = 80.0
                else:
                    status = "fail"
                    score = 50.0
                
                results.append(ValidationResult(
                    category="data",
                    test_name=f"field_{field}_completeness",
                    status=status,
                    message=f"å­—æ®µ {field} å®Œæ•´æ€§: {completeness:.1f}%",
                    details={"completeness": completeness, "null_count": null_count},
                    score=score
                ))
            
            # 3. æ£€æŸ¥æ•°æ®æ ¼å¼æœ‰æ•ˆæ€§
            # æ£€æŸ¥otel_trace_idæ ¼å¼ï¼ˆ32ä½åå…­è¿›åˆ¶ï¼‰
            invalid_otel_trace_ids = await connection.fetchval("""
                SELECT COUNT(*) FROM tracing_info 
                WHERE otel_trace_id !~ '^[0-9a-f]{32}$'
            """)
            
            otel_validity = ((total_records - invalid_otel_trace_ids) / total_records * 100) if total_records > 0 else 0
            
            results.append(ValidationResult(
                category="data",
                test_name="otel_trace_id_format",
                status="pass" if otel_validity == 100 else "fail",
                message=f"OpenTelemetry Trace ID æ ¼å¼æœ‰æ•ˆæ€§: {otel_validity:.1f}%",
                details={"invalid_count": invalid_otel_trace_ids},
                score=100.0 if otel_validity == 100 else 60.0
            ))
            
            # æ£€æŸ¥span_idæ ¼å¼ï¼ˆ16ä½åå…­è¿›åˆ¶ï¼‰
            invalid_span_ids = await connection.fetchval("""
                SELECT COUNT(*) FROM tracing_info 
                WHERE span_id !~ '^[0-9a-f]{16}$'
            """)
            
            span_validity = ((total_records - invalid_span_ids) / total_records * 100) if total_records > 0 else 0
            
            results.append(ValidationResult(
                category="data",
                test_name="span_id_format",
                status="pass" if span_validity == 100 else "fail",
                message=f"Span ID æ ¼å¼æœ‰æ•ˆæ€§: {span_validity:.1f}%",
                details={"invalid_count": invalid_span_ids},
                score=100.0 if span_validity == 100 else 60.0
            ))
            
            # 4. æ£€æŸ¥çŠ¶æ€å€¼æœ‰æ•ˆæ€§
            valid_statuses = ["pending", "active", "completed", "error"]
            invalid_statuses = await connection.fetchval("""
                SELECT COUNT(*) FROM tracing_info 
                WHERE status NOT IN ('pending', 'active', 'completed', 'error')
            """)
            
            status_validity = ((total_records - invalid_statuses) / total_records * 100) if total_records > 0 else 0
            
            results.append(ValidationResult(
                category="data",
                test_name="status_values_validity",
                status="pass" if status_validity == 100 else "fail",
                message=f"çŠ¶æ€å€¼æœ‰æ•ˆæ€§: {status_validity:.1f}%",
                details={"invalid_count": invalid_statuses, "valid_statuses": valid_statuses},
                score=100.0 if status_validity == 100 else 50.0
            ))
            
            # 5. æ£€æŸ¥æ—¶é—´é€»è¾‘ä¸€è‡´æ€§
            invalid_time_logic = await connection.fetchval("""
                SELECT COUNT(*) FROM tracing_info 
                WHERE end_time IS NOT NULL AND end_time < start_time
            """)
            
            time_consistency = ((total_records - invalid_time_logic) / total_records * 100) if total_records > 0 else 0
            
            results.append(ValidationResult(
                category="data",
                test_name="time_logic_consistency",
                status="pass" if time_consistency == 100 else "fail",
                message=f"æ—¶é—´é€»è¾‘ä¸€è‡´æ€§: {time_consistency:.1f}%",
                details={"invalid_count": invalid_time_logic},
                score=100.0 if time_consistency == 100 else 40.0
            ))
            
            # 6. æ£€æŸ¥é‡å¤è®°å½•
            duplicate_hb_trace_ids = await connection.fetchval("""
                SELECT COUNT(*) - COUNT(DISTINCT hb_trace_id) FROM tracing_info
            """)
            
            results.append(ValidationResult(
                category="data",
                test_name="duplicate_hb_trace_ids",
                status="pass" if duplicate_hb_trace_ids == 0 else "warning",
                message=f"é‡å¤çš„HB Trace ID: {duplicate_hb_trace_ids}",
                details={"duplicate_count": duplicate_hb_trace_ids},
                score=100.0 if duplicate_hb_trace_ids == 0 else 70.0
            ))
            
            # 7. æ£€æŸ¥JSONå­—æ®µæœ‰æ•ˆæ€§
            invalid_tags = await connection.fetchval("""
                SELECT COUNT(*) FROM tracing_info 
                WHERE tags IS NOT NULL AND NOT (tags::text ~ '^{.*}$')
            """)
            
            invalid_logs = await connection.fetchval("""
                SELECT COUNT(*) FROM tracing_info 
                WHERE logs IS NOT NULL AND NOT (logs::text ~ '^\\[.*\\]$')
            """)
            
            json_validity = ((total_records - invalid_tags - invalid_logs) / total_records * 100) if total_records > 0 else 0
            
            results.append(ValidationResult(
                category="data",
                test_name="json_fields_validity",
                status="pass" if json_validity == 100 else "fail",
                message=f"JSONå­—æ®µæœ‰æ•ˆæ€§: {json_validity:.1f}%",
                details={"invalid_tags": invalid_tags, "invalid_logs": invalid_logs},
                score=100.0 if json_validity == 100 else 60.0
            ))
            
            # 8. æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
            status_distribution = await connection.fetch("""
                SELECT status, COUNT(*) as count
                FROM tracing_info
                GROUP BY status
                ORDER BY count DESC
            """)
            
            status_dist = {row["status"]: row["count"] for row in status_distribution}
            
            results.append(ValidationResult(
                category="data",
                test_name="status_distribution",
                status="pass",
                message="çŠ¶æ€åˆ†å¸ƒç»Ÿè®¡",
                details={"distribution": status_dist},
                score=100.0
            ))
            
            # 9. æ£€æŸ¥æœ€è¿‘æ•°æ®æ´»è·ƒåº¦
            recent_records = await connection.fetchval("""
                SELECT COUNT(*) FROM tracing_info 
                WHERE created_at >= NOW() - INTERVAL '24 hours'
            """)
            
            activity_score = min(100.0, (recent_records / max(1, total_records * 0.1)) * 100)
            
            results.append(ValidationResult(
                category="data",
                test_name="recent_activity",
                status="pass" if recent_records > 0 else "warning",
                message=f"24å°æ—¶å†…æ–°å¢è®°å½•: {recent_records}",
                details={"recent_count": recent_records},
                score=activity_score
            ))
        
        return results
    
    async def validate_performance(self) -> Tuple[List[ValidationResult], PerformanceMetrics]:
        """éªŒè¯æ€§èƒ½æŒ‡æ ‡"""
        results = []
        metrics = PerformanceMetrics()
        
        if not self.connection_pool:
            raise DatabaseError("æ•°æ®åº“è¿æ¥æœªå»ºç«‹")
        
        # æ€§èƒ½æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            ("simple_count", "SELECT COUNT(*) FROM tracing_info"),
            ("hb_trace_id_lookup", "SELECT * FROM tracing_info WHERE hb_trace_id = 'test_trace_001' LIMIT 1"),
            ("status_filter", "SELECT COUNT(*) FROM tracing_info WHERE status = 'completed'"),
            ("time_range_query", "SELECT COUNT(*) FROM tracing_info WHERE start_time >= NOW() - INTERVAL '1 hour'"),
            ("json_tag_query", "SELECT COUNT(*) FROM tracing_info WHERE tags->>'model' = 'gpt-4'"),
            ("complex_aggregation", """
                SELECT service_name, operation_name, COUNT(*), AVG(duration_ms)
                FROM tracing_info 
                WHERE start_time >= NOW() - INTERVAL '24 hours'
                GROUP BY service_name, operation_name
            """)
        ]
        
        query_times = []
        
        async with self.connection_pool.acquire() as connection:
            for query_name, query_sql in test_queries:
                times = []
                
                # æ‰§è¡Œæ¯ä¸ªæŸ¥è¯¢3æ¬¡å–å¹³å‡å€¼
                for i in range(3):
                    start_time = time.time()
                    try:
                        await connection.fetch(query_sql)
                        end_time = time.time()
                        query_time_ms = (end_time - start_time) * 1000
                        times.append(query_time_ms)
                    except Exception as e:
                        results.append(ValidationResult(
                            category="performance",
                            test_name=f"query_{query_name}",
                            status="fail",
                            message=f"æŸ¥è¯¢ {query_name} æ‰§è¡Œå¤±è´¥: {e}",
                            score=0.0
                        ))
                        continue
                
                if times:
                    avg_time = statistics.mean(times)
                    query_times.extend(times)
                    
                    # æ€§èƒ½è¯„åˆ†ï¼ˆåŸºäºæŸ¥è¯¢æ—¶é—´ï¼‰
                    if avg_time < 10:  # 10msä»¥ä¸‹
                        score = 100.0
                        status = "pass"
                    elif avg_time < 50:  # 50msä»¥ä¸‹
                        score = 80.0
                        status = "pass"
                    elif avg_time < 200:  # 200msä»¥ä¸‹
                        score = 60.0
                        status = "warning"
                    else:
                        score = 40.0
                        status = "fail"
                    
                    results.append(ValidationResult(
                        category="performance",
                        test_name=f"query_{query_name}_performance",
                        status=status,
                        message=f"æŸ¥è¯¢ {query_name} å¹³å‡è€—æ—¶: {avg_time:.2f}ms",
                        details={"avg_time_ms": avg_time, "times": times},
                        score=score
                    ))
            
            # è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡
            if query_times:
                metrics.avg_query_time_ms = statistics.mean(query_times)
                metrics.max_query_time_ms = max(query_times)
                metrics.min_query_time_ms = min(query_times)
                
                if len(query_times) > 1:
                    sorted_times = sorted(query_times)
                    p95_index = int(len(sorted_times) * 0.95)
                    metrics.p95_query_time_ms = sorted_times[p95_index]
                
                # ä¼°ç®—ååé‡ï¼ˆåŸºäºå¹³å‡æŸ¥è¯¢æ—¶é—´ï¼‰
                if metrics.avg_query_time_ms > 0:
                    metrics.throughput_qps = 1000 / metrics.avg_query_time_ms
            
            # æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
            index_usage = await connection.fetch("""
                SELECT 
                    schemaname,
                    tablename,
                    indexname,
                    idx_tup_read,
                    idx_tup_fetch
                FROM pg_stat_user_indexes 
                WHERE tablename = 'tracing_info'
            """)
            
            total_index_reads = sum(row["idx_tup_read"] or 0 for row in index_usage)
            
            if total_index_reads > 0:
                metrics.index_efficiency = 100.0
                results.append(ValidationResult(
                    category="performance",
                    test_name="index_usage",
                    status="pass",
                    message=f"ç´¢å¼•ä½¿ç”¨è‰¯å¥½ï¼Œæ€»è¯»å–æ¬¡æ•°: {total_index_reads}",
                    details={"total_reads": total_index_reads},
                    score=100.0
                ))
            else:
                metrics.index_efficiency = 50.0
                results.append(ValidationResult(
                    category="performance",
                    test_name="index_usage",
                    status="warning",
                    message="ç´¢å¼•ä½¿ç”¨æƒ…å†µä¸æ˜ç¡®æˆ–è¾ƒå°‘",
                    score=50.0
                ))
            
            # æ£€æŸ¥è¡¨å¤§å°å’Œå­˜å‚¨æ•ˆç‡
            table_size = await connection.fetchval("""
                SELECT pg_total_relation_size('tracing_info')
            """)
            
            row_count = await connection.fetchval("SELECT COUNT(*) FROM tracing_info")
            
            if row_count > 0:
                avg_row_size = table_size / row_count
                
                # è¯„ä¼°å­˜å‚¨æ•ˆç‡ï¼ˆåŸºäºå¹³å‡è¡Œå¤§å°ï¼‰
                if avg_row_size < 1024:  # 1KBä»¥ä¸‹
                    storage_score = 100.0
                    storage_status = "pass"
                elif avg_row_size < 4096:  # 4KBä»¥ä¸‹
                    storage_score = 80.0
                    storage_status = "pass"
                elif avg_row_size < 10240:  # 10KBä»¥ä¸‹
                    storage_score = 60.0
                    storage_status = "warning"
                else:
                    storage_score = 40.0
                    storage_status = "warning"
                
                metrics.storage_efficiency = storage_score
                
                results.append(ValidationResult(
                    category="performance",
                    test_name="storage_efficiency",
                    status=storage_status,
                    message=f"å­˜å‚¨æ•ˆç‡: å¹³å‡è¡Œå¤§å° {avg_row_size:.0f} å­—èŠ‚",
                    details={
                        "table_size_bytes": table_size,
                        "row_count": row_count,
                        "avg_row_size_bytes": avg_row_size
                    },
                    score=storage_score
                ))
        
        return results, metrics
    
    async def calculate_data_quality_metrics(self) -> DataQualityMetrics:
        """è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡"""
        metrics = DataQualityMetrics()
        
        if not self.connection_pool:
            raise DatabaseError("æ•°æ®åº“è¿æ¥æœªå»ºç«‹")
        
        async with self.connection_pool.acquire() as connection:
            # æ€»è®°å½•æ•°
            metrics.total_records = await connection.fetchval(
                "SELECT COUNT(*) FROM tracing_info"
            )
            
            if metrics.total_records == 0:
                return metrics
            
            # æœ‰æ•ˆè®°å½•æ•°ï¼ˆæ‰€æœ‰å¿…éœ€å­—æ®µéƒ½ä¸ä¸ºç©ºï¼‰
            metrics.valid_records = await connection.fetchval("""
                SELECT COUNT(*) FROM tracing_info 
                WHERE hb_trace_id IS NOT NULL 
                  AND otel_trace_id IS NOT NULL 
                  AND operation_name IS NOT NULL 
                  AND service_name IS NOT NULL 
                  AND start_time IS NOT NULL 
                  AND status IS NOT NULL
            """)
            
            metrics.invalid_records = metrics.total_records - metrics.valid_records
            
            # é‡å¤è®°å½•æ•°
            metrics.duplicate_records = await connection.fetchval("""
                SELECT COUNT(*) - COUNT(DISTINCT hb_trace_id) FROM tracing_info
            """)
            
            # å­¤ç«‹è®°å½•æ•°ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºçŠ¶æ€å¼‚å¸¸çš„è®°å½•ï¼‰
            metrics.orphaned_records = await connection.fetchval("""
                SELECT COUNT(*) FROM tracing_info 
                WHERE status NOT IN ('pending', 'active', 'completed', 'error')
            """)
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            if metrics.total_records > 0:
                metrics.data_completeness = (metrics.valid_records / metrics.total_records) * 100
                
                # æ•°æ®å‡†ç¡®æ€§ï¼ˆåŸºäºæ ¼å¼éªŒè¯ï¼‰
                format_valid_records = await connection.fetchval("""
                    SELECT COUNT(*) FROM tracing_info 
                    WHERE otel_trace_id ~ '^[0-9a-f]{32}$'
                      AND span_id ~ '^[0-9a-f]{16}$'
                      AND status IN ('pending', 'active', 'completed', 'error')
                      AND (end_time IS NULL OR end_time >= start_time)
                """)
                
                metrics.data_accuracy = (format_valid_records / metrics.total_records) * 100
                
                # æ•°æ®ä¸€è‡´æ€§ï¼ˆåŸºäºé€»è¾‘éªŒè¯ï¼‰
                consistent_records = await connection.fetchval("""
                    SELECT COUNT(*) FROM tracing_info 
                    WHERE (status = 'completed' AND end_time IS NOT NULL AND duration_ms IS NOT NULL)
                       OR (status IN ('pending', 'active') AND end_time IS NULL)
                       OR (status = 'error')
                """)
                
                metrics.data_consistency = (consistent_records / metrics.total_records) * 100
                
                # æ€»ä½“è¯„åˆ†
                metrics.overall_score = (
                    metrics.data_completeness * 0.4 +
                    metrics.data_accuracy * 0.3 +
                    metrics.data_consistency * 0.3
                )
        
        return metrics
    
    async def generate_recommendations(self, validation_results: List[ValidationResult], 
                                     data_quality: DataQualityMetrics,
                                     performance_metrics: PerformanceMetrics) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºéªŒè¯ç»“æœçš„å»ºè®®
        failed_tests = [r for r in validation_results if r.status == "fail"]
        warning_tests = [r for r in validation_results if r.status == "warning"]
        
        if failed_tests:
            recommendations.append("ğŸ”´ å‘ç°ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³å¤„ç†:")
            for test in failed_tests:
                recommendations.append(f"   - {test.message}")
        
        if warning_tests:
            recommendations.append("ğŸŸ¡ å‘ç°æ½œåœ¨é—®é¢˜ï¼Œå»ºè®®ä¼˜åŒ–:")
            for test in warning_tests:
                recommendations.append(f"   - {test.message}")
        
        # åŸºäºæ•°æ®è´¨é‡çš„å»ºè®®
        if data_quality.data_completeness < 95:
            recommendations.append(f"ğŸ“Š æ•°æ®å®Œæ•´æ€§è¾ƒä½ ({data_quality.data_completeness:.1f}%)ï¼Œå»ºè®®:")
            recommendations.append("   - æ£€æŸ¥æ•°æ®å†™å…¥é€»è¾‘ï¼Œç¡®ä¿å¿…éœ€å­—æ®µä¸ä¸ºç©º")
            recommendations.append("   - æ·»åŠ æ•°æ®éªŒè¯è§„åˆ™")
        
        if data_quality.data_accuracy < 90:
            recommendations.append(f"ğŸ¯ æ•°æ®å‡†ç¡®æ€§è¾ƒä½ ({data_quality.data_accuracy:.1f}%)ï¼Œå»ºè®®:")
            recommendations.append("   - éªŒè¯æ•°æ®æ ¼å¼è§„åˆ™")
            recommendations.append("   - æ·»åŠ è¾“å…¥æ•°æ®æ ¡éªŒ")
        
        if data_quality.duplicate_records > 0:
            recommendations.append(f"ğŸ”„ å‘ç°é‡å¤è®°å½• ({data_quality.duplicate_records} æ¡)ï¼Œå»ºè®®:")
            recommendations.append("   - æ£€æŸ¥æ•°æ®å†™å…¥é€»è¾‘ï¼Œé¿å…é‡å¤æ’å…¥")
            recommendations.append("   - è€ƒè™‘æ·»åŠ å”¯ä¸€çº¦æŸ")
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡çš„å»ºè®®
        if performance_metrics.avg_query_time_ms > 100:
            recommendations.append(f"âš¡ æŸ¥è¯¢æ€§èƒ½è¾ƒæ…¢ (å¹³å‡ {performance_metrics.avg_query_time_ms:.1f}ms)ï¼Œå»ºè®®:")
            recommendations.append("   - æ£€æŸ¥å¹¶ä¼˜åŒ–ç´¢å¼•")
            recommendations.append("   - è€ƒè™‘æŸ¥è¯¢ä¼˜åŒ–")
            recommendations.append("   - è¯„ä¼°æ˜¯å¦éœ€è¦åˆ†åŒº")
        
        if performance_metrics.storage_efficiency < 80:
            recommendations.append(f"ğŸ’¾ å­˜å‚¨æ•ˆç‡è¾ƒä½ ({performance_metrics.storage_efficiency:.1f}%)ï¼Œå»ºè®®:")
            recommendations.append("   - æ£€æŸ¥æ•°æ®ç±»å‹æ˜¯å¦åˆé€‚")
            recommendations.append("   - è€ƒè™‘æ•°æ®å‹ç¼©")
            recommendations.append("   - æ¸…ç†å†å²æ•°æ®")
        
        # é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("âœ… ç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼Œå»ºè®®:")
            recommendations.append("   - å®šæœŸæ‰§è¡Œæ•°æ®éªŒè¯")
            recommendations.append("   - ç›‘æ§æ€§èƒ½æŒ‡æ ‡")
            recommendations.append("   - ä¿æŒæ•°æ®å¤‡ä»½")
        
        return recommendations
    
    async def _table_exists(self, connection, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
        return await connection.fetchval("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = $1
            )
        """, table_name)
    
    async def generate_full_report(self) -> ValidationReport:
        """ç”Ÿæˆå®Œæ•´çš„éªŒè¯æŠ¥å‘Š"""
        print("ğŸ” å¼€å§‹æ•°æ®éªŒè¯...")
        
        # è·å–æ•°æ®åº“ä¿¡æ¯
        async with self.connection_pool.acquire() as connection:
            db_version = await connection.fetchval("SELECT version()")
            db_size = await connection.fetchval("""
                SELECT pg_size_pretty(pg_database_size(current_database()))
            """)
            
            database_info = {
                "version": db_version,
                "size": db_size,
                "host": self.db_config["host"],
                "port": self.db_config["port"],
                "database": self.db_config["database"]
            }
        
        # æ‰§è¡Œå„é¡¹éªŒè¯
        print("ğŸ“‹ éªŒè¯æ•°æ®åº“æ¨¡å¼...")
        schema_results = await self.validate_schema()
        
        print("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        data_results = await self.validate_data_integrity()
        
        print("âš¡ éªŒè¯æ€§èƒ½æŒ‡æ ‡...")
        performance_results, performance_metrics = await self.validate_performance()
        
        print("ğŸ“Š è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡...")
        data_quality = await self.calculate_data_quality_metrics()
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        all_results = schema_results + data_results + performance_results
        if all_results:
            overall_score = sum(r.score for r in all_results) / len(all_results)
        else:
            overall_score = 0.0
        
        # ç”Ÿæˆå»ºè®®
        print("ğŸ’¡ ç”Ÿæˆæ”¹è¿›å»ºè®®...")
        recommendations = await self.generate_recommendations(
            all_results, data_quality, performance_metrics
        )
        
        # åˆ›å»ºæŠ¥å‘Š
        report = ValidationReport(
            timestamp=datetime.now(timezone.utc),
            database_info=database_info,
            schema_validation=schema_results,
            data_validation=data_results,
            performance_validation=performance_results,
            data_quality=data_quality,
            performance_metrics=performance_metrics,
            overall_score=overall_score,
            recommendations=recommendations
        )
        
        return report


def print_validation_report(report: ValidationReport):
    """æ‰“å°éªŒè¯æŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ğŸ” HarborAI è¿½è¸ªç³»ç»Ÿæ•°æ®éªŒè¯æŠ¥å‘Š")
    print("="*80)
    
    print(f"\nğŸ“… æŠ¥å‘Šæ—¶é—´: {report.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}")
    print(f"ğŸ—„ï¸  æ•°æ®åº“: {report.database_info['host']}:{report.database_info['port']}/{report.database_info['database']}")
    print(f"ğŸ“¦ æ•°æ®åº“ç‰ˆæœ¬: {report.database_info['version']}")
    print(f"ğŸ’¾ æ•°æ®åº“å¤§å°: {report.database_info['size']}")
    
    # æ€»ä½“è¯„åˆ†
    print(f"\nğŸ¯ æ€»ä½“è¯„åˆ†: {report.overall_score:.1f}/100")
    
    if report.overall_score >= 90:
        status_emoji = "ğŸŸ¢"
        status_text = "ä¼˜ç§€"
    elif report.overall_score >= 75:
        status_emoji = "ğŸŸ¡"
        status_text = "è‰¯å¥½"
    elif report.overall_score >= 60:
        status_emoji = "ğŸŸ "
        status_text = "ä¸€èˆ¬"
    else:
        status_emoji = "ğŸ”´"
        status_text = "éœ€è¦æ”¹è¿›"
    
    print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€: {status_emoji} {status_text}")
    
    # æ•°æ®è´¨é‡æŒ‡æ ‡
    print(f"\nğŸ“Š æ•°æ®è´¨é‡æŒ‡æ ‡:")
    print(f"   æ€»è®°å½•æ•°: {report.data_quality.total_records:,}")
    print(f"   æœ‰æ•ˆè®°å½•: {report.data_quality.valid_records:,}")
    print(f"   æ— æ•ˆè®°å½•: {report.data_quality.invalid_records:,}")
    print(f"   é‡å¤è®°å½•: {report.data_quality.duplicate_records:,}")
    print(f"   æ•°æ®å®Œæ•´æ€§: {report.data_quality.data_completeness:.1f}%")
    print(f"   æ•°æ®å‡†ç¡®æ€§: {report.data_quality.data_accuracy:.1f}%")
    print(f"   æ•°æ®ä¸€è‡´æ€§: {report.data_quality.data_consistency:.1f}%")
    print(f"   è´¨é‡è¯„åˆ†: {report.data_quality.overall_score:.1f}/100")
    
    # æ€§èƒ½æŒ‡æ ‡
    print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
    print(f"   å¹³å‡æŸ¥è¯¢æ—¶é—´: {report.performance_metrics.avg_query_time_ms:.2f}ms")
    print(f"   æœ€å¤§æŸ¥è¯¢æ—¶é—´: {report.performance_metrics.max_query_time_ms:.2f}ms")
    print(f"   P95æŸ¥è¯¢æ—¶é—´: {report.performance_metrics.p95_query_time_ms:.2f}ms")
    print(f"   ä¼°ç®—ååé‡: {report.performance_metrics.throughput_qps:.1f} QPS")
    print(f"   ç´¢å¼•æ•ˆç‡: {report.performance_metrics.index_efficiency:.1f}%")
    print(f"   å­˜å‚¨æ•ˆç‡: {report.performance_metrics.storage_efficiency:.1f}%")
    
    # éªŒè¯ç»“æœç»Ÿè®¡
    all_results = report.schema_validation + report.data_validation + report.performance_validation
    
    pass_count = len([r for r in all_results if r.status == "pass"])
    warning_count = len([r for r in all_results if r.status == "warning"])
    fail_count = len([r for r in all_results if r.status == "fail"])
    
    print(f"\nğŸ“‹ éªŒè¯ç»“æœç»Ÿè®¡:")
    print(f"   âœ… é€šè¿‡: {pass_count}")
    print(f"   âš ï¸  è­¦å‘Š: {warning_count}")
    print(f"   âŒ å¤±è´¥: {fail_count}")
    print(f"   ğŸ“Š æ€»è®¡: {len(all_results)}")
    
    # è¯¦ç»†ç»“æœï¼ˆä»…æ˜¾ç¤ºè­¦å‘Šå’Œå¤±è´¥ï¼‰
    issues = [r for r in all_results if r.status in ["warning", "fail"]]
    if issues:
        print(f"\nâš ï¸  éœ€è¦å…³æ³¨çš„é—®é¢˜:")
        for result in issues:
            emoji = "âŒ" if result.status == "fail" else "âš ï¸"
            print(f"   {emoji} [{result.category}] {result.test_name}: {result.message}")
    
    # æ”¹è¿›å»ºè®®
    if report.recommendations:
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for recommendation in report.recommendations:
            print(f"   {recommendation}")
    
    print("\n" + "="*80)


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="HarborAI è¿½è¸ªç³»ç»Ÿæ•°æ®éªŒè¯å·¥å…·")
    parser.add_argument("--host", default="localhost", help="æ•°æ®åº“ä¸»æœº")
    parser.add_argument("--port", type=int, default=5432, help="æ•°æ®åº“ç«¯å£")
    parser.add_argument("--database", required=True, help="æ•°æ®åº“åç§°")
    parser.add_argument("--user", required=True, help="æ•°æ®åº“ç”¨æˆ·")
    parser.add_argument("--password", required=True, help="æ•°æ®åº“å¯†ç ")
    parser.add_argument("--output", help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # æ•°æ®åº“é…ç½®
    db_config = {
        "host": args.host,
        "port": args.port,
        "database": args.database,
        "user": args.user,
        "password": args.password
    }
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = TracingDataValidator(db_config)
    
    try:
        # è¿æ¥æ•°æ®åº“
        await validator.connect()
        
        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = await validator.generate_full_report()
        
        # æ‰“å°æŠ¥å‘Š
        print_validation_report(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        if args.output:
            report_data = {
                "timestamp": report.timestamp.isoformat(),
                "database_info": report.database_info,
                "overall_score": report.overall_score,
                "data_quality": asdict(report.data_quality),
                "performance_metrics": asdict(report.performance_metrics),
                "schema_validation": [asdict(r) for r in report.schema_validation],
                "data_validation": [asdict(r) for r in report.data_validation],
                "performance_validation": [asdict(r) for r in report.performance_validation],
                "recommendations": report.recommendations
            }
            
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        
        # æ ¹æ®è¯„åˆ†è®¾ç½®é€€å‡ºç 
        if report.overall_score >= 75:
            sys.exit(0)  # æˆåŠŸ
        elif report.overall_score >= 50:
            sys.exit(1)  # è­¦å‘Š
        else:
            sys.exit(2)  # é”™è¯¯
    
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        sys.exit(3)
    
    finally:
        await validator.disconnect()


if __name__ == "__main__":
    asyncio.run(main())