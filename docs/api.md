# HarborAI API è¯¦ç»†æ–‡æ¡£

æœ¬æ–‡æ¡£æä¾› HarborAI çš„å®Œæ•´ API æ¥å£è¯´æ˜ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ”¯æŒçš„åŠŸèƒ½å’Œè¯¦ç»†çš„ä½¿ç”¨ç¤ºä¾‹ã€‚

## ğŸ“‹ ç›®å½•

- [åŸºç¡€ API](#åŸºç¡€-api)
- [èŠå¤©å®Œæˆ API](#èŠå¤©å®Œæˆ-api)
- [ç»“æ„åŒ–è¾“å‡º](#ç»“æ„åŒ–è¾“å‡º)
- [æ¨ç†æ¨¡å‹](#æ¨ç†æ¨¡å‹)
- [æµå¼å“åº”](#æµå¼å“åº”)
- [å¼‚æ­¥è°ƒç”¨](#å¼‚æ­¥è°ƒç”¨)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
- [æ€§èƒ½ä¼˜åŒ– API](#æ€§èƒ½ä¼˜åŒ–-api)

## åŸºç¡€ API

### å®¢æˆ·ç«¯åˆå§‹åŒ–

HarborAI æä¾›ä¸ OpenAI SDK å®Œå…¨å…¼å®¹çš„ API æ¥å£ï¼š

```python
from harborai import HarborAI

# åŸºç¡€åˆå§‹åŒ–
client = HarborAI(
    api_key="your-api-key",
    base_url="https://api.deepseek.com/v1"  # å¯é€‰ï¼Œé»˜è®¤ä¸º OpenAI
)

# é«˜æ€§èƒ½åˆå§‹åŒ–
from harborai.api.fast_client import FastHarborAI

fast_client = FastHarborAI(
    api_key="your-api-key",
    performance_mode="fast",  # fast, balanced, full
    enable_memory_optimization=True
)
```

### æ”¯æŒçš„æ¨¡å‹

HarborAI æ”¯æŒå¤šä¸ª AI æœåŠ¡æä¾›å•†çš„æ¨¡å‹ï¼š

| æä¾›å•† | æ¨¡å‹åç§° | ç‰¹æ€§ | æ¨èç”¨é€” |
|--------|----------|------|----------|
| **DeepSeek** | `deepseek-chat` | é«˜æ€§ä»·æ¯”ã€ä¸­æ–‡å‹å¥½ | é€šç”¨å¯¹è¯ã€ä»£ç ç”Ÿæˆ |
| **DeepSeek** | `deepseek-reasoner` | æ¨ç†èƒ½åŠ›å¼º | å¤æ‚æ¨ç†ã€æ•°å­¦é—®é¢˜ |
| **ç™¾åº¦åƒå¸†** | `ernie-x1-turbo-32k` | é•¿ä¸Šä¸‹æ–‡ã€ä¸­æ–‡ä¼˜åŒ– | é•¿æ–‡æ¡£å¤„ç† |
| **è±†åŒ…** | `doubao-1-6` | æ¨ç†æ¨¡å‹ | é€»è¾‘æ¨ç†ã€åˆ†æ |
| **OpenAI** | `gpt-4o` | å¤šæ¨¡æ€ã€é«˜è´¨é‡ | å¤æ‚ä»»åŠ¡ã€åˆ›æ„å†™ä½œ |

## èŠå¤©å®Œæˆ API

### åŸºç¡€èŠå¤©

```python
# åŒæ­¥è°ƒç”¨
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}
    ],
    temperature=0.7,
    max_tokens=150
)

print(response.choices[0].message.content)
```

### å¤šè½®å¯¹è¯

```python
# ç»´æŠ¤å¯¹è¯å†å²
conversation = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹ä¸“å®¶ã€‚"}
]

# ç¬¬ä¸€è½®å¯¹è¯
conversation.append({"role": "user", "content": "å¦‚ä½•åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼Ÿ"})
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=conversation
)
conversation.append({"role": "assistant", "content": response.choices[0].message.content})

# ç¬¬äºŒè½®å¯¹è¯
conversation.append({"role": "user", "content": "å¦‚ä½•å‘åˆ—è¡¨æ·»åŠ å…ƒç´ ï¼Ÿ"})
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=conversation
)
```

## ç»“æ„åŒ–è¾“å‡º

HarborAI æ”¯æŒä¸¤ç§ç»“æ„åŒ–è¾“å‡ºæ–¹å¼ï¼š

### 1. JSON Schema æ–¹å¼

```python
# å®šä¹‰ JSON Schema
person_schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string", "description": "äººå‘˜å§“å"},
        "age": {"type": "integer", "description": "å¹´é¾„"},
        "profession": {"type": "string", "description": "èŒä¸š"},
        "skills": {
            "type": "array",
            "items": {"type": "string"},
            "description": "æŠ€èƒ½åˆ—è¡¨"
        }
    },
    "required": ["name", "age", "profession"]
}

# ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "user", "content": "æå–ä¿¡æ¯ï¼šå¼ ä¸‰ï¼Œ30å²ï¼Œè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæ“…é•¿Pythonå’ŒJavaScript"}
    ],
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "person_info",
            "schema": person_schema
        }
    },
    structured_provider="agently"  # å¯é€‰ï¼š"agently" æˆ– "native"
)

# è§£æç»“æœ
import json
result = json.loads(response.choices[0].message.content)
print(f"å§“å: {result['name']}")
print(f"å¹´é¾„: {result['age']}")
print(f"èŒä¸š: {result['profession']}")
```

### 2. Pydantic æ¨¡å‹æ–¹å¼

```python
from pydantic import BaseModel
from typing import List

# å®šä¹‰ Pydantic æ¨¡å‹
class PersonInfo(BaseModel):
    """äººå‘˜ä¿¡æ¯æ¨¡å‹"""
    name: str  # å§“å
    age: int   # å¹´é¾„
    profession: str  # èŒä¸š
    skills: List[str] = []  # æŠ€èƒ½åˆ—è¡¨

# ä½¿ç”¨ Pydantic æ¨¡å‹
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "user", "content": "æå–ä¿¡æ¯ï¼šæå››ï¼Œ25å²ï¼Œæ•°æ®ç§‘å­¦å®¶ï¼Œæ“…é•¿æœºå™¨å­¦ä¹ å’Œæ•°æ®åˆ†æ"}
    ],
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "person_info",
            "schema": PersonInfo.model_json_schema()
        }
    }
)

# ç›´æ¥è§£æä¸º Pydantic å¯¹è±¡
person = PersonInfo.model_validate_json(response.choices[0].message.content)
print(f"å§“å: {person.name}")
print(f"å¹´é¾„: {person.age}")
print(f"æŠ€èƒ½: {', '.join(person.skills)}")
```

## æ¨ç†æ¨¡å‹

æ¨ç†æ¨¡å‹æ”¯æŒæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œé€‚åˆå¤æ‚çš„é€»è¾‘æ¨ç†ä»»åŠ¡ï¼š

```python
# ä½¿ç”¨æ¨ç†æ¨¡å‹
response = client.chat.completions.create(
    model="deepseek-reasoner",  # æˆ– "doubao-1-6"
    messages=[
        {"role": "user", "content": "è§£æ–¹ç¨‹ï¼š2x + 5 = 13ï¼Œè¯·è¯¦ç»†è¯´æ˜è§£é¢˜æ­¥éª¤"}
    ]
)

# æ¨ç†æ¨¡å‹ä¼šè¿”å›æ€è€ƒè¿‡ç¨‹
print("æ€è€ƒè¿‡ç¨‹:")
print(response.choices[0].message.content)
```

### å¤æ‚æ¨ç†ç¤ºä¾‹

```python
# æ•°å­¦é—®é¢˜æ¨ç†
math_problem = """
ä¸€ä¸ªç­çº§æœ‰30åå­¦ç”Ÿï¼Œå…¶ä¸­60%æ˜¯å¥³ç”Ÿã€‚
å¦‚æœæ–°è½¬æ¥5åç”·ç”Ÿï¼Œé‚£ä¹ˆå¥³ç”Ÿå æ€»äººæ•°çš„ç™¾åˆ†æ¯”æ˜¯å¤šå°‘ï¼Ÿ
è¯·è¯¦ç»†è®¡ç®—å¹¶è¯´æ˜æ¯ä¸€æ­¥ã€‚
"""

response = client.chat.completions.create(
    model="deepseek-reasoner",
    messages=[
        {"role": "user", "content": math_problem}
    ],
    temperature=0.1  # é™ä½éšæœºæ€§ï¼Œæé«˜æ¨ç†å‡†ç¡®æ€§
)

print(response.choices[0].message.content)
```

## æµå¼å“åº”

æµå¼å“åº”é€‚åˆéœ€è¦å®æ—¶æ˜¾ç¤ºç”Ÿæˆå†…å®¹çš„åœºæ™¯ï¼š

### åŒæ­¥æµå¼å“åº”

```python
# åŒæ­¥æµå¼è°ƒç”¨
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "user", "content": "è¯·å†™ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„çŸ­æ–‡"}
    ],
    stream=True
)

print("AI æ­£åœ¨ç”Ÿæˆå†…å®¹:")
for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print("\n")
```

### å¼‚æ­¥æµå¼å“åº”

```python
import asyncio

async def async_stream_chat():
    """å¼‚æ­¥æµå¼èŠå¤©ç¤ºä¾‹"""
    response = await client.chat.completions.acreate(
        model="deepseek-chat",
        messages=[
            {"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"}
        ],
        stream=True
    )
    
    print("AI æ­£åœ¨ç”Ÿæˆå†…å®¹:")
    async for chunk in response:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)
    print("\n")

# è¿è¡Œå¼‚æ­¥å‡½æ•°
asyncio.run(async_stream_chat())
```

## å¼‚æ­¥è°ƒç”¨

HarborAI æä¾›å®Œæ•´çš„å¼‚æ­¥æ”¯æŒï¼Œé€‚åˆé«˜å¹¶å‘åœºæ™¯ï¼š

### åŸºç¡€å¼‚æ­¥è°ƒç”¨

```python
import asyncio

async def async_chat_example():
    """å¼‚æ­¥èŠå¤©ç¤ºä¾‹"""
    response = await client.chat.completions.acreate(
        model="deepseek-chat",
        messages=[
            {"role": "user", "content": "ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿ"}
        ]
    )
    return response.choices[0].message.content

# è¿è¡Œå¼‚æ­¥å‡½æ•°
result = asyncio.run(async_chat_example())
print(result)
```

### å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚

```python
import asyncio

async def batch_process():
    """æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚"""
    questions = [
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ"
    ]
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = []
    for question in questions:
        task = client.chat.completions.acreate(
            model="deepseek-chat",
            messages=[{"role": "user", "content": question}]
        )
        tasks.append(task)
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    responses = await asyncio.gather(*tasks)
    
    # å¤„ç†ç»“æœ
    for i, response in enumerate(responses):
        print(f"é—®é¢˜ {i+1}: {questions[i]}")
        print(f"å›ç­”: {response.choices[0].message.content}")
        print("-" * 50)

# è¿è¡Œæ‰¹é‡å¤„ç†
asyncio.run(batch_process())
```

## é”™è¯¯å¤„ç†

HarborAI æä¾›å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼š

### åŸºç¡€é”™è¯¯å¤„ç†

```python
from harborai.core.exceptions import (
    HarborAIError,
    APIError,
    RateLimitError,
    AuthenticationError
)

try:
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "user", "content": "Hello, world!"}
        ]
    )
    print(response.choices[0].message.content)
    
except AuthenticationError as e:
    print(f"è®¤è¯é”™è¯¯: {e}")
except RateLimitError as e:
    print(f"è¯·æ±‚é¢‘ç‡é™åˆ¶: {e}")
except APIError as e:
    print(f"API é”™è¯¯: {e}")
except HarborAIError as e:
    print(f"HarborAI é”™è¯¯: {e}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

### é‡è¯•æœºåˆ¶

```python
from harborai.core.retry import RetryConfig

# é…ç½®é‡è¯•ç­–ç•¥
retry_config = RetryConfig(
    max_retries=3,
    base_delay=1.0,
    max_delay=60.0,
    exponential_base=2.0
)

# ä½¿ç”¨é‡è¯•é…ç½®
client = HarborAI(
    api_key="your-api-key",
    retry_config=retry_config
)
```

## æ€§èƒ½ä¼˜åŒ– API

### FastHarborAI å®¢æˆ·ç«¯

```python
from harborai.api.fast_client import FastHarborAI

# åˆ›å»ºé«˜æ€§èƒ½å®¢æˆ·ç«¯
fast_client = FastHarborAI(
    api_key="your-api-key",
    performance_mode="fast",  # æ€§èƒ½æ¨¡å¼
    enable_memory_optimization=True,  # å¯ç”¨å†…å­˜ä¼˜åŒ–
    enable_lazy_loading=True,  # å¯ç”¨å»¶è¿ŸåŠ è½½
    memory_optimization={
        'cache_size': 2000,
        'object_pool_size': 200,
        'memory_threshold_mb': 100.0,
        'auto_cleanup_interval': 600
    }
)
```

### æ€§èƒ½ç›‘æ§

```python
# è·å–æ€§èƒ½ç»Ÿè®¡
if hasattr(fast_client, 'get_memory_stats'):
    stats = fast_client.get_memory_stats()
    if stats:
        print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats['cache']['hit_rate']:.1%}")
        print(f"å†…å­˜ä½¿ç”¨: {stats['system_memory']['rss_mb']:.1f}MB")
        print(f"è¯·æ±‚æ€»æ•°: {stats['requests']['total']}")

# æ‰‹åŠ¨æ¸…ç†å†…å­˜
if hasattr(fast_client, 'cleanup_memory'):
    fast_client.cleanup_memory(force_clear=True)
```

### æ€§èƒ½æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | æˆæœ¬è·Ÿè¸ª | è¯¦ç»†æ—¥å¿— | ç›‘æ§ | é“¾è·¯è¿½è¸ª | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|------|----------|----------|
| **FAST** | âŒ | âŒ | âŒ | âŒ | é«˜å¹¶å‘ç”Ÿäº§ç¯å¢ƒ |
| **BALANCED** | âœ… | âŒ | âœ… | âŒ | ä¸€èˆ¬ç”Ÿäº§ç¯å¢ƒ |
| **FULL** | âœ… | âœ… | âœ… | âœ… | å¼€å‘è°ƒè¯•ç¯å¢ƒ |

## æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©å»ºè®®

```python
# æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©åˆé€‚çš„æ¨¡å‹
def choose_model(task_type: str) -> str:
    """æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹"""
    model_mapping = {
        "chat": "deepseek-chat",           # é€šç”¨å¯¹è¯
        "reasoning": "deepseek-reasoner",   # å¤æ‚æ¨ç†
        "long_context": "ernie-x1-turbo-32k",  # é•¿æ–‡æ¡£å¤„ç†
        "creative": "gpt-4o",              # åˆ›æ„å†™ä½œ
        "code": "deepseek-chat"            # ä»£ç ç”Ÿæˆ
    }
    return model_mapping.get(task_type, "deepseek-chat")
```

### 2. é”™è¯¯é‡è¯•ç­–ç•¥

```python
import time
from typing import Optional

async def robust_chat_call(
    client: HarborAI,
    messages: list,
    model: str = "deepseek-chat",
    max_retries: int = 3
) -> Optional[str]:
    """å¥å£®çš„èŠå¤©è°ƒç”¨ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
    
    for attempt in range(max_retries):
        try:
            response = await client.chat.completions.acreate(
                model=model,
                messages=messages
            )
            return response.choices[0].message.content
            
        except RateLimitError:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                await asyncio.sleep(wait_time)
                continue
            raise
            
        except Exception as e:
            if attempt < max_retries - 1:
                await asyncio.sleep(1)
                continue
            raise
    
    return None
```

### 3. å†…å­˜ä¼˜åŒ–ä½¿ç”¨

```python
# å¤§æ‰¹é‡å¤„ç†æ—¶çš„å†…å­˜ä¼˜åŒ–
async def process_large_batch(questions: list, batch_size: int = 10):
    """åˆ†æ‰¹å¤„ç†å¤§é‡è¯·æ±‚ï¼Œé¿å…å†…å­˜æº¢å‡º"""
    
    results = []
    for i in range(0, len(questions), batch_size):
        batch = questions[i:i + batch_size]
        
        # å¤„ç†å½“å‰æ‰¹æ¬¡
        batch_tasks = [
            client.chat.completions.acreate(
                model="deepseek-chat",
                messages=[{"role": "user", "content": q}]
            )
            for q in batch
        ]
        
        batch_results = await asyncio.gather(*batch_tasks)
        results.extend(batch_results)
        
        # æ¸…ç†å†…å­˜ï¼ˆå¦‚æœä½¿ç”¨ FastHarborAIï¼‰
        if hasattr(client, 'cleanup_memory'):
            client.cleanup_memory()
        
        # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        await asyncio.sleep(0.1)
    
    return results
```

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ€§èƒ½æ¨¡å¼ï¼Ÿ

**A**: æ ¹æ®æ‚¨çš„ä½¿ç”¨åœºæ™¯é€‰æ‹©ï¼š
- **ç”Ÿäº§ç¯å¢ƒé«˜å¹¶å‘**: ä½¿ç”¨ `FAST` æ¨¡å¼
- **ä¸€èˆ¬ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨ `BALANCED` æ¨¡å¼  
- **å¼€å‘è°ƒè¯•**: ä½¿ç”¨ `FULL` æ¨¡å¼

### Q: ç»“æ„åŒ–è¾“å‡ºå¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š
1. æ£€æŸ¥ JSON Schema æ ¼å¼æ˜¯å¦æ­£ç¡®
2. ä½¿ç”¨ `structured_provider="agently"` æé«˜æˆåŠŸç‡
3. åœ¨ prompt ä¸­æ˜ç¡®è¦æ±‚è¿”å› JSON æ ¼å¼
4. é™ä½ `temperature` å‚æ•°æé«˜ä¸€è‡´æ€§

### Q: å¦‚ä½•å¤„ç†é•¿æ–‡æœ¬ï¼Ÿ

**A**: 
1. ä½¿ç”¨æ”¯æŒé•¿ä¸Šä¸‹æ–‡çš„æ¨¡å‹å¦‚ `ernie-x1-turbo-32k`
2. å°†é•¿æ–‡æœ¬åˆ†æ®µå¤„ç†
3. ä½¿ç”¨æµå¼å“åº”é¿å…è¶…æ—¶

---

**æ›´å¤š API è¯¦æƒ…è¯·å‚è€ƒ**: [HarborAI GitHub](https://github.com/ailijian/harborai) | [ç¤ºä¾‹ä»£ç ](../examples/)