#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
多模型LLM客户端
支持多种模型的统一调用接口，包括流式/非流式输出、原生/JSON输出等功能
"""

import os
import json
import time
import logging
import traceback
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Union, Iterator, Any, Generator, AsyncGenerator
from dataclasses import dataclass
from enum import Enum

from openai import OpenAI

# Agently相关导入
try:
    import Agently
except ImportError:
    print("警告: Agently库未安装，JSON模式功能将不可用")
    Agently = None

# tiktoken用于token计算
try:
    import tiktoken
except ImportError:
    print("警告: tiktoken库未安装，token估算功能将不可用")
    tiktoken = None

# 预定义的模型配置
MODEL_CONFIGS = {
    # DeepSeek模型
    "deepseek-chat": {
        "base_url": "https://api.deepseek.com",
        "api_key": "sk-d996b310528f44ffb1d7bf5b23b5313b",
        "temperature": 0.1
    },
    # ERNIE系列模型配置
    "ernie-3.5-8k": {
        "base_url": "https://qianfan.baidubce.com/v2",
        "api_key": "bce-v3/ALTAK-zOkbkPd1PRzUP9Vlnv4iT/70d795c8c18da64a63a357228ff0c66c9bf3032a",
        "model": "ernie-3.5-8k",
        "temperature": 0.1,
        "is_reasoning": False
    },
    "ernie-4.0-turbo-8k": {
        "base_url": "https://qianfan.baidubce.com/v2",
        "api_key": "bce-v3/ALTAK-zOkbkPd1PRzUP9Vlnv4iT/70d795c8c18da64a63a357228ff0c66c9bf3032a",
        "model": "ernie-4.0-turbo-8k",
        "temperature": 0.1,
        "is_reasoning": False
    },
    "ernie-x1-turbo-32k": {
        "base_url": "https://qianfan.baidubce.com/v2",
        "api_key": "bce-v3/ALTAK-zOkbkPd1PRzUP9Vlnv4iT/70d795c8c18da64a63a357228ff0c66c9bf3032a",
        "model": "ernie-x1-turbo-32k",
        "temperature": 0.1,
        "is_reasoning": True
    },
    # 豆包模型
    "doubao-1-5-pro-32k-character-250715": {
        "base_url": "https://ark.cn-beijing.volces.com/api/v3",
        "api_key": "4ed46be9-4eb4-45f1-8576-d2fc3d115026",
        "temperature": 0.1
    },
    "doubao-seed-1-6-250615": {
        "base_url": "https://ark.cn-beijing.volces.com/api/v3",
        "api_key": "4ed46be9-4eb4-45f1-8576-d2fc3d115026",
        "temperature": 0.1
    }
}


class OutputMode(Enum):
    """输出模式枚举"""
    NATIVE = "native"  # 原生输出
    JSON = "json"      # JSON输出
    STREAM = "stream"   # 流式输出
    INSTANT = "instant" # 实时解析输出


class CallMode(Enum):
    """调用模式枚举"""
    NATIVE = "native"  # 原生模式
    JSON = "json"      # JSON模式


class ModelType(Enum):
    """模型类型枚举"""
    NORMAL = "normal"      # 普通模型
    REASONING = "reasoning"  # 思考模型


@dataclass
class RequestInfo:
    """请求信息数据类"""
    timestamp: str
    model: str
    prompt: str
    stream: bool
    output_mode: OutputMode
    model_type: ModelType


@dataclass
class ResponseInfo:
    """响应信息数据类"""
    input_tokens: int
    output_tokens: int
    total_tokens: int
    response_time: float
    content: str
    thinking_content: Optional[str] = None  # 思考过程内容（仅思考模型）


@dataclass
class CallResult:
    """调用结果数据类"""
    success: bool
    content: Any
    token_usage: Optional[str] = None
    cost: Optional[float] = None
    duration: Optional[float] = None
    error: Optional[str] = None


@dataclass
class ModelConfig:
    """模型配置数据类"""
    model_name: str
    api_key: str
    base_url: str
    temperature: float = 0.1
    max_tokens: Optional[int] = None
    is_reasoning: bool = False


class TokenExtractor:
    """Token提取器，用于从不同厂商的响应中提取token使用信息"""
    
    @staticmethod
    def extract_from_response(response: Any, model_name: str) -> Dict[str, int]:
        """
        从响应中提取token使用信息
        
        Args:
            response: 模型响应对象
            model_name: 模型名称
            
        Returns:
            包含token使用信息的字典
        """
        try:
            # 1. 标准OpenAI格式
            if hasattr(response, 'usage'):
                usage = response.usage
                return {
                    'prompt_tokens': getattr(usage, 'prompt_tokens', 0),
                    'completion_tokens': getattr(usage, 'completion_tokens', 0),
                    'total_tokens': getattr(usage, 'total_tokens', 0)
                }
            
            # 2. 字典格式响应
            if isinstance(response, dict):
                if 'usage' in response:
                    usage = response['usage']
                    return {
                        'prompt_tokens': usage.get('prompt_tokens', 0),
                        'completion_tokens': usage.get('completion_tokens', 0),
                        'total_tokens': usage.get('total_tokens', 0)
                    }
                
                # 检查其他可能的字段名
                for key in ['token_usage', 'tokens', 'usage_info']:
                    if key in response:
                        usage = response[key]
                        return {
                            'prompt_tokens': usage.get('input_tokens', usage.get('prompt_tokens', 0)),
                            'completion_tokens': usage.get('output_tokens', usage.get('completion_tokens', 0)),
                            'total_tokens': usage.get('total_tokens', 0)
                        }
            
            # 3. 尝试从model_dump()获取
            if hasattr(response, 'model_dump'):
                response_dict = response.model_dump()
                if 'usage' in response_dict:
                    usage = response_dict['usage']
                    return {
                        'prompt_tokens': usage.get('prompt_tokens', 0),
                        'completion_tokens': usage.get('completion_tokens', 0),
                        'total_tokens': usage.get('total_tokens', 0)
                    }
            
            return {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
            
        except Exception as e:
            print(f"提取token信息时出错: {str(e)}")
            return {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}


class TokenCalculator:
    """Token计算器，用于计算token使用量和成本"""
    
    # 模型价格配置（每1K tokens的价格，单位：美元）
    MODEL_PRICING = {
        "deepseek-chat": {"input": 0.00014, "output": 0.00028},
        "ernie-3.5-8k": {"input": 0.0008, "output": 0.002},
        "ernie-4.0-turbo-8k": {"input": 0.003, "output": 0.009},
        "ernie-x1-turbo-32k": {"input": 0.008, "output": 0.024},
        "doubao-1-5-pro-32k-character-250715": {"input": 0.0008, "output": 0.002},
        "doubao-seed-1-6-250615": {"input": 0.0005, "output": 0.001}
    }
    
    def __init__(self):
        self.token_extractor = TokenExtractor()
    
    def estimate_tokens(self, text: str, model_name: str = "gpt-3.5-turbo") -> int:
        """
        估算文本的token数量
        
        Args:
            text: 要估算的文本
            model_name: 模型名称
            
        Returns:
            估算的token数量
        """
        if not text:
            return 0
        
        try:
            if tiktoken:
                # 使用tiktoken进行精确估算
                encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")  # 使用通用编码
                return len(encoding.encode(text))
            else:
                # 简单估算：中文字符按1.5个token计算，英文单词按1个token计算
                chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
                other_chars = len(text) - chinese_chars
                return int(chinese_chars * 1.5 + other_chars * 0.25)
        except Exception:
            # 备用估算方法
            return len(text) // 4
    
    def get_token_usage(self, prompt: str, response_content: str, 
                       response: Any, model_name: str, base_url: str) -> str:
        """
        获取token使用信息，优先使用厂商返回的真实数值
        
        Args:
            prompt: 输入提示词
            response_content: 响应内容
            response: 原始响应对象
            model_name: 模型名称
            base_url: API基础URL
            
        Returns:
            格式化的token使用信息字符串
        """
        # 首先尝试从响应中提取真实的token数据
        token_info = self.token_extractor.extract_from_response(response, model_name)
        
        input_tokens = token_info.get('prompt_tokens', 0)
        output_tokens = token_info.get('completion_tokens', 0)
        total_tokens = token_info.get('total_tokens', 0)
        
        # 如果没有获取到真实数据，则进行估算
        if total_tokens == 0:
            input_tokens = self.estimate_tokens(prompt, model_name)
            output_tokens = self.estimate_tokens(response_content, model_name)
            total_tokens = input_tokens + output_tokens
            estimation_note = " (估算)"
        else:
            estimation_note = " (厂商数据)"
        
        return f"输入: {input_tokens}, 输出: {output_tokens}, 总计: {total_tokens}{estimation_note}"
    
    def calculate_cost(self, input_tokens: int, output_tokens: int, model_name: str) -> Optional[float]:
        """
        计算调用成本
        
        Args:
            input_tokens: 输入token数量
            output_tokens: 输出token数量
            model_name: 模型名称
            
        Returns:
            成本（美元），如果无法计算则返回None
        """
        if model_name not in self.MODEL_PRICING:
            return None
        
        pricing = self.MODEL_PRICING[model_name]
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        
        return input_cost + output_cost


class AgentyLogger:
    """Agently日志记录器"""
    
    def __init__(self, logger_name: str = "AgentyClient"):
        self.logger = logging.getLogger(logger_name)
        self.logger.setLevel(logging.INFO)
        
        # 如果没有处理器，添加一个控制台处理器
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def log_request(self, model_name: str, messages: List[Dict], **kwargs):
        """记录请求信息"""
        self.logger.info(f"发送请求到模型: {model_name}")
        self.logger.debug(f"请求消息: {messages}")
        if kwargs:
            self.logger.debug(f"请求参数: {kwargs}")
    
    def log_response(self, response_content: str, token_usage: str, cost: Optional[float] = None):
        """记录响应信息"""
        self.logger.info(f"收到响应，Token使用: {token_usage}")
        if cost is not None:
            self.logger.info(f"调用成本: ${cost:.6f}")
        self.logger.debug(f"响应内容: {response_content[:200]}..." if len(response_content) > 200 else response_content)
    
    def log_stream_event(self, event_type: str, content: str = ""):
        """记录流式事件"""
        self.logger.debug(f"流式事件: {event_type}, 内容: {content[:100]}..." if len(content) > 100 else content)
    
    def log_error(self, error: Exception, context: str = ""):
        """记录错误信息"""
        self.logger.error(f"错误发生 {context}: {str(error)}")
    
    def log_raw_response(self, response: Any):
        """记录原始响应（调试用）"""
        self.logger.debug(f"原始响应: {response}")


class LLMClient:
    """多模型LLM客户端类"""
    
    def __init__(self, debug: bool = False, log_level: str = "INFO"):
        """
        初始化LLM客户端
        
        Args:
            debug: 是否开启调试模式
            log_level: 日志级别
        """
        self.debug = debug
        self.logger = self._setup_logger(log_level)
        self.clients = {}  # 存储不同模型的客户端实例
        
        # 初始化工具类
        self.token_calculator = TokenCalculator()
        self.agently_logger = AgentyLogger("LLMClient")
        
        # 初始化Agently相关组件
        try:
            import agently
            self.Agently = agently.Agently
            # 设置当前模型为OpenAICompatible
            self.Agently.set_settings("current_model", "OpenAICompatible")
            self.agently_available = True
            self.logger.info("Agently初始化成功")
        except Exception as e:
            self.logger.warning(f"Agently初始化失败: {e}")
            self.Agently = None
            self.agently_available = False
        
        # 初始化所有模型的客户端
        self._initialize_clients()
    
    def _setup_logger(self, log_level: str) -> logging.Logger:
        """
        设置日志记录器
        
        Args:
            log_level: 日志级别
            
        Returns:
            配置好的日志记录器
        """
        logger = logging.getLogger("llm_client")
        logger.setLevel(getattr(logging, log_level.upper()))
        
        # 创建控制台处理器
        console_handler = logging.StreamHandler()
        console_handler.setLevel(getattr(logging, log_level.upper()))
        
        # 创建文件处理器
        file_handler = logging.FileHandler("llm_client.log", encoding="utf-8")
        file_handler.setLevel(logging.INFO)
        
        # 创建格式器
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)
        
        # 添加处理器
        if not logger.handlers:
            logger.addHandler(console_handler)
            logger.addHandler(file_handler)
        
        return logger
    
    def _initialize_clients(self):
        """
        初始化所有模型的OpenAI客户端实例
        """
        for model_name, config in MODEL_CONFIGS.items():
            try:
                client = OpenAI(
                    api_key=config["api_key"],
                    base_url=config["base_url"]
                )
                self.clients[model_name] = client
                self.logger.info(f"成功初始化模型客户端: {model_name}")
            except Exception as e:
                self.logger.error(f"初始化模型客户端失败 {model_name}: {str(e)}")
    
    def _log_debug_info(self, request_info: RequestInfo, response_info: Optional[ResponseInfo] = None):
        """
        记录调试信息
        
        Args:
            request_info: 请求信息
            response_info: 响应信息（可选）
        """
        if not self.debug:
            return
        
        debug_info = {
            "timestamp": request_info.timestamp,
            "model": request_info.model,
            "prompt": request_info.prompt,
            "stream": request_info.stream,
            "output_mode": request_info.output_mode.value,
            "model_type": request_info.model_type.value
        }
        
        if response_info:
            # 添加token使用统计信息
            token_stats = {
                "input_tokens": response_info.input_tokens,
                "output_tokens": response_info.output_tokens,
                "total_tokens": response_info.total_tokens
            }
            
            debug_info.update({
                "token_usage": token_stats,
                "response_time": response_info.response_time,
                "stream_mode": request_info.stream
            })
            
            # 为流式响应添加特殊标识和token效率统计
            if request_info.stream:
                debug_info["stream_info"] = {
                    "is_streaming": True,
                    "token_efficiency": f"{response_info.output_tokens / response_info.response_time:.2f} tokens/s" if response_info.response_time > 0 and response_info.output_tokens > 0 else "N/A"
                }
            
            # 添加思考过程信息（仅思考模型）
            if response_info.thinking_content:
                debug_info["thinking_content"] = {
                    "length": len(response_info.thinking_content),
                    "preview": response_info.thinking_content[:200] + "..." if len(response_info.thinking_content) > 200 else response_info.thinking_content
                }
        
        self.logger.debug(f"调试信息: {json.dumps(debug_info, ensure_ascii=False, indent=2)}")
    
    def set_debug(self, debug: bool):
        """
        设置调试模式开关
        
        Args:
            debug: 是否开启调试模式
        """
        self.debug = debug
        if debug:
            self.logger.setLevel(logging.DEBUG)
            self.agently_logger.logger.setLevel(logging.DEBUG)
        else:
            self.logger.setLevel(logging.INFO)
            self.agently_logger.logger.setLevel(logging.INFO)
        self.logger.info(f"调试模式已{'开启' if debug else '关闭'}")
    
    def _configure_agently_client(self, model_name: str, base_url: str, api_key: str, model_type: ModelType = ModelType.NORMAL):
        """
        配置Agently客户端
        
        Args:
            model_name: 模型名称
            base_url: API基础URL
            api_key: API密钥
            model_type: 模型类型（推理模型或普通模型）
            
        Returns:
            配置好的Agently代理实例
        """
        if not self.agently_available:
            raise RuntimeError("Agently未正确初始化")
        
        try:
            # 基础配置
            openai_compatible_config = {
                "base_url": base_url,
                "model": model_name,
                "model_type": "chat",
                "auth": {"api_key": api_key},
                "request_options": {
                    "temperature": 0.7,
                    "top_p": 1.0,
                    "frequency_penalty": 0.0,
                    "presence_penalty": 0.0,
                }
            }
            
            # 针对推理模型的特殊配置
            if model_type == ModelType.REASONING:
                # 推理模型可能需要特殊的参数配置
                # 例如：更低的temperature以获得更稳定的推理过程
                openai_compatible_config["request_options"]["temperature"] = 0.1
                
                # 某些推理模型可能需要特殊的系统提示或配置
                # 这里可以根据具体的推理模型需求进行扩展
                self.logger.debug(f"为推理模型 {model_name} 应用特殊配置")
            
            # 设置配置
            self.Agently.set_settings("OpenAICompatible", openai_compatible_config)
            
            # 创建代理实例
            agent = self.Agently.create_agent()
            
            # 为推理模型设置特殊的输出格式或行为
            if model_type == ModelType.REASONING:
                # 可以在这里为推理模型设置特殊的输出格式
                # 例如：确保输出包含推理过程
                pass
            
            return agent
        except Exception as e:
            self.logger.error(f"配置Agently客户端失败: {e}")
            raise
    
    def get_available_models(self) -> List[str]:
        """
        获取可用的模型列表
        
        Returns:
            可用模型名称列表
        """
        return list(MODEL_CONFIGS.keys())
    
    def validate_model(self, model: str) -> bool:
        """
        验证模型是否可用
        
        Args:
            model: 模型名称
            
        Returns:
            模型是否可用
        """
        return model in MODEL_CONFIGS and model in self.clients
    
    def call_native(
        self,
        model: str,
        messages: List[Dict[str, str]],
        stream: bool = False,
        model_type: ModelType = ModelType.NORMAL,
        **kwargs
    ) -> Union[Dict[str, Any], Iterator[Dict[str, Any]]]:
        """
        原生模式调用模型（基于OpenAI标准）
        
        Args:
            model: 模型名称
            messages: 消息列表
            stream: 是否流式输出
            model_type: 模型类型
            **kwargs: 其他参数
            
        Returns:
            模型响应（原生格式）
            
        Raises:
            ValueError: 模型不可用或参数错误时抛出
            ConnectionError: 网络连接失败时抛出
            TimeoutError: 请求超时时抛出
            Exception: 其他调用失败时抛出
        """
        # 参数验证
        if not model:
            raise ValueError("模型名称不能为空")
        if not messages or not isinstance(messages, list):
            raise ValueError("消息列表不能为空且必须是列表格式")
        if not self.validate_model(model):
            raise ValueError(f"模型 {model} 不可用")
        
        # 记录请求信息
        request_info = RequestInfo(
            timestamp=datetime.now().isoformat(),
            model=model,
            prompt=json.dumps(messages, ensure_ascii=False),
            stream=stream,
            output_mode=OutputMode.NATIVE,
            model_type=model_type
        )
        
        self._log_debug_info(request_info)
        
        try:
            client = self.clients[model]
            config = MODEL_CONFIGS[model]
            
            # 合并配置参数
            params = {
                "model": model,
                "messages": messages,
                "stream": stream,
                "temperature": config.get("temperature", 0.1),
                **kwargs
            }
            
            # 为流式响应添加stream_options以获取token统计
            if stream:
                params["stream_options"] = {"include_usage": True}
            
            start_time = time.time()
            
            if stream:
                return self._handle_stream_response(client, params, request_info, start_time)
            else:
                return self._handle_non_stream_response(client, params, request_info, start_time)
                
        except ConnectionError as e:
            error_msg = f"连接模型 {model} 失败: {str(e)}"
            self.logger.error(error_msg)
            raise ConnectionError(error_msg)
        except TimeoutError as e:
            error_msg = f"调用模型 {model} 超时: {str(e)}"
            self.logger.error(error_msg)
            raise TimeoutError(error_msg)
        except ValueError as e:
            error_msg = f"模型 {model} 参数错误: {str(e)}"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
        except Exception as e:
            error_msg = f"调用模型 {model} 失败: {str(e)}"
            self.logger.error(error_msg)
            raise Exception(error_msg)
    
    def _handle_non_stream_response(
        self,
        client: OpenAI,
        params: Dict[str, Any],
        request_info: RequestInfo,
        start_time: float
    ) -> Dict[str, Any]:
        """
        处理非流式响应
        
        Args:
            client: OpenAI客户端
            params: 请求参数
            request_info: 请求信息
            start_time: 开始时间
            
        Returns:
            模型响应
        """
        response = client.chat.completions.create(**params)
        response_time = time.time() - start_time
        
        # 提取token使用信息
        usage = response.usage
        
        # 提取思考过程内容（仅思考模型）
        thinking_content = None
        if request_info.model_type == ModelType.REASONING and response.choices:
            message = response.choices[0].message
            
            # 1. 检查标准OpenAI格式（ERNIE-X1使用）
            if hasattr(message, 'reasoning_content') and message.reasoning_content:
                thinking_content = message.reasoning_content
            # 2. 检查豆包特殊格式
            elif hasattr(message, 'thinking_content') and message.thinking_content:
                thinking_content = message.thinking_content
            # 3. 兼容性处理：也检查原始响应字典格式
            elif hasattr(response, 'model_dump'):
                response_dict = response.model_dump()
                if response_dict.get('choices') and response_dict['choices'][0].get('message'):
                    msg_dict = response_dict['choices'][0]['message']
                    if msg_dict.get('reasoning_content'):
                        thinking_content = msg_dict['reasoning_content']
                    elif msg_dict.get('thinking_content'):
                        thinking_content = msg_dict['thinking_content']
        
        response_info = ResponseInfo(
            input_tokens=usage.prompt_tokens if usage else 0,
            output_tokens=usage.completion_tokens if usage else 0,
            total_tokens=usage.total_tokens if usage else 0,
            response_time=response_time,
            content=response.choices[0].message.content if response.choices else "",
            thinking_content=thinking_content
        )
        
        self._log_debug_info(request_info, response_info)
        # 在非流式调用完成日志中包含token使用信息
        token_info = f"Token使用: 输入={response_info.input_tokens}, 输出={response_info.output_tokens}, 总计={response_info.total_tokens}"
        self.logger.info(f"模型 {request_info.model} 调用成功，耗时: {response_time:.2f}s，{token_info}")
        
        return response.model_dump()
    
    def _handle_stream_response(
        self,
        client: OpenAI,
        params: Dict[str, Any],
        request_info: RequestInfo,
        start_time: float
    ) -> Iterator[Dict[str, Any]]:
        """
        处理流式响应
        
        Args:
            client: OpenAI客户端
            params: 请求参数
            request_info: 请求信息
            start_time: 开始时间
            
        Yields:
            流式响应数据（包含思考过程和回答内容）
        """
        stream = client.chat.completions.create(**params)
        
        total_content = ""
        total_thinking_content = ""  # 累积思考过程内容
        chunk_count = 0
        input_tokens = 0
        output_tokens = 0
        total_tokens = 0
        
        try:
            for chunk in stream:
                chunk_count += 1
                chunk_dict = chunk.model_dump()
                
                # 累积内容用于调试
                if chunk.choices and chunk.choices[0].delta.content:
                    total_content += chunk.choices[0].delta.content
                
                # 实时提取思考过程内容（仅思考模型）
                current_thinking_content = ""
                if (request_info.model_type == ModelType.REASONING and 
                    chunk.choices and chunk.choices[0].delta):
                    
                    delta = chunk.choices[0].delta
                    
                    # 1. 检查标准OpenAI格式（ERNIE-X1使用）
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        current_thinking_content = delta.reasoning_content
                    # 2. 检查字典格式
                    elif isinstance(chunk_dict.get('choices', [{}])[0].get('delta', {}), dict):
                        delta_dict = chunk_dict['choices'][0]['delta']
                        if delta_dict.get('reasoning_content'):
                            current_thinking_content = delta_dict['reasoning_content']
                    # 3. 检查豆包特殊格式（可能使用其他字段名）
                    elif hasattr(delta, 'thinking_content') and delta.thinking_content:
                        current_thinking_content = delta.thinking_content
                    elif delta_dict.get('thinking_content'):
                        current_thinking_content = delta_dict['thinking_content']
                    
                    # 累积思考过程
                    if current_thinking_content:
                        total_thinking_content += current_thinking_content
                
                # 在chunk中添加实时思考过程内容
                if current_thinking_content:
                    chunk_dict['thinking_content'] = current_thinking_content
                
                # 检查是否有usage信息（通常在最后一个chunk中）
                if hasattr(chunk, 'usage') and chunk.usage:
                    if hasattr(chunk.usage, 'prompt_tokens'):
                        input_tokens = chunk.usage.prompt_tokens
                    if hasattr(chunk.usage, 'completion_tokens'):
                        output_tokens = chunk.usage.completion_tokens
                    if hasattr(chunk.usage, 'total_tokens'):
                        total_tokens = chunk.usage.total_tokens
                
                # 立即yield chunk，实现真正的流式输出
                yield chunk_dict
            
            response_time = time.time() - start_time
            
            # 记录流式响应的调试信息（在流式结束后记录）
            response_info = ResponseInfo(
                input_tokens=input_tokens,  # 使用从流式响应中收集的token统计
                output_tokens=output_tokens,
                total_tokens=total_tokens,
                response_time=response_time,
                content=total_content,
                thinking_content=total_thinking_content if total_thinking_content else None
            )
            
            self._log_debug_info(request_info, response_info)
            # 在流式调用完成日志中包含token使用信息
            token_info = f"Token使用: 输入={input_tokens}, 输出={output_tokens}, 总计={total_tokens}"
            self.logger.info(f"模型 {request_info.model} 流式调用完成，共 {chunk_count} 个chunk，耗时: {response_time:.2f}s，{token_info}")
            
        except Exception as e:
            self.logger.error(f"流式响应处理失败: {str(e)}")
            raise
    
    def call_json(
        self,
        model: str,
        prompt: str,
        stream: bool = False,
        model_type: ModelType = ModelType.NORMAL,
        output_schema: Optional[Dict] = None,
        instant_keys: Optional[List[str]] = None,
        use_generator: bool = False,
        **kwargs
    ) -> Union[Dict[str, Any], Iterator[Dict[str, Any]], Generator[Dict[str, Any], None, None]]:
        """
        JSON模式调用模型（基于Agently库的Instant流式解析）
        
        重要说明：
        - 完全基于Agently库实现，支持结构化数据的实时流式解析
        - 支持推理模型和非推理模型的JSON格式输出
        - 集成Agently Instant模式，实现真正的流式JSON解析
        - 与原生模式完全分离，各司其职
        
        Args:
            model: 模型名称
            prompt: 提示词
            stream: 是否流式输出
            model_type: 模型类型
                - NORMAL: 普通模型，按JSON schema输出
                - REASONING: 思考模型，在JSON模式下输出结构化数据
            output_schema: JSON输出模式（定义输出的JSON结构）
            instant_keys: 需要实时监听的特定key列表
            use_generator: 是否使用generator模式（支持for循环轮询）
            **kwargs: 其他参数
            
        Returns:
            JSON格式的模型响应
            
        Raises:
            RuntimeError: Agently未正确初始化时抛出
            ValueError: 模型不可用或参数错误时抛出
            ConnectionError: 网络连接失败时抛出
            TimeoutError: 请求超时时抛出
            Exception: 其他调用失败时抛出
        """
        # 参数验证
        if not self.agently_available:
            raise RuntimeError("Agently未正确初始化，无法使用JSON模式")
        
        if not model:
            raise ValueError("模型名称不能为空")
        if not prompt or not isinstance(prompt, str):
            raise ValueError("提示词不能为空且必须是字符串格式")
        if not self.validate_model(model):
            raise ValueError(f"不支持的模型: {model}")
        
        # 获取模型配置
        model_config = MODEL_CONFIGS.get(model)
        if not model_config:
            raise ValueError(f"未找到模型配置: {model}")
        
        # 验证instant_keys参数
        if instant_keys is not None and not isinstance(instant_keys, list):
            raise ValueError("instant_keys必须是列表格式")
        
        start_time = time.time()
        
        try:
            # 配置Agently客户端
            agent = self._configure_agently_client(
                model, 
                model_config['base_url'], 
                model_config['api_key'],
                model_type
            )
            
            # 记录请求信息
            self.agently_logger.log_request(model, [{"role": "user", "content": prompt}], **kwargs)
            
            # 设置输出模式
            if output_schema:
                agent.output(output_schema)
            
            # 根据不同模式调用
            if stream and use_generator:
                # Generator模式：支持for循环轮询
                return self._handle_json_generator(agent, prompt, model, start_time, instant_keys, **kwargs)
            elif stream:
                # 标准流式模式：使用Instant实时解析
                return self._handle_json_instant_stream(agent, prompt, model, start_time, instant_keys, **kwargs)
            else:
                # 同步模式
                return self._handle_json_sync(agent, prompt, model, start_time, **kwargs)
                
        except ConnectionError as e:
            error_msg = f"连接模型 {model} 失败: {str(e)}"
            self.agently_logger.log_error(e, "JSON模式连接失败")
            raise ConnectionError(error_msg)
        except TimeoutError as e:
            error_msg = f"调用模型 {model} 超时: {str(e)}"
            self.agently_logger.log_error(e, "JSON模式调用超时")
            raise TimeoutError(error_msg)
        except ValueError as e:
            error_msg = f"JSON模式参数错误: {str(e)}"
            self.agently_logger.log_error(e, "JSON模式参数错误")
            raise ValueError(error_msg)
        except Exception as e:
            error_msg = f"JSON模式调用失败: {str(e)}"
            self.agently_logger.log_error(e, "JSON模式调用")
            raise Exception(error_msg)
    
    def _handle_json_sync(self, agent, prompt: str, model: str, start_time: float, **kwargs) -> Dict[str, Any]:
        """
        处理JSON模式的同步调用
        
        Args:
            agent: Agently代理实例
            prompt: 提示词
            model: 模型名称
            start_time: 开始时间
            **kwargs: 其他参数
            
        Returns:
            JSON格式的响应
        """
        try:
            # 执行同步调用
            response = agent.input(prompt).start()
            
            # 解析JSON响应
            if hasattr(response, 'data'):
                json_content = response.data
            elif hasattr(response, 'get_data'):
                json_content = response.get_data()
            else:
                # 如果response是字符串，尝试解析为JSON
                if isinstance(response, str):
                    try:
                        json_content = json.loads(response)
                    except json.JSONDecodeError:
                        json_content = response
                else:
                    # 如果response已经是dict或其他对象，直接使用
                    json_content = response
            
            # 计算响应时间
            response_time = time.time() - start_time
            
            # 获取token使用信息
            token_usage = self.token_calculator.get_token_usage(
                prompt, str(json_content), response, model, 
                MODEL_CONFIGS[model]['base_url']
            )
            
            # 计算成本
            token_info = self.token_calculator.token_extractor.extract_from_response(response, model)
            cost = self.token_calculator.calculate_cost(
                token_info.get('prompt_tokens', 0),
                token_info.get('completion_tokens', 0),
                model
            )
            
            # 记录响应信息
            self.agently_logger.log_response(str(json_content), token_usage, cost)
            
            return {
                'type': 'sync',
                'content': json_content,
                'model': model,
                'response_time': response_time,
                'token_usage': token_info,
                'cost': cost
            }
            
        except Exception as e:
            self.agently_logger.log_error(e, "JSON同步调用")
            raise
    
    def _handle_json_instant_stream(self, agent, prompt: str, model: str, start_time: float, instant_keys: Optional[List[str]] = None, **kwargs) -> Iterator[Dict[str, Any]]:
        """
        处理JSON模式的Instant流式调用（基于Agently官方Instant模式API）
        
        Args:
            agent: Agently代理实例
            prompt: 提示词
            model: 模型名称
            start_time: 开始时间
            instant_keys: 需要实时监听的特定key列表
            **kwargs: 其他参数
            
        Yields:
            实时流式JSON响应
        """
        try:
            # 存储累积数据
            accumulated_data = {}
            
            # 根据官方文档，使用get_generator(content="instant")获取流式解析生成器
            generator = agent.input(prompt).get_generator(content="instant")
            
            # 遍历生成器数据
            for instant_message in generator:
                # instant_message是StreamingData对象，直接访问属性
                path = getattr(instant_message, 'path', '').split('.') if hasattr(instant_message, 'path') else []
                delta = getattr(instant_message, 'delta', '')
                value = getattr(instant_message, 'value', '')
                is_complete = getattr(instant_message, 'is_complete', False)
                
                # 获取当前字段名（path的最后一个元素）
                current_key = path[-1] if path else ''
                
                # 如果指定了特定key，只处理这些key的更新
                if instant_keys and current_key not in instant_keys:
                    continue
                
                # 更新累积数据
                if current_key:
                    if delta:  # 增量更新
                        if current_key not in accumulated_data:
                            accumulated_data[current_key] = delta
                        else:
                            # 如果是字符串类型，进行累积
                            if isinstance(accumulated_data[current_key], str) and isinstance(delta, str):
                                accumulated_data[current_key] += delta
                            else:
                                accumulated_data[current_key] = delta
                    elif value:  # 完整值更新
                        accumulated_data[current_key] = value
                
                # 构建事件结果
                event_result = {
                    'type': 'instant_delta',
                    'key': current_key,
                    'delta': delta,
                    'value': value,
                    'path': path,
                    'accumulated_content': accumulated_data.copy(),
                    'model': model,
                    'timestamp': time.time(),
                    'is_complete': is_complete
                }
                
                yield event_result
                
                # 如果完成，退出循环
                if is_complete:
                    break
            
            # 计算响应时间
            response_time = time.time() - start_time
            
            # 获取token使用信息（使用累积数据作为最终结果）
            token_usage = self.token_calculator.get_token_usage(
                prompt, str(accumulated_data), accumulated_data, model, 
                MODEL_CONFIGS[model]['base_url']
            )
            
            # 计算成本
            token_info = self.token_calculator.token_extractor.extract_from_response(accumulated_data, model)
            cost = self.token_calculator.calculate_cost(
                token_info.get('prompt_tokens', 0),
                token_info.get('completion_tokens', 0),
                model
            )
            
            # 记录响应信息
            self.agently_logger.log_response(str(accumulated_data), token_usage, cost)
            
            # 发送完成事件
            yield {
                'type': 'instant_complete',
                'content': accumulated_data,
                'model': model,
                'response_time': response_time,
                'token_usage': token_info,
                'cost': cost
            }
                    
        except Exception as e:
            self.agently_logger.log_error(e, "JSON Instant流式调用")
            raise
    
    def _handle_json_generator(self, agent, prompt: str, model: str, start_time: float, instant_keys: Optional[List[str]] = None, **kwargs) -> Generator[Dict[str, Any], None, None]:
        """
        处理JSON模式的Generator调用（基于Agently官方Instant模式的Generator API）
        
        Args:
            agent: Agently代理实例
            prompt: 提示词
            model: 模型名称
            start_time: 开始时间
            instant_keys: 需要实时监听的特定key列表
            **kwargs: 其他参数
            
        Yields:
            Generator格式的JSON响应
        """
        try:
            # 存储累积数据
            accumulated_data = {}
            
            # 根据官方文档，使用get_generator(content="instant")获取流式解析生成器
            generator = agent.input(prompt).get_generator(content="instant")
            
            # 遍历生成器数据
            for instant_message in generator:
                # instant_message是StreamingData对象，直接访问属性
                path = getattr(instant_message, 'path', '').split('.') if hasattr(instant_message, 'path') else []
                delta = getattr(instant_message, 'delta', '')
                value = getattr(instant_message, 'value', '')
                is_complete = getattr(instant_message, 'is_complete', False)
                
                # 获取当前字段名（path的最后一个元素）
                current_key = path[-1] if path else ''
                
                # 如果指定了特定key，只处理这些key的更新
                if instant_keys and current_key not in instant_keys:
                    continue
                
                # 更新累积数据
                if current_key:
                    if delta:  # 增量更新
                        if current_key not in accumulated_data:
                            accumulated_data[current_key] = delta
                        else:
                            # 如果是字符串类型，进行累积
                            if isinstance(accumulated_data[current_key], str) and isinstance(delta, str):
                                accumulated_data[current_key] += delta
                            else:
                                accumulated_data[current_key] = delta
                    elif value:  # 完整值更新
                        accumulated_data[current_key] = value
                
                # 构建事件结果
                event_result = {
                    'type': 'generator_delta',
                    'key': current_key,
                    'delta': delta,
                    'value': value,
                    'path': path,
                    'accumulated_content': accumulated_data.copy(),
                    'model': model,
                    'timestamp': time.time(),
                    'is_complete': is_complete
                }
                
                yield event_result
                
                # 如果完成，退出循环
                if is_complete:
                    break
            
            # 计算响应时间
            response_time = time.time() - start_time
            
            # 使用累积数据作为最终结果
            final_data = accumulated_data
            
            # 获取token使用信息
            token_usage = self.token_calculator.get_token_usage(
                prompt, str(final_data), final_data, model, 
                MODEL_CONFIGS[model]['base_url']
            )
            
            # 计算成本
            token_info = self.token_calculator.token_extractor.extract_from_response(final_data, model)
            cost = self.token_calculator.calculate_cost(
                token_info.get('prompt_tokens', 0),
                token_info.get('completion_tokens', 0),
                model
            )
            
            # 记录响应信息
            self.agently_logger.log_response(str(final_data), token_usage, cost)
            
            # 发送完成事件
            yield {
                'type': 'generator_complete',
                'content': final_data,
                'model': model,
                'response_time': response_time,
                'token_usage': token_info,
                'cost': cost
            }
                    
        except Exception as e:
            self.agently_logger.log_error(e, "JSON Generator调用")
            raise
    
    def call_with_thinking(self, model: str, prompt: str, 
                          call_mode: str = "native", 
                          output_schema: Optional[Dict] = None,
                          stream: bool = False,
                          instant_keys: Optional[List[str]] = None,
                          use_generator: bool = False,
                          **kwargs) -> Union[AsyncGenerator[Dict[str, Any], None], Iterator[Dict[str, Any]], Generator[Dict[str, Any], None, None], Dict[str, Any]]:
        """
        调用思考模型的统一接口
        
        Args:
            model: 模型名称
            prompt: 提示词
            call_mode: 调用模式
                - "native": 原生模式，返回reasoning_content和content
                - "json": JSON模式，按指定schema输出（需要提供output_schema）
            output_schema: JSON输出模式（仅在call_mode="json"时需要）
            stream: 是否流式输出
            instant_keys: 需要实时监听的特定key列表（仅JSON模式）
            use_generator: 是否使用generator模式（仅JSON模式）
            **kwargs: 其他参数
            
        Returns:
            思考模型的响应
            
        Raises:
            ValueError: 参数错误时抛出
            RuntimeError: 模型不支持思考功能时抛出
            ConnectionError: 网络连接失败时抛出
            TimeoutError: 请求超时时抛出
            Exception: 其他调用失败时抛出
        """
        # 参数验证
        if not model:
            raise ValueError("模型名称不能为空")
        if not prompt or not isinstance(prompt, str):
            raise ValueError("提示词不能为空且必须是字符串格式")
        if call_mode not in ["native", "json"]:
            raise ValueError(f"不支持的调用模式: {call_mode}。支持的模式: 'native', 'json'")
        
        # 验证模型是否支持思考功能
        if not self.validate_model(model):
            raise ValueError(f"模型 {model} 不可用")
        
        try:
            if call_mode == "native":
                # 原生模式：直接调用思考模型，获取reasoning_content和content
                messages = [{"role": "user", "content": prompt}]
                return self.call_native(
                    model, messages, 
                    stream=stream,
                    model_type=ModelType.REASONING,
                    **kwargs
                )
                    
            elif call_mode == "json":
                # JSON模式：思考模型按指定schema输出结构化数据
                if not output_schema:
                    raise ValueError("JSON模式需要提供output_schema参数")
                if not isinstance(output_schema, dict):
                    raise ValueError("output_schema必须是字典格式")
                    
                return self.call_json(
                    model,
                    prompt,
                    stream=stream,
                    model_type=ModelType.REASONING,
                    output_schema=output_schema,
                    instant_keys=instant_keys,
                    use_generator=use_generator,
                    **kwargs
                )
                    
        except (ConnectionError, TimeoutError, ValueError, RuntimeError) as e:
            # 重新抛出已知异常类型
            raise
        except Exception as e:
            error_msg = f"调用思考模型 {model} 失败: {str(e)}"
            self.logger.error(error_msg)
            raise Exception(error_msg)
    
    def _parse_thinking_response(self, content: str) -> Dict[str, str]:
        """
        解析普通模型模拟的思考响应（已废弃）
        
        注意：此方法用于解析普通模型模拟的思考格式，不适用于真正的思考模型。
        真正的思考模型（如ERNIE-X1）会直接在reasoning_content字段返回思考过程。
        
        Args:
            content: 响应内容
            
        Returns:
            包含thinking和content字段的字典
        """
        try:
            # 尝试解析JSON格式
            import json
            parsed = json.loads(content)
            if isinstance(parsed, dict) and ('thinking' in parsed or 'content' in parsed):
                return {
                    'thinking': parsed.get('thinking', ''),
                    'content': parsed.get('content', '')
                }
        except (json.JSONDecodeError, ValueError):
            pass
        
        # 尝试解析特定标记格式
        thinking_content = ""
        final_content = ""
        
        # 查找thinking标记
        if "thinking:" in content.lower():
            parts = content.split("thinking:", 1)
            if len(parts) > 1:
                remaining = parts[1]
                # 查找answer或content标记
                for marker in ["answer:", "content:", "回答:", "答案:"]:
                    if marker in remaining.lower():
                        thinking_part, answer_part = remaining.split(marker, 1)
                        thinking_content = thinking_part.strip()
                        final_content = answer_part.strip()
                        break
                else:
                    thinking_content = remaining.strip()
        
        # 如果没有找到标记，将整个内容作为最终答案
        if not thinking_content and not final_content:
            final_content = content
        
        return {
            'thinking': thinking_content,
            'content': final_content
        }
    
    def call(self, model: str, messages: Union[List[Dict[str, str]], str], 
             output_mode: OutputMode = OutputMode.NATIVE,
             model_type: ModelType = ModelType.NORMAL,
             stream: bool = False,
             output_schema: Optional[Dict] = None,
             instant_keys: Optional[List[str]] = None,
             use_generator: bool = False,
             **kwargs) -> Union[AsyncGenerator[Dict[str, Any], None], Iterator[Dict[str, Any]], Generator[Dict[str, Any], None, None], Dict[str, Any]]:
        """
        统一的模型调用接口
        
        Args:
            model: 模型名称
            messages: 消息列表或提示词字符串
            output_mode: 输出模式（NATIVE或JSON）
            model_type: 模型类型（NORMAL或REASONING）
            stream: 是否流式输出
            output_schema: JSON输出模式（仅在JSON模式下使用）
            instant_keys: 需要实时监听的特定key列表（仅JSON模式）
            use_generator: 是否使用generator模式（仅JSON模式）
            **kwargs: 其他参数
            
        Returns:
            模型响应
            
        Raises:
            ValueError: 参数错误时抛出
            RuntimeError: 模型或服务不可用时抛出
            ConnectionError: 网络连接失败时抛出
            TimeoutError: 请求超时时抛出
            Exception: 其他调用失败时抛出
            
        重要说明：
        - JSON模式完全基于Agently库实现，支持Instant流式解析
        - 原生模式依赖OpenAI标准方法，不涉及JSON格式化
        - 两种模式职责分离，各司其职
        """
        # 参数验证
        if not model:
            raise ValueError("模型名称不能为空")
        if not messages:
            raise ValueError("消息内容不能为空")
        if not isinstance(output_mode, OutputMode):
            raise ValueError(f"不支持的输出模式: {output_mode}")
        if not isinstance(model_type, ModelType):
            raise ValueError(f"不支持的模型类型: {model_type}")
        
        try:
            if output_mode == OutputMode.NATIVE:
                # 如果传入的是字符串，转换为消息格式
                if isinstance(messages, str):
                    messages = [{"role": "user", "content": messages}]
                elif not isinstance(messages, list):
                    raise ValueError("原生模式下messages必须是列表或字符串格式")
                return self.call_native(model, messages, stream, model_type, **kwargs)
                
            elif output_mode == OutputMode.JSON:
                # JSON模式只接受字符串提示词
                if isinstance(messages, list):
                    # 简单提取最后一条用户消息
                    prompt = messages[-1].get("content", "") if messages else ""
                    if not prompt:
                        raise ValueError("无法从消息列表中提取有效的提示词")
                else:
                    prompt = messages
                    if not isinstance(prompt, str):
                        raise ValueError("JSON模式下messages必须是字符串或包含有效内容的消息列表")
                        
                return self.call_json(
                    model, prompt, stream, model_type, 
                    output_schema=output_schema,
                    instant_keys=instant_keys,
                    use_generator=use_generator,
                    **kwargs
                )
            else:
                raise ValueError(f"不支持的输出模式: {output_mode}")
                
        except (ConnectionError, TimeoutError, ValueError, RuntimeError) as e:
            # 重新抛出已知异常类型
            raise
        except Exception as e:
            error_msg = f"统一调用接口失败: {str(e)}"
            self.logger.error(error_msg)
            raise Exception(error_msg)
            