#!/usr/bin/env python3
"""
HarborAI å¼‚æ­¥è°ƒç”¨ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨HarborAIè¿›è¡Œå¼‚æ­¥è°ƒç”¨ï¼Œ
æå‡å¹¶å‘æ€§èƒ½ï¼Œé€‚åˆéœ€è¦åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚çš„åœºæ™¯ã€‚

åœºæ™¯æè¿°:
- å¼‚æ­¥/ç­‰å¾…è¯­æ³•ä½¿ç”¨
- å¹¶å‘è¯·æ±‚å¤„ç†
- æ€§èƒ½å¯¹æ¯”å±•ç¤º

åº”ç”¨ä»·å€¼:
- æå‡åº”ç”¨å“åº”é€Ÿåº¦
- ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡
- æ”¯æŒé«˜å¹¶å‘åœºæ™¯
"""

import os
import time
import asyncio
from typing import List, Dict, Any
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

try:
    from harborai import HarborAI
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£… HarborAI: pip install harborai")
    exit(1)


def create_client() -> HarborAI:
    """
    åˆ›å»ºHarborAIå®¢æˆ·ç«¯
    
    Returns:
        HarborAI: é…ç½®å¥½çš„å®¢æˆ·ç«¯å®ä¾‹
    """
    api_key = os.getenv("DEEPSEEK_API_KEY")
    base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
    
    if not api_key:
        raise ValueError("è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® DEEPSEEK_API_KEY")
    
    return HarborAI(
        api_key=api_key,
        base_url=base_url
    )


async def async_chat_single(client: HarborAI, question: str, model: str = "deepseek-chat") -> Dict[str, Any]:
    """
    å•ä¸ªå¼‚æ­¥èŠå¤©è°ƒç”¨
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        question: ç”¨æˆ·é—®é¢˜
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        
    Returns:
        Dict: åŒ…å«å“åº”å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    start_time = time.time()
    
    try:
        # å¼‚æ­¥è°ƒç”¨æ¨¡å‹
        response = await client.chat.completions.acreate(
            model=model,
            messages=[{"role": "user", "content": question}],
            temperature=0.7,
            max_tokens=500
        )
        
        elapsed_time = time.time() - start_time
        answer = response.choices[0].message.content
        usage = response.usage
        
        return {
            "success": True,
            "question": question,
            "answer": answer,
            "usage": usage,
            "elapsed_time": elapsed_time,
            "model": model
        }
        
    except Exception as e:
        return {
            "success": False,
            "question": question,
            "error": str(e),
            "elapsed_time": time.time() - start_time,
            "model": model
        }


def sync_chat_single(client: HarborAI, question: str, model: str = "deepseek-chat") -> Dict[str, Any]:
    """
    å•ä¸ªåŒæ­¥èŠå¤©è°ƒç”¨
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        question: ç”¨æˆ·é—®é¢˜
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        
    Returns:
        Dict: åŒ…å«å“åº”å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    start_time = time.time()
    
    try:
        # åŒæ­¥è°ƒç”¨æ¨¡å‹
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": question}],
            temperature=0.7,
            max_tokens=500
        )
        
        elapsed_time = time.time() - start_time
        answer = response.choices[0].message.content
        usage = response.usage
        
        return {
            "success": True,
            "question": question,
            "answer": answer,
            "usage": usage,
            "elapsed_time": elapsed_time,
            "model": model
        }
        
    except Exception as e:
        return {
            "success": False,
            "question": question,
            "error": str(e),
            "elapsed_time": time.time() - start_time,
            "model": model
        }


async def async_batch_processing(client: HarborAI, questions: List[str], model: str = "deepseek-chat") -> List[Dict[str, Any]]:
    """
    å¼‚æ­¥æ‰¹é‡å¤„ç†
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        questions: é—®é¢˜åˆ—è¡¨
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        
    Returns:
        List[Dict]: æ‰€æœ‰å“åº”ç»“æœ
    """
    print(f"\nğŸš€ å¼€å§‹å¼‚æ­¥æ‰¹é‡å¤„ç† {len(questions)} ä¸ªè¯·æ±‚...")
    start_time = time.time()
    
    # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
    tasks = [async_chat_single(client, question, model) for question in questions]
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    total_time = time.time() - start_time
    
    # å¤„ç†ç»“æœ
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            processed_results.append({
                "success": False,
                "question": questions[i],
                "error": str(result),
                "model": model
            })
        else:
            processed_results.append(result)
    
    print(f"âœ… å¼‚æ­¥æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    return processed_results


def sync_batch_processing(client: HarborAI, questions: List[str], model: str = "deepseek-chat") -> List[Dict[str, Any]]:
    """
    åŒæ­¥æ‰¹é‡å¤„ç†
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        questions: é—®é¢˜åˆ—è¡¨
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        
    Returns:
        List[Dict]: æ‰€æœ‰å“åº”ç»“æœ
    """
    print(f"\nğŸŒ å¼€å§‹åŒæ­¥æ‰¹é‡å¤„ç† {len(questions)} ä¸ªè¯·æ±‚...")
    start_time = time.time()
    
    results = []
    for question in questions:
        result = sync_chat_single(client, question, model)
        results.append(result)
    
    total_time = time.time() - start_time
    print(f"âœ… åŒæ­¥æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    return results


def print_results(results: List[Dict[str, Any]], title: str):
    """
    æ‰“å°ç»“æœç»Ÿè®¡
    
    Args:
        results: ç»“æœåˆ—è¡¨
        title: æ ‡é¢˜
    """
    print(f"\nğŸ“Š {title}")
    print("-" * 50)
    
    successful_results = [r for r in results if r["success"]]
    failed_results = [r for r in results if not r["success"]]
    
    if successful_results:
        total_time = sum(r["elapsed_time"] for r in successful_results)
        avg_time = total_time / len(successful_results)
        total_tokens = sum(r["usage"].total_tokens for r in successful_results)
        
        print(f"âœ… æˆåŠŸ: {len(successful_results)}/{len(results)}")
        print(f"â±ï¸  å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’")
        print(f"ğŸ¯ æ€»tokens: {total_tokens}")
        
        # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
        for i, result in enumerate(successful_results[:3]):
            print(f"\nğŸ“ ç»“æœ {i+1}:")
            print(f"   é—®é¢˜: {result['question'][:50]}...")
            print(f"   å›ç­”: {result['answer'][:100]}...")
            print(f"   è€—æ—¶: {result['elapsed_time']:.2f}ç§’")
    
    if failed_results:
        print(f"\nâŒ å¤±è´¥: {len(failed_results)}")
        for result in failed_results:
            print(f"   {result['question'][:50]}... -> {result['error']}")


async def performance_comparison(client: HarborAI):
    """
    æ€§èƒ½å¯¹æ¯”æµ‹è¯•
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
    """
    print("\n" + "="*60)
    print("âš¡ å¼‚æ­¥ vs åŒæ­¥æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
        "Pythonæœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯äº‘è®¡ç®—ï¼Ÿ",
        "åŒºå—é“¾æŠ€æœ¯çš„åº”ç”¨åœºæ™¯æœ‰å“ªäº›ï¼Ÿ"
    ]
    
    print(f"ğŸ“‹ æµ‹è¯•é—®é¢˜æ•°é‡: {len(test_questions)}")
    
    # åŒæ­¥å¤„ç†
    sync_results = sync_batch_processing(client, test_questions)
    sync_total_time = sum(r["elapsed_time"] for r in sync_results if r["success"])
    
    # å¼‚æ­¥å¤„ç†
    async_results = await async_batch_processing(client, test_questions)
    async_total_time = sum(r["elapsed_time"] for r in async_results if r["success"])
    
    # æ‰“å°ç»“æœ
    print_results(sync_results, "åŒæ­¥å¤„ç†ç»“æœ")
    print_results(async_results, "å¼‚æ­¥å¤„ç†ç»“æœ")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ† æ€§èƒ½å¯¹æ¯”æ€»ç»“:")
    print("-" * 30)
    print(f"åŒæ­¥æ€»è€—æ—¶: {sync_total_time:.2f}ç§’")
    print(f"å¼‚æ­¥æ€»è€—æ—¶: {async_total_time:.2f}ç§’")
    
    if async_total_time > 0:
        improvement = ((sync_total_time - async_total_time) / async_total_time) * 100
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")
    
    return sync_results, async_results


async def concurrent_different_models(client: HarborAI):
    """
    å¹¶å‘è°ƒç”¨ä¸åŒæ¨¡å‹
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
    """
    print("\n" + "="*60)
    print("ğŸ”„ å¹¶å‘è°ƒç”¨ä¸åŒæ¨¡å‹ç¤ºä¾‹")
    print("="*60)
    
    question = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"
    
    # å‡†å¤‡ä¸åŒçš„å®¢æˆ·ç«¯å’Œæ¨¡å‹
    tasks = []
    
    # DeepSeekæ¨¡å‹
    tasks.append(async_chat_single(client, question, "deepseek-chat"))
    
    # å¦‚æœé…ç½®äº†å…¶ä»–æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥å¹¶å‘è°ƒç”¨
    if os.getenv("OPENAI_API_KEY"):
        openai_client = HarborAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        )
        tasks.append(async_chat_single(openai_client, question, "gpt-3.5-turbo"))
    
    # å¹¶å‘æ‰§è¡Œ
    start_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    total_time = time.time() - start_time
    
    print(f"âœ… å¹¶å‘è°ƒç”¨å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    # æ˜¾ç¤ºç»“æœ
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"\nâŒ æ¨¡å‹ {i+1} è°ƒç”¨å¤±è´¥: {result}")
        elif result["success"]:
            print(f"\nâœ… æ¨¡å‹: {result['model']}")
            print(f"   è€—æ—¶: {result['elapsed_time']:.2f}ç§’")
            print(f"   å›ç­”: {result['answer'][:200]}...")
        else:
            print(f"\nâŒ æ¨¡å‹: {result['model']} è°ƒç”¨å¤±è´¥: {result['error']}")


async def rate_limited_requests(client: HarborAI, questions: List[str], rate_limit: int = 2):
    """
    é™æµå¼‚æ­¥è¯·æ±‚ç¤ºä¾‹
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        questions: é—®é¢˜åˆ—è¡¨
        rate_limit: æ¯ç§’æœ€å¤§è¯·æ±‚æ•°
    """
    print(f"\nğŸš¦ é™æµå¼‚æ­¥è¯·æ±‚ç¤ºä¾‹ (æ¯ç§’æœ€å¤š {rate_limit} ä¸ªè¯·æ±‚)")
    print("-" * 50)
    
    semaphore = asyncio.Semaphore(rate_limit)
    
    async def rate_limited_call(question: str):
        async with semaphore:
            result = await async_chat_single(client, question)
            await asyncio.sleep(1 / rate_limit)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡
            return result
    
    start_time = time.time()
    tasks = [rate_limited_call(q) for q in questions]
    results = await asyncio.gather(*tasks)
    total_time = time.time() - start_time
    
    print(f"âœ… é™æµè¯·æ±‚å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"ğŸ“Š å¹³å‡è¯·æ±‚é—´éš”: {total_time/len(questions):.2f}ç§’")
    
    return results


async def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸš€ HarborAI å¼‚æ­¥è°ƒç”¨ç¤ºä¾‹")
    print("="*60)
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = create_client()
        print("âœ… HarborAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # æ€§èƒ½å¯¹æ¯”æµ‹è¯•
        await performance_comparison(client)
        
        # å¹¶å‘è°ƒç”¨ä¸åŒæ¨¡å‹
        await concurrent_different_models(client)
        
        # é™æµè¯·æ±‚ç¤ºä¾‹
        rate_limit_questions = [
            "ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿ",
            "Python asyncioçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•ä¼˜åŒ–APIè°ƒç”¨æ€§èƒ½ï¼Ÿ"
        ]
        await rate_limited_requests(client, rate_limit_questions)
        
        print(f"\nğŸ‰ æ‰€æœ‰å¼‚æ­¥è°ƒç”¨ç¤ºä¾‹æ‰§è¡Œå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print("\nğŸ’¡ è¯·æ£€æŸ¥:")
        print("1. æ˜¯å¦æ­£ç¡®é…ç½®äº†ç¯å¢ƒå˜é‡")
        print("2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ")


if __name__ == "__main__":
    asyncio.run(main())