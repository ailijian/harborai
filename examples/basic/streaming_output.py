#!/usr/bin/env python3
"""
HarborAI æµå¼è¾“å‡ºç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨HarborAIçš„æµå¼è¾“å‡ºåŠŸèƒ½ï¼Œ
å®ç°å®æ—¶å“åº”æ˜¾ç¤ºï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

åœºæ™¯æè¿°:
- æµå¼å“åº”å¤„ç†
- å®æ—¶å†…å®¹æ˜¾ç¤º
- æ‰“å­—æœºæ•ˆæœå®ç°

åº”ç”¨ä»·å€¼:
- æå‡ç”¨æˆ·ä½“éªŒ
- é™ä½æ„ŸçŸ¥å»¶è¿Ÿ
- æ”¯æŒé•¿æ–‡æœ¬ç”Ÿæˆ
"""

import os
import time
import asyncio
import sys
from typing import Iterator, AsyncIterator, Dict, Any
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

try:
    from harborai import HarborAI
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£… HarborAI: pip install harborai")
    exit(1)


def create_client() -> HarborAI:
    """
    åˆ›å»ºHarborAIå®¢æˆ·ç«¯
    
    Returns:
        HarborAI: é…ç½®å¥½çš„å®¢æˆ·ç«¯å®ä¾‹
    """
    api_key = os.getenv("DEEPSEEK_API_KEY")
    base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
    
    if not api_key:
        raise ValueError("è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® DEEPSEEK_API_KEY")
    
    return HarborAI(
        api_key=api_key,
        base_url=base_url
    )


def stream_chat_sync(client: HarborAI, question: str, model: str = "deepseek-chat") -> Dict[str, Any]:
    """
    åŒæ­¥æµå¼èŠå¤©
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        question: ç”¨æˆ·é—®é¢˜
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        
    Returns:
        Dict: åŒ…å«å®Œæ•´å“åº”å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    print(f"\nğŸ¤– AIæ­£åœ¨æ€è€ƒ: {question}")
    print("ğŸ’­ å›ç­”: ", end="", flush=True)
    
    start_time = time.time()
    full_response = ""
    chunk_count = 0
    
    try:
        # åˆ›å»ºæµå¼è¯·æ±‚
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": question}],
            temperature=0.7,
            max_tokens=1000,
            stream=True  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        # å¤„ç†æµå¼å“åº”
        for chunk in stream:
            chunk_count += 1
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                
                # å®æ—¶æ˜¾ç¤ºå†…å®¹ï¼ˆæ‰“å­—æœºæ•ˆæœï¼‰
                print(content, end="", flush=True)
                
                # æ·»åŠ å°å»¶è¿Ÿä»¥æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ
                time.sleep(0.02)
        
        elapsed_time = time.time() - start_time
        print(f"\n\nâœ… æµå¼å“åº”å®Œæˆ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - å“åº”æ—¶é—´: {elapsed_time:.2f}ç§’")
        print(f"   - æ•°æ®å—æ•°: {chunk_count}")
        print(f"   - å“åº”é•¿åº¦: {len(full_response)}å­—ç¬¦")
        
        return {
            "success": True,
            "question": question,
            "answer": full_response,
            "elapsed_time": elapsed_time,
            "chunk_count": chunk_count,
            "model": model
        }
        
    except Exception as e:
        print(f"\nâŒ æµå¼è°ƒç”¨å¤±è´¥: {e}")
        return {
            "success": False,
            "question": question,
            "error": str(e),
            "elapsed_time": time.time() - start_time,
            "model": model
        }


async def stream_chat_async(client: HarborAI, question: str, model: str = "deepseek-chat") -> Dict[str, Any]:
    """
    å¼‚æ­¥æµå¼èŠå¤©
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        question: ç”¨æˆ·é—®é¢˜
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        
    Returns:
        Dict: åŒ…å«å®Œæ•´å“åº”å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    print(f"\nğŸ¤– AIæ­£åœ¨å¼‚æ­¥æ€è€ƒ: {question}")
    print("ğŸ’­ å›ç­”: ", end="", flush=True)
    
    start_time = time.time()
    full_response = ""
    chunk_count = 0
    
    try:
        # åˆ›å»ºå¼‚æ­¥æµå¼è¯·æ±‚
        stream = await client.chat.completions.acreate(
            model=model,
            messages=[{"role": "user", "content": question}],
            temperature=0.7,
            max_tokens=1000,
            stream=True  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        # å¤„ç†å¼‚æ­¥æµå¼å“åº”
        async for chunk in stream:
            chunk_count += 1
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                
                # å®æ—¶æ˜¾ç¤ºå†…å®¹
                print(content, end="", flush=True)
                
                # å¼‚æ­¥å»¶è¿Ÿ
                await asyncio.sleep(0.02)
        
        elapsed_time = time.time() - start_time
        print(f"\n\nâœ… å¼‚æ­¥æµå¼å“åº”å®Œæˆ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - å“åº”æ—¶é—´: {elapsed_time:.2f}ç§’")
        print(f"   - æ•°æ®å—æ•°: {chunk_count}")
        print(f"   - å“åº”é•¿åº¦: {len(full_response)}å­—ç¬¦")
        
        return {
            "success": True,
            "question": question,
            "answer": full_response,
            "elapsed_time": elapsed_time,
            "chunk_count": chunk_count,
            "model": model
        }
        
    except Exception as e:
        print(f"\nâŒ å¼‚æ­¥æµå¼è°ƒç”¨å¤±è´¥: {e}")
        return {
            "success": False,
            "question": question,
            "error": str(e),
            "elapsed_time": time.time() - start_time,
            "model": model
        }


def stream_with_progress(client: HarborAI, question: str, model: str = "deepseek-chat") -> Dict[str, Any]:
    """
    å¸¦è¿›åº¦æŒ‡ç¤ºçš„æµå¼è¾“å‡º
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        question: ç”¨æˆ·é—®é¢˜
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        
    Returns:
        Dict: åŒ…å«å®Œæ•´å“åº”å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    print(f"\nğŸ¯ å¸¦è¿›åº¦çš„æµå¼å“åº”: {question}")
    
    start_time = time.time()
    full_response = ""
    chunk_count = 0
    progress_chars = ["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "]
    progress_index = 0
    
    try:
        # æ˜¾ç¤ºåˆå§‹è¿›åº¦
        print("ğŸ”„ æ­£åœ¨ç”Ÿæˆå›ç­”... ", end="", flush=True)
        
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": question}],
            temperature=0.7,
            max_tokens=1000,
            stream=True
        )
        
        # æ¸…é™¤è¿›åº¦æŒ‡ç¤ºå™¨
        print("\r" + " " * 50 + "\r", end="", flush=True)
        print("ğŸ’­ å›ç­”: ", end="", flush=True)
        
        for chunk in stream:
            chunk_count += 1
            
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                print(content, end="", flush=True)
            
            # æ›´æ–°è¿›åº¦æŒ‡ç¤ºå™¨ï¼ˆåœ¨æ²¡æœ‰å†…å®¹æ—¶æ˜¾ç¤ºï¼‰
            else:
                progress_char = progress_chars[progress_index % len(progress_chars)]
                print(f"\rğŸ’­ å›ç­”: {full_response}{progress_char}", end="", flush=True)
                progress_index += 1
                time.sleep(0.1)
        
        elapsed_time = time.time() - start_time
        print(f"\n\nâœ… å¸¦è¿›åº¦çš„æµå¼å“åº”å®Œæˆ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - å“åº”æ—¶é—´: {elapsed_time:.2f}ç§’")
        print(f"   - æ•°æ®å—æ•°: {chunk_count}")
        print(f"   - å“åº”é•¿åº¦: {len(full_response)}å­—ç¬¦")
        
        return {
            "success": True,
            "question": question,
            "answer": full_response,
            "elapsed_time": elapsed_time,
            "chunk_count": chunk_count,
            "model": model
        }
        
    except Exception as e:
        print(f"\nâŒ å¸¦è¿›åº¦çš„æµå¼è°ƒç”¨å¤±è´¥: {e}")
        return {
            "success": False,
            "question": question,
            "error": str(e),
            "elapsed_time": time.time() - start_time,
            "model": model
        }


def compare_streaming_vs_normal(client: HarborAI, question: str, model: str = "deepseek-chat"):
    """
    å¯¹æ¯”æµå¼è¾“å‡ºä¸æ™®é€šè¾“å‡º
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        question: ç”¨æˆ·é—®é¢˜
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
    """
    print("\n" + "="*60)
    print("âš¡ æµå¼è¾“å‡º vs æ™®é€šè¾“å‡ºå¯¹æ¯”")
    print("="*60)
    
    # æ™®é€šè¾“å‡º
    print("\nğŸŒ æ™®é€šè¾“å‡ºæ¨¡å¼:")
    start_time = time.time()
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": question}],
            temperature=0.7,
            max_tokens=1000,
            stream=False
        )
        
        normal_time = time.time() - start_time
        answer = response.choices[0].message.content
        
        print(f"â±ï¸  ç­‰å¾…æ—¶é—´: {normal_time:.2f}ç§’")
        print(f"ğŸ’­ å®Œæ•´å›ç­”: {answer[:200]}...")
        
    except Exception as e:
        print(f"âŒ æ™®é€šè°ƒç”¨å¤±è´¥: {e}")
        return
    
    # æµå¼è¾“å‡º
    print(f"\nğŸš€ æµå¼è¾“å‡ºæ¨¡å¼:")
    stream_result = stream_chat_sync(client, question, model)
    
    if stream_result["success"]:
        # å¯¹æ¯”åˆ†æ
        print(f"\nğŸ“Š å¯¹æ¯”åˆ†æ:")
        print(f"   æ™®é€šæ¨¡å¼æ€»æ—¶é—´: {normal_time:.2f}ç§’")
        print(f"   æµå¼æ¨¡å¼æ€»æ—¶é—´: {stream_result['elapsed_time']:.2f}ç§’")
        print(f"   æµå¼æ•°æ®å—æ•°: {stream_result['chunk_count']}")
        print(f"   ç”¨æˆ·ä½“éªŒæå‡: å®æ—¶åé¦ˆ vs ç­‰å¾…{normal_time:.1f}ç§’")


async def multiple_concurrent_streams(client: HarborAI, questions: list, model: str = "deepseek-chat"):
    """
    å¤šä¸ªå¹¶å‘æµå¼è¯·æ±‚
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        questions: é—®é¢˜åˆ—è¡¨
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
    """
    print("\n" + "="*60)
    print("ğŸ”„ å¤šä¸ªå¹¶å‘æµå¼è¯·æ±‚ç¤ºä¾‹")
    print("="*60)
    
    async def stream_with_id(question: str, stream_id: int):
        """å¸¦IDçš„æµå¼å¤„ç†"""
        print(f"\nğŸ¯ æµ {stream_id}: {question}")
        print(f"ğŸ’­ å›ç­” {stream_id}: ", end="", flush=True)
        
        try:
            stream = await client.chat.completions.acreate(
                model=model,
                messages=[{"role": "user", "content": question}],
                temperature=0.7,
                max_tokens=500,
                stream=True
            )
            
            full_response = ""
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    print(content, end="", flush=True)
                    await asyncio.sleep(0.01)
            
            print(f"\nâœ… æµ {stream_id} å®Œæˆ")
            return {"id": stream_id, "question": question, "answer": full_response}
            
        except Exception as e:
            print(f"\nâŒ æµ {stream_id} å¤±è´¥: {e}")
            return {"id": stream_id, "question": question, "error": str(e)}
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = [stream_with_id(q, i+1) for i, q in enumerate(questions)]
    
    # å¹¶å‘æ‰§è¡Œ
    start_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    total_time = time.time() - start_time
    
    print(f"\nğŸ‰ æ‰€æœ‰å¹¶å‘æµå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    return results


def interactive_streaming_chat(client: HarborAI, model: str = "deepseek-chat"):
    """
    äº¤äº’å¼æµå¼èŠå¤©
    
    Args:
        client: HarborAIå®¢æˆ·ç«¯
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
    """
    print("\n" + "="*60)
    print("ğŸ’¬ äº¤äº’å¼æµå¼èŠå¤©")
    print("="*60)
    print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºèŠå¤©")
    print("ğŸ’¡ è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    
    conversation_history = []
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if user_input.lower() in ['clear', 'æ¸…ç©º']:
                conversation_history = []
                print("ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            
            if not user_input:
                continue
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            conversation_history.append({"role": "user", "content": user_input})
            
            # æµå¼å“åº”
            print("ğŸ¤– AI: ", end="", flush=True)
            
            stream = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )
            
            ai_response = ""
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    ai_response += content
                    print(content, end="", flush=True)
                    time.sleep(0.02)
            
            # æ·»åŠ AIå“åº”åˆ°å†å²
            conversation_history.append({"role": "assistant", "content": ai_response})
            
            # é™åˆ¶å†å²é•¿åº¦
            if len(conversation_history) > 10:
                conversation_history = conversation_history[-10:]
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ èŠå¤©è¢«ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ èŠå¤©å‡ºé”™: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸŒŠ HarborAI æµå¼è¾“å‡ºç¤ºä¾‹")
    print("="*60)
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = create_client()
        print("âœ… HarborAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼ŒåŒ…æ‹¬å…¶å‘å±•å†ç¨‹å’Œä¸»è¦åº”ç”¨é¢†åŸŸã€‚",
            "ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‰né¡¹ã€‚",
            "æè¿°æœºå™¨å­¦ä¹ ä¸­ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«ã€‚"
        ]
        
        # 1. åŒæ­¥æµå¼è¾“å‡º
        print("\nğŸ”¹ 1. åŒæ­¥æµå¼è¾“å‡ºç¤ºä¾‹")
        stream_chat_sync(client, test_questions[0])
        
        # 2. å¼‚æ­¥æµå¼è¾“å‡º
        print("\nğŸ”¹ 2. å¼‚æ­¥æµå¼è¾“å‡ºç¤ºä¾‹")
        await stream_chat_async(client, test_questions[1])
        
        # 3. å¸¦è¿›åº¦çš„æµå¼è¾“å‡º
        print("\nğŸ”¹ 3. å¸¦è¿›åº¦æŒ‡ç¤ºçš„æµå¼è¾“å‡º")
        stream_with_progress(client, test_questions[2])
        
        # 4. æµå¼ vs æ™®é€šè¾“å‡ºå¯¹æ¯”
        print("\nğŸ”¹ 4. æµå¼è¾“å‡ºä¸æ™®é€šè¾“å‡ºå¯¹æ¯”")
        compare_streaming_vs_normal(client, "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
        
        # 5. å¤šä¸ªå¹¶å‘æµå¼è¯·æ±‚
        print("\nğŸ”¹ 5. å¤šä¸ªå¹¶å‘æµå¼è¯·æ±‚")
        concurrent_questions = [
            "ä»€ä¹ˆæ˜¯äº‘è®¡ç®—ï¼Ÿ",
            "è§£é‡ŠåŒºå—é“¾æŠ€æœ¯",
            "Pythonçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        await multiple_concurrent_streams(client, concurrent_questions)
        
        # 6. äº¤äº’å¼æµå¼èŠå¤©
        print("\nğŸ”¹ 6. äº¤äº’å¼æµå¼èŠå¤©")
        choice = input("æ˜¯å¦å¼€å§‹äº¤äº’å¼èŠå¤©ï¼Ÿ(y/n): ").strip().lower()
        if choice in ['y', 'yes', 'æ˜¯']:
            interactive_streaming_chat(client)
        
        print(f"\nğŸ‰ æ‰€æœ‰æµå¼è¾“å‡ºç¤ºä¾‹æ‰§è¡Œå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print("\nğŸ’¡ è¯·æ£€æŸ¥:")
        print("1. æ˜¯å¦æ­£ç¡®é…ç½®äº†ç¯å¢ƒå˜é‡")
        print("2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ")


if __name__ == "__main__":
    asyncio.run(main())