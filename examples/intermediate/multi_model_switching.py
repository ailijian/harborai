#!/usr/bin/env python3
"""
HarborAI å¤šæ¨¡å‹åˆ‡æ¢ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨HarborAIä¸­åŠ¨æ€åˆ‡æ¢ä¸åŒçš„AIæ¨¡å‹ï¼Œ
æ ¹æ®ä»»åŠ¡ç±»å‹ã€æˆæœ¬è€ƒè™‘æˆ–æ€§èƒ½éœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹ã€‚

åœºæ™¯æè¿°:
- åŠ¨æ€æ¨¡å‹é€‰æ‹©
- æ€§èƒ½å¯¹æ¯”åˆ†æ
- æ™ºèƒ½è·¯ç”±ç­–ç•¥
- æˆæœ¬æ•ˆç›Šä¼˜åŒ–

åº”ç”¨ä»·å€¼:
- ä¼˜åŒ–æˆæœ¬æ•ˆç›Š
- æå‡ä»»åŠ¡é€‚é…æ€§
- å®ç°æ™ºèƒ½è´Ÿè½½å‡è¡¡
- æ”¯æŒå¤šå‚å•†æ¨¡å‹
"""

import os
import time
import asyncio
import statistics
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ æœ¬åœ°æºç è·¯å¾„
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from harborai import HarborAI
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥ HarborAIï¼Œè¯·æ£€æŸ¥è·¯å¾„é…ç½®")
    exit(1)


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    CHAT = "chat"
    CODE = "code"
    REASONING = "reasoning"
    TRANSLATION = "translation"
    CREATIVE = "creative"
    ANALYSIS = "analysis"


class ModelProvider(Enum):
    """æ¨¡å‹æä¾›å•†æšä¸¾"""
    DEEPSEEK = "deepseek"


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    name: str
    provider: ModelProvider
    api_key_env: str
    base_url_env: str
    default_base_url: str
    cost_per_1k_tokens: float
    max_tokens: int
    strengths: List[TaskType]
    description: str


@dataclass
class ModelPerformance:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    model_name: str
    response_time: float
    token_count: int
    cost: float
    quality_score: Optional[float] = None
    success: bool = True
    error: Optional[str] = None


class ModelRouter:
    """æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨"""
    
    def __init__(self):
        self.models = {
            "deepseek-chat": ModelConfig(
                name="deepseek-chat",
                provider=ModelProvider.DEEPSEEK,
                api_key_env="DEEPSEEK_API_KEY",
                base_url_env="DEEPSEEK_BASE_URL",
                default_base_url="https://api.deepseek.com",
                cost_per_1k_tokens=0.0014,
                max_tokens=4096,
                strengths=[TaskType.CHAT, TaskType.CODE, TaskType.CREATIVE, TaskType.ANALYSIS, TaskType.TRANSLATION],
                description="DeepSeeké€šç”¨æ¨¡å‹ï¼Œé€‚ç”¨äºå¯¹è¯ã€ä»£ç ç”Ÿæˆã€å†…å®¹åˆ›ä½œç­‰å¤šç§ä»»åŠ¡"
            ),
            "deepseek-reasoner": ModelConfig(
                name="deepseek-reasoner",
                provider=ModelProvider.DEEPSEEK,
                api_key_env="DEEPSEEK_API_KEY",
                base_url_env="DEEPSEEK_BASE_URL",
                default_base_url="https://api.deepseek.com",
                cost_per_1k_tokens=0.0055,
                max_tokens=4096,
                strengths=[TaskType.REASONING],
                description="DeepSeekæ¨ç†ä¸“ç”¨æ¨¡å‹ï¼Œé€‚ç”¨äºå¤æ‚æ¨ç†ã€æ•°å­¦è®¡ç®—ã€é€»è¾‘åˆ†æ"
            )
        }
        
        self.clients = {}
        self.performance_history = []
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        available = []
        for model_name, config in self.models.items():
            api_key = os.getenv(config.api_key_env)
            if api_key:
                available.append(model_name)
        return available
    
    def get_client(self, model_name: str) -> Optional[HarborAI]:
        """è·å–æŒ‡å®šæ¨¡å‹çš„å®¢æˆ·ç«¯"""
        if model_name not in self.models:
            return None
        
        if model_name in self.clients:
            return self.clients[model_name]
        
        config = self.models[model_name]
        api_key = os.getenv(config.api_key_env)
        base_url = os.getenv(config.base_url_env, config.default_base_url)
        
        if not api_key:
            return None
        
        client = HarborAI(api_key=api_key, base_url=base_url)
        self.clients[model_name] = client
        return client
    
    def recommend_model(self, task_type: TaskType, budget_priority: bool = False) -> str:
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹æ¨èæ¨¡å‹
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            budget_priority: æ˜¯å¦ä¼˜å…ˆè€ƒè™‘æˆæœ¬
            
        Returns:
            str: æ¨èçš„æ¨¡å‹åç§°
        """
        available_models = self.get_available_models()
        suitable_models = []
        
        for model_name in available_models:
            config = self.models[model_name]
            if task_type in config.strengths:
                suitable_models.append((model_name, config))
        
        if not suitable_models:
            # å¦‚æœæ²¡æœ‰ä¸“é—¨é€‚åˆçš„æ¨¡å‹ï¼Œè¿”å›é€šç”¨æ¨¡å‹
            for model_name in available_models:
                config = self.models[model_name]
                if TaskType.CHAT in config.strengths:
                    return model_name
            return available_models[0] if available_models else None
        
        if budget_priority:
            # æŒ‰æˆæœ¬æ’åº
            suitable_models.sort(key=lambda x: x[1].cost_per_1k_tokens)
        else:
            # æŒ‰èƒ½åŠ›æ’åºï¼ˆè¿™é‡Œç®€åŒ–ä¸ºæŒ‰æˆæœ¬å€’åºï¼Œå®é™…åº”è¯¥æœ‰æ›´å¤æ‚çš„è¯„åˆ†ï¼‰
            suitable_models.sort(key=lambda x: x[1].cost_per_1k_tokens, reverse=True)
        
        return suitable_models[0][0]
    
    def record_performance(self, performance: ModelPerformance):
        """è®°å½•æ¨¡å‹æ€§èƒ½"""
        self.performance_history.append(performance)
    
    def get_performance_stats(self, model_name: str = None) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if model_name:
            records = [p for p in self.performance_history if p.model_name == model_name]
        else:
            records = self.performance_history
        
        if not records:
            return {}
        
        successful_records = [r for r in records if r.success]
        
        if not successful_records:
            return {"total_calls": len(records), "success_rate": 0}
        
        return {
            "total_calls": len(records),
            "successful_calls": len(successful_records),
            "success_rate": len(successful_records) / len(records),
            "avg_response_time": statistics.mean(r.response_time for r in successful_records),
            "avg_token_count": statistics.mean(r.token_count for r in successful_records),
            "avg_cost": statistics.mean(r.cost for r in successful_records),
            "total_cost": sum(r.cost for r in successful_records)
        }


async def call_model_with_metrics(router: ModelRouter, model_name: str, prompt: str, **kwargs) -> ModelPerformance:
    """
    è°ƒç”¨æ¨¡å‹å¹¶è®°å½•æ€§èƒ½æŒ‡æ ‡
    
    Args:
        router: æ¨¡å‹è·¯ç”±å™¨
        model_name: æ¨¡å‹åç§°
        prompt: æç¤ºè¯
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ModelPerformance: æ€§èƒ½æŒ‡æ ‡
    """
    client = router.get_client(model_name)
    if not client:
        return ModelPerformance(
            model_name=model_name,
            response_time=0,
            token_count=0,
            cost=0,
            success=False,
            error="æ¨¡å‹å®¢æˆ·ç«¯ä¸å¯ç”¨"
        )
    
    config = router.models[model_name]
    start_time = time.time()
    
    try:
        response = await client.chat.completions.acreate(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=kwargs.get("temperature", 0.7),
            max_tokens=kwargs.get("max_tokens")  # é»˜è®¤æ— é™åˆ¶ï¼Œç”±æ¨¡å‹å‚å•†æ§åˆ¶
        )
        
        elapsed_time = time.time() - start_time
        usage = response.usage
        cost = (usage.total_tokens / 1000) * config.cost_per_1k_tokens
        
        performance = ModelPerformance(
            model_name=model_name,
            response_time=elapsed_time,
            token_count=usage.total_tokens,
            cost=cost,
            success=True
        )
        
        router.record_performance(performance)
        return performance
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        performance = ModelPerformance(
            model_name=model_name,
            response_time=elapsed_time,
            token_count=0,
            cost=0,
            success=False,
            error=str(e)
        )
        
        router.record_performance(performance)
        return performance


async def compare_models_performance(router: ModelRouter, prompt: str, models: List[str] = None) -> Dict[str, ModelPerformance]:
    """
    å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
    
    Args:
        router: æ¨¡å‹è·¯ç”±å™¨
        prompt: æµ‹è¯•æç¤º
        models: è¦å¯¹æ¯”çš„æ¨¡å‹åˆ—è¡¨
        
    Returns:
        Dict: å„æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡
    """
    if models is None:
        models = router.get_available_models()
    
    print(f"\nâš–ï¸  å¯¹æ¯”æ¨¡å‹æ€§èƒ½")
    print(f"ğŸ¯ æµ‹è¯•æç¤º: {prompt[:100]}...")
    print(f"ğŸ“‹ æµ‹è¯•æ¨¡å‹: {', '.join(models)}")
    
    # å¹¶å‘è°ƒç”¨æ‰€æœ‰æ¨¡å‹
    tasks = [call_model_with_metrics(router, model, prompt) for model in models]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ•´ç†ç»“æœ
    performance_results = {}
    for i, result in enumerate(results):
        model_name = models[i]
        if isinstance(result, Exception):
            performance_results[model_name] = ModelPerformance(
                model_name=model_name,
                response_time=0,
                token_count=0,
                cost=0,
                success=False,
                error=str(result)
            )
        else:
            performance_results[model_name] = result
    
    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("-" * 80)
    print(f"{'æ¨¡å‹åç§°':<20} {'å“åº”æ—¶é—´':<10} {'Tokenæ•°':<10} {'æˆæœ¬':<10} {'çŠ¶æ€':<10}")
    print("-" * 80)
    
    for model_name, perf in performance_results.items():
        if perf.success:
            print(f"{model_name:<20} {perf.response_time:<10.2f} {perf.token_count:<10} ${perf.cost:<9.4f} {'âœ…æˆåŠŸ':<10}")
        else:
            print(f"{model_name:<20} {'N/A':<10} {'N/A':<10} {'N/A':<10} {'âŒå¤±è´¥':<10}")
    
    return performance_results


async def intelligent_model_routing(router: ModelRouter, tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    æ™ºèƒ½æ¨¡å‹è·¯ç”±
    
    Args:
        router: æ¨¡å‹è·¯ç”±å™¨
        tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«promptå’Œtask_type
        
    Returns:
        List: å¤„ç†ç»“æœ
    """
    print(f"\nğŸ§  æ™ºèƒ½æ¨¡å‹è·¯ç”±")
    print(f"ğŸ“‹ å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡")
    
    results = []
    
    for i, task in enumerate(tasks):
        prompt = task["prompt"]
        task_type = task["task_type"]
        budget_priority = task.get("budget_priority", False)
        
        # æ¨èæ¨¡å‹
        recommended_model = router.recommend_model(task_type, budget_priority)
        
        if not recommended_model:
            results.append({
                "task_index": i,
                "task_type": task_type.value,
                "prompt": prompt[:50] + "...",
                "recommended_model": None,
                "success": False,
                "error": "æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹"
            })
            continue
        
        print(f"\nğŸ¯ ä»»åŠ¡ {i+1}: {task_type.value}")
        print(f"   æ¨èæ¨¡å‹: {recommended_model}")
        print(f"   é¢„ç®—ä¼˜å…ˆ: {budget_priority}")
        
        # è°ƒç”¨æ¨èçš„æ¨¡å‹
        performance = await call_model_with_metrics(router, recommended_model, prompt)
        
        result = {
            "task_index": i,
            "task_type": task_type.value,
            "prompt": prompt[:50] + "...",
            "recommended_model": recommended_model,
            "performance": performance,
            "success": performance.success
        }
        
        if performance.success:
            print(f"   âœ… å®Œæˆ - è€—æ—¶: {performance.response_time:.2f}s, æˆæœ¬: ${performance.cost:.4f}")
        else:
            print(f"   âŒ å¤±è´¥ - {performance.error}")
            result["error"] = performance.error
        
        results.append(result)
    
    return results


async def load_balancing_demo(router: ModelRouter, prompt: str, num_requests: int = 10):
    """
    è´Ÿè½½å‡è¡¡æ¼”ç¤º
    
    Args:
        router: æ¨¡å‹è·¯ç”±å™¨
        prompt: æµ‹è¯•æç¤º
        num_requests: è¯·æ±‚æ•°é‡
    """
    print(f"\nâš–ï¸  è´Ÿè½½å‡è¡¡æ¼”ç¤º")
    print(f"ğŸ“‹ å‘é€ {num_requests} ä¸ªå¹¶å‘è¯·æ±‚")
    
    available_models = router.get_available_models()
    if len(available_models) < 2:
        print("âŒ éœ€è¦è‡³å°‘2ä¸ªå¯ç”¨æ¨¡å‹è¿›è¡Œè´Ÿè½½å‡è¡¡æ¼”ç¤º")
        return
    
    # è½®è¯¢åˆ†é…è¯·æ±‚åˆ°ä¸åŒæ¨¡å‹
    tasks = []
    for i in range(num_requests):
        model = available_models[i % len(available_models)]
        tasks.append(call_model_with_metrics(router, model, f"{prompt} (è¯·æ±‚ {i+1})"))
    
    start_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    total_time = time.time() - start_time
    
    # ç»Ÿè®¡ç»“æœ
    model_stats = {}
    for i, result in enumerate(results):
        model = available_models[i % len(available_models)]
        if model not in model_stats:
            model_stats[model] = {"success": 0, "failed": 0, "total_time": 0, "total_cost": 0}
        
        if isinstance(result, Exception) or not result.success:
            model_stats[model]["failed"] += 1
        else:
            model_stats[model]["success"] += 1
            model_stats[model]["total_time"] += result.response_time
            model_stats[model]["total_cost"] += result.cost
    
    print(f"\nğŸ“Š è´Ÿè½½å‡è¡¡ç»“æœ (æ€»è€—æ—¶: {total_time:.2f}ç§’):")
    print("-" * 60)
    print(f"{'æ¨¡å‹':<20} {'æˆåŠŸ':<8} {'å¤±è´¥':<8} {'å¹³å‡è€—æ—¶':<12} {'æ€»æˆæœ¬':<10}")
    print("-" * 60)
    
    for model, stats in model_stats.items():
        avg_time = stats["total_time"] / stats["success"] if stats["success"] > 0 else 0
        print(f"{model:<20} {stats['success']:<8} {stats['failed']:<8} {avg_time:<12.2f} ${stats['total_cost']:<9.4f}")


def show_model_capabilities(router: ModelRouter):
    """æ˜¾ç¤ºæ¨¡å‹èƒ½åŠ›å¯¹æ¯”"""
    print(f"\nğŸ“‹ æ¨¡å‹èƒ½åŠ›å¯¹æ¯”")
    print("=" * 80)
    
    available_models = router.get_available_models()
    
    print(f"{'æ¨¡å‹åç§°':<20} {'æä¾›å•†':<12} {'æˆæœ¬/1K':<10} {'æ“…é•¿é¢†åŸŸ':<30} {'æè¿°'}")
    print("-" * 80)
    
    for model_name in available_models:
        config = router.models[model_name]
        strengths = ", ".join([t.value for t in config.strengths])
        print(f"{model_name:<20} {config.provider.value:<12} ${config.cost_per_1k_tokens:<9.4f} {strengths:<30} {config.description}")


async def adaptive_model_selection(router: ModelRouter, prompts: List[str]):
    """
    è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©æ¼”ç¤º
    
    Args:
        router: æ¨¡å‹è·¯ç”±å™¨
        prompts: æµ‹è¯•æç¤ºåˆ—è¡¨
    """
    print(f"\nğŸ¯ è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©æ¼”ç¤º")
    print("=" * 60)
    
    # å®šä¹‰ä¸åŒç±»å‹çš„ä»»åŠ¡
    task_configs = [
        {"task_type": TaskType.CHAT, "budget_priority": True},
        {"task_type": TaskType.CODE, "budget_priority": False},
        {"task_type": TaskType.REASONING, "budget_priority": False},
        {"task_type": TaskType.CREATIVE, "budget_priority": True},
    ]
    
    for i, prompt in enumerate(prompts):
        if i >= len(task_configs):
            break
        
        config = task_configs[i]
        task_type = config["task_type"]
        budget_priority = config["budget_priority"]
        
        print(f"\nğŸ“ ä»»åŠ¡ {i+1}: {task_type.value}")
        print(f"   æç¤º: {prompt[:80]}...")
        print(f"   é¢„ç®—ä¼˜å…ˆ: {budget_priority}")
        
        # è·å–æ¨èæ¨¡å‹
        recommended = router.recommend_model(task_type, budget_priority)
        print(f"   æ¨èæ¨¡å‹: {recommended}")
        
        if recommended:
            # æ‰§è¡Œä»»åŠ¡
            performance = await call_model_with_metrics(router, recommended, prompt)
            if performance.success:
                print(f"   âœ… æ‰§è¡ŒæˆåŠŸ - è€—æ—¶: {performance.response_time:.2f}s, æˆæœ¬: ${performance.cost:.4f}")
            else:
                print(f"   âŒ æ‰§è¡Œå¤±è´¥: {performance.error}")


async def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ”„ HarborAI å¤šæ¨¡å‹åˆ‡æ¢ç¤ºä¾‹")
    print("="*60)
    
    # åˆå§‹åŒ–è·¯ç”±å™¨
    router = ModelRouter()
    available_models = router.get_available_models()
    
    if not available_models:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®")
        print("ğŸ’¡ éœ€è¦é…ç½®çš„ç¯å¢ƒå˜é‡:")
        for model_name, config in router.models.items():
            print(f"   - {config.api_key_env}: {config.description}")
        return
    
    print(f"âœ… å‘ç° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹: {', '.join(available_models)}")
    
    # æ˜¾ç¤ºæ¨¡å‹èƒ½åŠ›
    show_model_capabilities(router)
    
    # æµ‹è¯•æç¤º
    test_prompts = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        "è¯·å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚",
        "å¦‚æœä¸€ä¸ªå†œå¤«æœ‰17åªç¾Šï¼Œé™¤äº†9åªä»¥å¤–éƒ½æ­»äº†ï¼Œè¯·é—®å†œå¤«è¿˜æœ‰å‡ åªç¾Šï¼Ÿ",
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ã€‚"
    ]
    
    # 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ”¹ 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    await compare_models_performance(router, test_prompts[0], available_models[:3])
    
    # 2. æ™ºèƒ½æ¨¡å‹è·¯ç”±
    print(f"\nğŸ”¹ 2. æ™ºèƒ½æ¨¡å‹è·¯ç”±")
    routing_tasks = [
        {"prompt": test_prompts[0], "task_type": TaskType.CHAT, "budget_priority": True},
        {"prompt": test_prompts[1], "task_type": TaskType.CODE, "budget_priority": False},
        {"prompt": test_prompts[2], "task_type": TaskType.REASONING, "budget_priority": False},
        {"prompt": test_prompts[3], "task_type": TaskType.CREATIVE, "budget_priority": True}
    ]
    await intelligent_model_routing(router, routing_tasks)
    
    # 3. è´Ÿè½½å‡è¡¡æ¼”ç¤º
    if len(available_models) >= 2:
        print(f"\nğŸ”¹ 3. è´Ÿè½½å‡è¡¡æ¼”ç¤º")
        await load_balancing_demo(router, "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½ã€‚", 6)
    
    # 4. è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©
    print(f"\nğŸ”¹ 4. è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©")
    await adaptive_model_selection(router, test_prompts)
    
    # 5. æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š
    print(f"\nğŸ”¹ 5. æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 60)
    
    overall_stats = router.get_performance_stats()
    if overall_stats:
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»è°ƒç”¨æ¬¡æ•°: {overall_stats['total_calls']}")
        print(f"   æˆåŠŸç‡: {overall_stats['success_rate']:.1%}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {overall_stats['avg_response_time']:.2f}ç§’")
        print(f"   å¹³å‡Tokenæ•°: {overall_stats['avg_token_count']:.0f}")
        print(f"   æ€»æˆæœ¬: ${overall_stats['total_cost']:.4f}")
        
        print(f"\nğŸ“ˆ å„æ¨¡å‹ç»Ÿè®¡:")
        for model in available_models:
            model_stats = router.get_performance_stats(model)
            if model_stats and model_stats['total_calls'] > 0:
                print(f"   {model}:")
                print(f"     è°ƒç”¨æ¬¡æ•°: {model_stats['total_calls']}")
                print(f"     æˆåŠŸç‡: {model_stats['success_rate']:.1%}")
                if model_stats['successful_calls'] > 0:
                    print(f"     å¹³å‡è€—æ—¶: {model_stats['avg_response_time']:.2f}ç§’")
                    print(f"     æ€»æˆæœ¬: ${model_stats['total_cost']:.4f}")
    
    print(f"\nğŸ‰ å¤šæ¨¡å‹åˆ‡æ¢ç¤ºä¾‹æ‰§è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())