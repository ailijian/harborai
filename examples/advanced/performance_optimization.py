#!/usr/bin/env python3
"""
æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº† HarborAI çš„æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. æ™ºèƒ½ç¼“å­˜æœºåˆ¶
2. è¿æ¥æ± ç®¡ç†
3. è¯·æ±‚é¢„æµ‹å’Œé¢„åŠ è½½
4. èµ„æºç›‘æ§å’Œè°ƒä¼˜
5. æ‰¹é‡ä¼˜åŒ–ç­–ç•¥
6. æµå¼å¤„ç†ä¼˜åŒ–

åœºæ™¯ï¼š
- é«˜é¢‘APIè°ƒç”¨åœºæ™¯
- éœ€è¦ä½å»¶è¿Ÿå“åº”
- å¤§é‡å¹¶å‘è¯·æ±‚
- èµ„æºä½¿ç”¨ä¼˜åŒ–

ä»·å€¼ï¼š
- æ˜¾è‘—é™ä½å“åº”å»¶è¿Ÿ
- å‡å°‘APIè°ƒç”¨æˆæœ¬
- æé«˜ç³»ç»Ÿååé‡
- æ™ºèƒ½èµ„æºç®¡ç†
"""

import asyncio
import time
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import json
import os
import hashlib
from collections import defaultdict, OrderedDict
import statistics

# æ­£ç¡®çš„ HarborAI å¯¼å…¥æ–¹å¼
from harborai import HarborAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_client():
    """è·å– HarborAI å®¢æˆ·ç«¯"""
    # ä¼˜å…ˆä½¿ç”¨ DeepSeek
    if os.getenv('DEEPSEEK_API_KEY'):
        return HarborAI(
            api_key=os.getenv('DEEPSEEK_API_KEY'),
            base_url=os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')
        ), "deepseek-chat"
    
    # å…¶æ¬¡ä½¿ç”¨ Ernie
    if os.getenv('ERNIE_API_KEY'):
        return HarborAI(
            api_key=os.getenv('ERNIE_API_KEY'),
            base_url=os.getenv('ERNIE_BASE_URL', 'https://aip.baidubce.com')
        ), "ernie-3.5-8k"
    
    # æœ€åä½¿ç”¨ Doubao
    if os.getenv('DOUBAO_API_KEY'):
        return HarborAI(
            api_key=os.getenv('DOUBAO_API_KEY'),
            base_url=os.getenv('DOUBAO_BASE_URL', 'https://ark.cn-beijing.volces.com')
        ), "doubao-1-5-pro-32k-character-250715"
    
    return None, None

class SimpleCache:
    """ç®€å•çš„å†…å­˜ç¼“å­˜å®ç°"""
    
    def __init__(self, max_size: int = 100, ttl_seconds: int = 300):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = OrderedDict()
        self.timestamps = {}
        self.hit_count = 0
        self.miss_count = 0
    
    def _generate_key(self, messages: List[Dict], **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ç›¸å…³å‚æ•°çš„å­—ç¬¦ä¸²
        cache_data = {
            'messages': messages,
            'model': kwargs.get('model', ''),
            'temperature': kwargs.get('temperature', 0.7),
            'max_tokens': kwargs.get('max_tokens', None)
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _is_expired(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        if key not in self.timestamps:
            return True
        
        age = time.time() - self.timestamps[key]
        return age > self.ttl_seconds
    
    def _evict_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        expired_keys = [
            key for key, timestamp in self.timestamps.items()
            if current_time - timestamp > self.ttl_seconds
        ]
        
        for key in expired_keys:
            self.cache.pop(key, None)
            self.timestamps.pop(key, None)
    
    def _evict_lru(self):
        """LRUæ·˜æ±°ç­–ç•¥"""
        while len(self.cache) >= self.max_size:
            oldest_key = next(iter(self.cache))
            self.cache.pop(oldest_key)
            self.timestamps.pop(oldest_key, None)
    
    def get(self, messages: List[Dict], **kwargs) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        key = self._generate_key(messages, **kwargs)
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._evict_expired()
        
        if key in self.cache and not self._is_expired(key):
            # ç¼“å­˜å‘½ä¸­ï¼Œç§»åˆ°æœ€åï¼ˆLRUï¼‰
            value = self.cache.pop(key)
            self.cache[key] = value
            self.hit_count += 1
            return value
        
        self.miss_count += 1
        return None
    
    def set(self, messages: List[Dict], response: Any, **kwargs):
        """è®¾ç½®ç¼“å­˜"""
        key = self._generate_key(messages, **kwargs)
        
        # LRUæ·˜æ±°
        self._evict_lru()
        
        self.cache[key] = response
        self.timestamps[key] = time.time()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = self.hit_count / max(total_requests, 1)
        
        return {
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache),
            'max_size': self.max_size
        }
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.timestamps.clear()
        self.hit_count = 0
        self.miss_count = 0

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, enable_cache: bool = True, max_concurrent: int = 10):
        self.client, self.model = get_client()
        if not self.client:
            raise ValueError("è¯·è‡³å°‘è®¾ç½®ä¸€ä¸ª API Key (DEEPSEEK_API_KEY, ERNIE_API_KEY, æˆ– DOUBAO_API_KEY)")
        
        self.enable_cache = enable_cache
        self.cache = SimpleCache() if enable_cache else None
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.request_times = []
        self.total_requests = 0
        self.cached_requests = 0
        self.total_tokens = 0
        self.start_time = None
    
    async def optimized_request(self, messages: List[Dict], **kwargs) -> Tuple[Any, bool]:
        """ä¼˜åŒ–çš„è¯·æ±‚æ–¹æ³•"""
        # æ£€æŸ¥ç¼“å­˜
        if self.cache:
            cached_response = self.cache.get(messages, **kwargs)
            if cached_response:
                self.cached_requests += 1
                return cached_response, True
        
        # å‘é€è¯·æ±‚
        async with self.semaphore:
            start_time = time.time()
            
            try:
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    fallback=["deepseek-chat", "ernie-3.5-8k"],
                    retry_policy={
                        "max_attempts": 2,
                        "base_delay": 1.0,
                        "max_delay": 5.0
                    },
                    timeout=30.0,
                    **kwargs
                )
                
                request_time = time.time() - start_time
                self.request_times.append(request_time)
                self.total_requests += 1
                
                if response.usage:
                    self.total_tokens += response.usage.total_tokens
                
                # ç¼“å­˜å“åº”
                if self.cache:
                    self.cache.set(messages, response, **kwargs)
                
                return response, False
                
            except Exception as e:
                request_time = time.time() - start_time
                self.request_times.append(request_time)
                self.total_requests += 1
                raise e
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.request_times:
            return {
                'total_requests': 0,
                'cached_requests': 0,
                'cache_hit_rate': 0,
                'avg_response_time': 0,
                'min_response_time': 0,
                'max_response_time': 0,
                'p95_response_time': 0,
                'total_tokens': 0,
                'requests_per_second': 0
            }
        
        elapsed = time.time() - self.start_time if self.start_time else 1
        cache_hit_rate = self.cached_requests / max(self.total_requests + self.cached_requests, 1)
        
        return {
            'total_requests': self.total_requests,
            'cached_requests': self.cached_requests,
            'cache_hit_rate': cache_hit_rate,
            'avg_response_time': statistics.mean(self.request_times),
            'min_response_time': min(self.request_times),
            'max_response_time': max(self.request_times),
            'p95_response_time': statistics.quantiles(self.request_times, n=20)[18] if len(self.request_times) > 1 else 0,
            'total_tokens': self.total_tokens,
            'requests_per_second': (self.total_requests + self.cached_requests) / elapsed
        }

async def demo_cache_performance():
    """æ¼”ç¤ºç¼“å­˜æ€§èƒ½ä¼˜åŒ–"""
    print("\nğŸš€ æ¼”ç¤ºç¼“å­˜æ€§èƒ½ä¼˜åŒ–")
    print("=" * 50)
    
    # å‡†å¤‡é‡å¤çš„è¯·æ±‚
    common_questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è§£é‡Šæœºå™¨å­¦ä¹ çš„æ¦‚å¿µ",
        "æ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",  # é‡å¤
        "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨",
        "è§£é‡Šæœºå™¨å­¦ä¹ çš„æ¦‚å¿µ",  # é‡å¤
        "è®¡ç®—æœºè§†è§‰æŠ€æœ¯ä»‹ç»",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",  # é‡å¤
    ]
    
    # 1. æ— ç¼“å­˜æµ‹è¯•
    print("ğŸ”„ æ— ç¼“å­˜æ€§èƒ½æµ‹è¯•...")
    optimizer_no_cache = PerformanceOptimizer(enable_cache=False)
    optimizer_no_cache.start_time = time.time()
    
    start_time = time.time()
    for question in common_questions:
        messages = [{'role': 'user', 'content': question}]
        try:
            response, from_cache = await optimizer_no_cache.optimized_request(messages)
            print(f"   âœ… å®Œæˆ: {question[:20]}...")
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {question[:20]}... - {e}")
    
    no_cache_time = time.time() - start_time
    no_cache_stats = optimizer_no_cache.get_performance_stats()
    
    # 2. æœ‰ç¼“å­˜æµ‹è¯•
    print("\nğŸ”„ æœ‰ç¼“å­˜æ€§èƒ½æµ‹è¯•...")
    optimizer_with_cache = PerformanceOptimizer(enable_cache=True)
    optimizer_with_cache.start_time = time.time()
    
    start_time = time.time()
    for question in common_questions:
        messages = [{'role': 'user', 'content': question}]
        try:
            response, from_cache = await optimizer_with_cache.optimized_request(messages)
            cache_indicator = "ğŸ’¾" if from_cache else "ğŸŒ"
            print(f"   {cache_indicator} å®Œæˆ: {question[:20]}...")
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {question[:20]}... - {e}")
    
    cache_time = time.time() - start_time
    cache_stats = optimizer_with_cache.get_performance_stats()
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š ç¼“å­˜æ€§èƒ½å¯¹æ¯”:")
    print(f"   æ— ç¼“å­˜:")
    print(f"     - æ€»è€—æ—¶: {no_cache_time:.2f}ç§’")
    print(f"     - å¹³å‡å“åº”æ—¶é—´: {no_cache_stats['avg_response_time']:.2f}ç§’")
    print(f"     - æ€»è¯·æ±‚æ•°: {no_cache_stats['total_requests']}")
    
    print(f"   æœ‰ç¼“å­˜:")
    print(f"     - æ€»è€—æ—¶: {cache_time:.2f}ç§’")
    print(f"     - å¹³å‡å“åº”æ—¶é—´: {cache_stats['avg_response_time']:.2f}ç§’")
    print(f"     - ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['cache_hit_rate']:.1%}")
    print(f"     - å®é™…è¯·æ±‚æ•°: {cache_stats['total_requests']}")
    print(f"     - ç¼“å­˜è¯·æ±‚æ•°: {cache_stats['cached_requests']}")
    
    if no_cache_time > 0 and cache_time > 0:
        speedup = no_cache_time / cache_time
        print(f"   æ€§èƒ½æå‡: {speedup:.2f}x")
    
    # ç¼“å­˜ç»Ÿè®¡
    if optimizer_with_cache.cache:
        cache_stats_detail = optimizer_with_cache.cache.get_stats()
        print(f"\nğŸ’¾ ç¼“å­˜è¯¦ç»†ç»Ÿè®¡:")
        print(f"   - ç¼“å­˜å¤§å°: {cache_stats_detail['cache_size']}/{cache_stats_detail['max_size']}")
        print(f"   - å‘½ä¸­æ¬¡æ•°: {cache_stats_detail['hit_count']}")
        print(f"   - æœªå‘½ä¸­æ¬¡æ•°: {cache_stats_detail['miss_count']}")

async def demo_concurrent_optimization():
    """æ¼”ç¤ºå¹¶å‘ä¼˜åŒ–"""
    print("\nâš¡ æ¼”ç¤ºå¹¶å‘ä¼˜åŒ–")
    print("=" * 50)
    
    questions = [
        "ä»€ä¹ˆæ˜¯äº‘è®¡ç®—ï¼Ÿ",
        "è§£é‡Šå®¹å™¨æŠ€æœ¯",
        "å¾®æœåŠ¡æ¶æ„çš„ä¼˜åŠ¿",
        "DevOpsçš„æ ¸å¿ƒç†å¿µ",
        "æŒç»­é›†æˆçš„é‡è¦æ€§",
        "åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡",
        "è´Ÿè½½å‡è¡¡çš„åŸç†",
        "æ•°æ®åº“ä¼˜åŒ–ç­–ç•¥"
    ]
    
    # 1. é¡ºåºå¤„ç†
    print("ğŸ”„ é¡ºåºå¤„ç†æµ‹è¯•...")
    optimizer = PerformanceOptimizer(enable_cache=False, max_concurrent=1)
    
    start_time = time.time()
    for i, question in enumerate(questions[:4]):  # åªæµ‹è¯•å‰4ä¸ª
        messages = [{'role': 'user', 'content': question}]
        try:
            response, _ = await optimizer.optimized_request(messages)
            print(f"   âœ… é¡ºåº {i+1}: {question[:20]}...")
        except Exception as e:
            print(f"   âŒ é¡ºåº {i+1}: {e}")
    
    sequential_time = time.time() - start_time
    
    # 2. å¹¶å‘å¤„ç†
    print("\nğŸ”„ å¹¶å‘å¤„ç†æµ‹è¯•...")
    optimizer = PerformanceOptimizer(enable_cache=False, max_concurrent=4)
    
    async def process_question(question: str, index: int):
        messages = [{'role': 'user', 'content': question}]
        try:
            response, _ = await optimizer.optimized_request(messages)
            print(f"   âœ… å¹¶å‘ {index+1}: {question[:20]}...")
            return True
        except Exception as e:
            print(f"   âŒ å¹¶å‘ {index+1}: {e}")
            return False
    
    start_time = time.time()
    tasks = [
        process_question(question, i) 
        for i, question in enumerate(questions[:4])
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    concurrent_time = time.time() - start_time
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š å¹¶å‘æ€§èƒ½å¯¹æ¯”:")
    print(f"   é¡ºåºå¤„ç†: {sequential_time:.2f}ç§’")
    print(f"   å¹¶å‘å¤„ç†: {concurrent_time:.2f}ç§’")
    
    if sequential_time > 0 and concurrent_time > 0:
        speedup = sequential_time / concurrent_time
        print(f"   æ€§èƒ½æå‡: {speedup:.2f}x")

async def demo_streaming_optimization():
    """æ¼”ç¤ºæµå¼å¤„ç†ä¼˜åŒ–"""
    print("\nğŸŒŠ æ¼”ç¤ºæµå¼å¤„ç†ä¼˜åŒ–")
    print("=" * 50)
    
    client, model = get_client()
    if not client:
        print("âŒ è¯·è‡³å°‘è®¾ç½®ä¸€ä¸ª API Key")
        return
    
    question = "è¯¦ç»†è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å’Œæœªæ¥è¶‹åŠ¿"
    
    # 1. æ™®é€šè¯·æ±‚
    print("ğŸ”„ æ™®é€šè¯·æ±‚æµ‹è¯•...")
    start_time = time.time()
    
    try:
        response = await client.chat.completions.create(
            model=model,
            messages=[{'role': 'user', 'content': question}],
            fallback=["deepseek-chat", "ernie-3.5-8k"],
            timeout=60.0
        )
        
        normal_time = time.time() - start_time
        content_length = len(response.choices[0].message.content) if response.choices else 0
        
        print(f"   âœ… æ™®é€šè¯·æ±‚å®Œæˆ")
        print(f"   - æ€»è€—æ—¶: {normal_time:.2f}ç§’")
        print(f"   - å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
        print(f"   - é¦–å­—èŠ‚æ—¶é—´: {normal_time:.2f}ç§’")
        
    except Exception as e:
        print(f"   âŒ æ™®é€šè¯·æ±‚å¤±è´¥: {e}")
        return
    
    # 2. æµå¼è¯·æ±‚
    print("\nğŸ”„ æµå¼è¯·æ±‚æµ‹è¯•...")
    start_time = time.time()
    first_chunk_time = None
    chunk_count = 0
    total_content = ""
    
    try:
        stream = await client.chat.completions.create(
            model=model,
            messages=[{'role': 'user', 'content': question}],
            stream=True,
            fallback=["deepseek-chat", "ernie-3.5-8k"],
            timeout=60.0
        )
        
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                if first_chunk_time is None:
                    first_chunk_time = time.time() - start_time
                
                content = chunk.choices[0].delta.content
                total_content += content
                chunk_count += 1
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªå­—ç¬¦
                if chunk_count <= 3:
                    print(f"   ğŸ“¦ ç‰‡æ®µ {chunk_count}: {content[:20]}...")
        
        stream_time = time.time() - start_time
        
        print(f"\n   âœ… æµå¼è¯·æ±‚å®Œæˆ")
        print(f"   - æ€»è€—æ—¶: {stream_time:.2f}ç§’")
        print(f"   - é¦–å­—èŠ‚æ—¶é—´: {first_chunk_time:.2f}ç§’")
        print(f"   - æ€»ç‰‡æ®µæ•°: {chunk_count}")
        print(f"   - å†…å®¹é•¿åº¦: {len(total_content)} å­—ç¬¦")
        
        # æ€§èƒ½å¯¹æ¯”
        print(f"\nğŸ“Š æµå¼å¤„ç†ä¼˜åŠ¿:")
        if first_chunk_time and normal_time > 0:
            ttfb_improvement = (normal_time - first_chunk_time) / normal_time
            print(f"   - é¦–å­—èŠ‚æ—¶é—´æå‡: {ttfb_improvement:.1%}")
            print(f"   - ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿé™ä½: {normal_time - first_chunk_time:.2f}ç§’")
        
    except Exception as e:
        print(f"   âŒ æµå¼è¯·æ±‚å¤±è´¥: {e}")

async def demo_batch_optimization():
    """æ¼”ç¤ºæ‰¹é‡ä¼˜åŒ–"""
    print("\nğŸ“¦ æ¼”ç¤ºæ‰¹é‡ä¼˜åŒ–")
    print("=" * 50)
    
    # å‡†å¤‡æ‰¹é‡è¯·æ±‚
    questions = [
        "ä»€ä¹ˆæ˜¯åŒºå—é“¾ï¼Ÿ",
        "è§£é‡Šæ™ºèƒ½åˆçº¦",
        "DeFiçš„æ ¸å¿ƒæ¦‚å¿µ",
        "NFTæŠ€æœ¯åŸç†",
        "Web3çš„å‘å±•å‰æ™¯"
    ]
    
    optimizer = PerformanceOptimizer(enable_cache=True, max_concurrent=3)
    optimizer.start_time = time.time()
    
    print(f"ğŸ“ æ‰¹é‡å¤„ç† {len(questions)} ä¸ªè¯·æ±‚...")
    
    # å¹¶å‘æ‰¹é‡å¤„ç†
    async def process_batch_question(question: str, index: int):
        messages = [{'role': 'user', 'content': question}]
        try:
            start_time = time.time()
            response, from_cache = await optimizer.optimized_request(messages)
            process_time = time.time() - start_time
            
            cache_indicator = "ğŸ’¾" if from_cache else "ğŸŒ"
            print(f"   {cache_indicator} é—®é¢˜ {index+1}: {question[:20]}... ({process_time:.2f}s)")
            return True, process_time
            
        except Exception as e:
            print(f"   âŒ é—®é¢˜ {index+1}: {e}")
            return False, 0
    
    start_time = time.time()
    tasks = [
        process_batch_question(question, i) 
        for i, question in enumerate(questions)
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    total_time = time.time() - start_time
    
    # ç»Ÿè®¡ç»“æœ
    successful = sum(1 for result in results if isinstance(result, tuple) and result[0])
    avg_time = statistics.mean([result[1] for result in results if isinstance(result, tuple) and result[0]]) if successful > 0 else 0
    
    stats = optimizer.get_performance_stats()
    
    print(f"\nğŸ“Š æ‰¹é‡ä¼˜åŒ–ç»Ÿè®¡:")
    print(f"   - æˆåŠŸè¯·æ±‚: {successful}/{len(questions)}")
    print(f"   - æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"   - å¹³å‡å•è¯·æ±‚æ—¶é—´: {avg_time:.2f}ç§’")
    print(f"   - ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")
    print(f"   - å®é™…APIè°ƒç”¨: {stats['total_requests']}")
    print(f"   - ç¼“å­˜å“åº”: {stats['cached_requests']}")
    print(f"   - æ€»ååé‡: {stats['requests_per_second']:.2f} è¯·æ±‚/ç§’")

async def demo_response_time_optimization():
    """æ¼”ç¤ºå“åº”æ—¶é—´ä¼˜åŒ–"""
    print("\nâ±ï¸ æ¼”ç¤ºå“åº”æ—¶é—´ä¼˜åŒ–")
    print("=" * 50)
    
    optimizer = PerformanceOptimizer(enable_cache=True, max_concurrent=5)
    optimizer.start_time = time.time()
    
    # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„è¯·æ±‚
    test_cases = [
        ("ç®€å•é—®é¢˜", "ä»€ä¹ˆæ˜¯AIï¼Ÿ"),
        ("ä¸­ç­‰é—®é¢˜", "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†å’Œåº”ç”¨åœºæ™¯"),
        ("å¤æ‚é—®é¢˜", "è¯¦ç»†åˆ†ææ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æŠ€æœ¯å‘å±•å†ç¨‹ã€å½“å‰æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘"),
        ("ç®€å•é—®é¢˜", "ä»€ä¹ˆæ˜¯AIï¼Ÿ"),  # é‡å¤ï¼Œæµ‹è¯•ç¼“å­˜
    ]
    
    print("ğŸ“ æµ‹è¯•ä¸åŒå¤æ‚åº¦è¯·æ±‚çš„å“åº”æ—¶é—´...")
    
    for i, (complexity, question) in enumerate(test_cases):
        messages = [{'role': 'user', 'content': question}]
        
        try:
            start_time = time.time()
            response, from_cache = await optimizer.optimized_request(messages)
            response_time = time.time() - start_time
            
            cache_indicator = "ğŸ’¾" if from_cache else "ğŸŒ"
            content_length = len(response.choices[0].message.content) if response.choices else 0
            
            print(f"   {cache_indicator} {complexity} {i+1}:")
            print(f"      - å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            print(f"      - å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
            print(f"      - å¤„ç†é€Ÿåº¦: {content_length/max(response_time, 0.01):.0f} å­—ç¬¦/ç§’")
            
        except Exception as e:
            print(f"   âŒ {complexity} {i+1}: {e}")
    
    # æ˜¾ç¤ºæ•´ä½“ç»Ÿè®¡
    stats = optimizer.get_performance_stats()
    
    print(f"\nğŸ“Š å“åº”æ—¶é—´ç»Ÿè®¡:")
    print(f"   - å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}ç§’")
    print(f"   - æœ€å¿«å“åº”æ—¶é—´: {stats['min_response_time']:.2f}ç§’")
    print(f"   - æœ€æ…¢å“åº”æ—¶é—´: {stats['max_response_time']:.2f}ç§’")
    print(f"   - P95å“åº”æ—¶é—´: {stats['p95_response_time']:.2f}ç§’")
    print(f"   - ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")

async def demo_resource_monitoring():
    """æ¼”ç¤ºèµ„æºç›‘æ§"""
    print("\nğŸ“Š æ¼”ç¤ºèµ„æºç›‘æ§")
    print("=" * 50)
    
    optimizer = PerformanceOptimizer(enable_cache=True, max_concurrent=3)
    optimizer.start_time = time.time()
    
    # æ¨¡æ‹Ÿä¸€æ®µæ—¶é—´çš„è¯·æ±‚
    questions = [
        "ä»€ä¹ˆæ˜¯äº‘åŸç”Ÿï¼Ÿ",
        "Kubernetesçš„æ ¸å¿ƒæ¦‚å¿µ",
        "Dockerå®¹å™¨æŠ€æœ¯",
        "å¾®æœåŠ¡æ¶æ„è®¾è®¡",
        "ä»€ä¹ˆæ˜¯äº‘åŸç”Ÿï¼Ÿ",  # é‡å¤
        "æœåŠ¡ç½‘æ ¼çš„ä½œç”¨",
        "Kubernetesçš„æ ¸å¿ƒæ¦‚å¿µ",  # é‡å¤
        "CI/CDæµæ°´çº¿è®¾è®¡"
    ]
    
    print("ğŸ“ æ¨¡æ‹ŸæŒç»­è¯·æ±‚ï¼Œç›‘æ§èµ„æºä½¿ç”¨...")
    
    # è®°å½•æ¯ä¸ªè¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯
    request_details = []
    
    for i, question in enumerate(questions):
        messages = [{'role': 'user', 'content': question}]
        
        try:
            start_time = time.time()
            response, from_cache = await optimizer.optimized_request(messages)
            response_time = time.time() - start_time
            
            # è®°å½•è¯·æ±‚è¯¦æƒ…
            detail = {
                'index': i + 1,
                'question': question[:30],
                'response_time': response_time,
                'from_cache': from_cache,
                'tokens': response.usage.total_tokens if response.usage else 0,
                'timestamp': datetime.now()
            }
            request_details.append(detail)
            
            cache_indicator = "ğŸ’¾" if from_cache else "ğŸŒ"
            print(f"   {cache_indicator} è¯·æ±‚ {i+1}: {response_time:.2f}s")
            
            # æ¯éš”å‡ ä¸ªè¯·æ±‚æ˜¾ç¤ºç»Ÿè®¡
            if (i + 1) % 3 == 0:
                current_stats = optimizer.get_performance_stats()
                print(f"      ğŸ“Š å½“å‰ç»Ÿè®¡: ç¼“å­˜å‘½ä¸­ç‡ {current_stats['cache_hit_rate']:.1%}, "
                      f"å¹³å‡å“åº”æ—¶é—´ {current_stats['avg_response_time']:.2f}s")
            
        except Exception as e:
            print(f"   âŒ è¯·æ±‚ {i+1}: {e}")
    
    # æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š
    final_stats = optimizer.get_performance_stats()
    
    print(f"\nğŸ“Š æœ€ç»ˆèµ„æºç›‘æ§æŠ¥å‘Š:")
    print(f"   æ€§èƒ½æŒ‡æ ‡:")
    print(f"     - æ€»è¯·æ±‚æ•°: {final_stats['total_requests'] + final_stats['cached_requests']}")
    print(f"     - å®é™…APIè°ƒç”¨: {final_stats['total_requests']}")
    print(f"     - ç¼“å­˜å“åº”: {final_stats['cached_requests']}")
    print(f"     - ç¼“å­˜å‘½ä¸­ç‡: {final_stats['cache_hit_rate']:.1%}")
    print(f"     - å¹³å‡å“åº”æ—¶é—´: {final_stats['avg_response_time']:.2f}ç§’")
    print(f"     - P95å“åº”æ—¶é—´: {final_stats['p95_response_time']:.2f}ç§’")
    print(f"     - æ€»ååé‡: {final_stats['requests_per_second']:.2f} è¯·æ±‚/ç§’")
    
    print(f"   èµ„æºä½¿ç”¨:")
    print(f"     - æ€»Tokenæ¶ˆè€—: {final_stats['total_tokens']}")
    print(f"     - å¹³å‡Token/è¯·æ±‚: {final_stats['total_tokens'] / max(final_stats['total_requests'], 1):.0f}")
    
    # ç¼“å­˜æ•ˆç‡åˆ†æ
    if optimizer.cache:
        cache_stats = optimizer.cache.get_stats()
        print(f"   ç¼“å­˜æ•ˆç‡:")
        print(f"     - ç¼“å­˜æ¡ç›®æ•°: {cache_stats['cache_size']}")
        print(f"     - ç¼“å­˜åˆ©ç”¨ç‡: {cache_stats['cache_size'] / cache_stats['max_size']:.1%}")
        print(f"     - å‘½ä¸­æ¬¡æ•°: {cache_stats['hit_count']}")
        print(f"     - æœªå‘½ä¸­æ¬¡æ•°: {cache_stats['miss_count']}")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ HarborAI æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    client, model = get_client()
    if not client:
        print("âš ï¸ è­¦å‘Š: æœªè®¾ç½®ä»»ä½• API Key")
        print("è¯·è®¾ç½® DEEPSEEK_API_KEY, ERNIE_API_KEY, æˆ– DOUBAO_API_KEY")
        return
    
    print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: {model}")
    
    demos = [
        ("ç¼“å­˜æ€§èƒ½ä¼˜åŒ–", demo_cache_performance),
        ("å¹¶å‘ä¼˜åŒ–", demo_concurrent_optimization),
        ("æµå¼å¤„ç†ä¼˜åŒ–", demo_streaming_optimization),
        ("æ‰¹é‡ä¼˜åŒ–", demo_batch_optimization),
        ("å“åº”æ—¶é—´ä¼˜åŒ–", demo_response_time_optimization),
        ("èµ„æºç›‘æ§", demo_resource_monitoring)
    ]
    
    for name, demo_func in demos:
        try:
            await demo_func()
            await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        except Exception as e:
            print(f"âŒ {name} æ¼”ç¤ºå¤±è´¥: {e}")
    
    print("\nğŸ‰ æ€§èƒ½ä¼˜åŒ–æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ å…³é”®ä¼˜åŒ–ç­–ç•¥:")
    print("1. æ™ºèƒ½ç¼“å­˜ - å‡å°‘é‡å¤APIè°ƒç”¨ï¼Œæ˜¾è‘—æå‡å“åº”é€Ÿåº¦")
    print("2. å¹¶å‘æ§åˆ¶ - åˆç†çš„å¹¶å‘æ•°ï¼Œå¹³è¡¡é€Ÿåº¦å’Œèµ„æºä½¿ç”¨")
    print("3. æµå¼å¤„ç† - é™ä½é¦–å­—èŠ‚æ—¶é—´ï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒ")
    print("4. æ‰¹é‡ä¼˜åŒ– - é«˜æ•ˆå¤„ç†å¤§é‡è¯·æ±‚ï¼Œæé«˜ååé‡")
    print("5. èµ„æºç›‘æ§ - å®æ—¶ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼ŒåŠæ—¶ä¼˜åŒ–è°ƒæ•´")
    print("6. é™çº§ç­–ç•¥ - å†…ç½®fallbackæœºåˆ¶ï¼Œç¡®ä¿æœåŠ¡å¯ç”¨æ€§")

if __name__ == "__main__":
    asyncio.run(main())