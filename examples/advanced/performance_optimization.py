#!/usr/bin/env python3
"""
HarborAI æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º

åœºæ™¯æè¿°:
åœ¨é«˜å¹¶å‘ã€å¤§æµé‡çš„ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œé€šè¿‡æ™ºèƒ½ç¼“å­˜ã€è¿æ¥æ± ç®¡ç†ã€è¯·æ±‚é¢„æµ‹ç­‰
å¤šç§ä¼˜åŒ–æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡ç³»ç»Ÿå“åº”é€Ÿåº¦å’Œèµ„æºåˆ©ç”¨æ•ˆç‡ã€‚

åº”ç”¨ä»·å€¼:
- æ˜¾è‘—æå‡å“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒ
- å‡å°‘èµ„æºæ¶ˆè€—å’Œè¿è¥æˆæœ¬
- æé«˜ç³»ç»Ÿå¹¶å‘å¤„ç†èƒ½åŠ›
- ä¼˜åŒ–APIè°ƒç”¨æ•ˆç‡

æ ¸å¿ƒåŠŸèƒ½:
1. æ™ºèƒ½ç¼“å­˜ç­–ç•¥
2. è¿æ¥æ± ç®¡ç†
3. è¯·æ±‚é¢„æµ‹ä¸é¢„åŠ è½½
4. æ€§èƒ½åŸºå‡†æµ‹è¯•
5. èµ„æºç›‘æ§ä¸è°ƒä¼˜
"""

import asyncio
import time
import hashlib
import pickle
import random
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable, Any, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
import threading
from collections import OrderedDict, defaultdict
import psutil
import aiohttp
from openai import OpenAI, AsyncOpenAI
from openai.types.chat import ChatCompletion

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CacheStrategy(Enum):
    """ç¼“å­˜ç­–ç•¥"""
    LRU = "lru"          # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "lfu"          # æœ€å°‘ä½¿ç”¨é¢‘ç‡
    TTL = "ttl"          # æ—¶é—´è¿‡æœŸ
    ADAPTIVE = "adaptive" # è‡ªé€‚åº”

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    value: Any
    created_at: datetime
    last_accessed: datetime
    access_count: int = 0
    ttl: Optional[float] = None
    
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if self.ttl is None:
            return False
        return (datetime.now() - self.created_at).total_seconds() > self.ttl
    
    def touch(self):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        self.last_accessed = datetime.now()
        self.access_count += 1

class IntelligentCache:
    """æ™ºèƒ½ç¼“å­˜"""
    
    def __init__(self, 
                 max_size: int = 1000,
                 default_ttl: float = 3600,
                 strategy: CacheStrategy = CacheStrategy.ADAPTIVE):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.strategy = strategy
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "total_requests": 0
        }
        self._lock = threading.RLock()
    
    def _generate_key(self, messages: List[Dict], model: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ç›¸å…³å‚æ•°çš„å­—ç¬¦ä¸²
        cache_data = {
            "messages": messages,
            "model": model,
            **{k: v for k, v in kwargs.items() if k not in ["stream", "timeout"]}
        }
        
        # ä½¿ç”¨JSONåºåˆ—åŒ–å¹¶è®¡ç®—å“ˆå¸Œ
        cache_str = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def get(self, messages: List[Dict], model: str, **kwargs) -> Optional[ChatCompletion]:
        """è·å–ç¼“å­˜"""
        key = self._generate_key(messages, model, **kwargs)
        
        with self._lock:
            self.stats["total_requests"] += 1
            
            if key in self.cache:
                entry = self.cache[key]
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if entry.is_expired():
                    del self.cache[key]
                    self.stats["misses"] += 1
                    return None
                
                # æ›´æ–°è®¿é—®ä¿¡æ¯
                entry.touch()
                
                # LRUç­–ç•¥ï¼šç§»åŠ¨åˆ°æœ«å°¾
                if self.strategy in [CacheStrategy.LRU, CacheStrategy.ADAPTIVE]:
                    self.cache.move_to_end(key)
                
                self.stats["hits"] += 1
                logger.debug(f"Cache hit for key: {key[:8]}...")
                return entry.value
            
            self.stats["misses"] += 1
            return None
    
    def put(self, messages: List[Dict], model: str, response: ChatCompletion, **kwargs):
        """å­˜å‚¨ç¼“å­˜"""
        key = self._generate_key(messages, model, **kwargs)
        
        with self._lock:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œæ‰§è¡Œæ·˜æ±°ç­–ç•¥
            if len(self.cache) >= self.max_size:
                self._evict()
            
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            entry = CacheEntry(
                key=key,
                value=response,
                created_at=datetime.now(),
                last_accessed=datetime.now(),
                ttl=self.default_ttl
            )
            
            self.cache[key] = entry
            logger.debug(f"Cached response for key: {key[:8]}...")
    
    def _evict(self):
        """æ·˜æ±°ç¼“å­˜æ¡ç›®"""
        if not self.cache:
            return
        
        if self.strategy == CacheStrategy.LRU:
            # åˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„
            self.cache.popitem(last=False)
        elif self.strategy == CacheStrategy.LFU:
            # åˆ é™¤ä½¿ç”¨é¢‘ç‡æœ€ä½çš„
            min_key = min(self.cache.keys(), key=lambda k: self.cache[k].access_count)
            del self.cache[min_key]
        elif self.strategy == CacheStrategy.TTL:
            # åˆ é™¤æœ€æ—©è¿‡æœŸçš„
            now = datetime.now()
            expired_keys = [k for k, v in self.cache.items() if v.is_expired()]
            if expired_keys:
                del self.cache[expired_keys[0]]
            else:
                self.cache.popitem(last=False)
        else:  # ADAPTIVE
            # è‡ªé€‚åº”ç­–ç•¥ï¼šç»“åˆLRUå’ŒLFU
            now = datetime.now()
            
            # é¦–å…ˆåˆ é™¤è¿‡æœŸçš„
            expired_keys = [k for k, v in self.cache.items() if v.is_expired()]
            if expired_keys:
                del self.cache[expired_keys[0]]
            else:
                # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆæ—¶é—´ + é¢‘ç‡ï¼‰
                scores = {}
                for key, entry in self.cache.items():
                    time_score = (now - entry.last_accessed).total_seconds()
                    freq_score = 1.0 / (entry.access_count + 1)
                    scores[key] = time_score + freq_score * 100
                
                # åˆ é™¤åˆ†æ•°æœ€é«˜çš„ï¼ˆæœ€ä¹…æœªä½¿ç”¨ä¸”é¢‘ç‡æœ€ä½ï¼‰
                worst_key = max(scores.keys(), key=lambda k: scores[k])
                del self.cache[worst_key]
        
        self.stats["evictions"] += 1
    
    def get_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        if self.stats["total_requests"] == 0:
            return 0.0
        return self.stats["hits"] / self.stats["total_requests"]
    
    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            **self.stats,
            "hit_rate": self.get_hit_rate(),
            "cache_size": len(self.cache),
            "max_size": self.max_size
        }
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self.cache.clear()
            self.stats = {
                "hits": 0,
                "misses": 0,
                "evictions": 0,
                "total_requests": 0
            }

class ConnectionPool:
    """è¿æ¥æ± ç®¡ç†"""
    
    def __init__(self, 
                 max_connections: int = 20,
                 max_keepalive_connections: int = 10,
                 keepalive_expiry: float = 30.0):
        self.max_connections = max_connections
        self.max_keepalive_connections = max_keepalive_connections
        self.keepalive_expiry = keepalive_expiry
        
        # è¿æ¥æ± ç»Ÿè®¡
        self.stats = {
            "total_connections": 0,
            "active_connections": 0,
            "reused_connections": 0,
            "connection_errors": 0
        }
        
        # åˆ›å»ºè¿æ¥å™¨
        self.connector = aiohttp.TCPConnector(
            limit=max_connections,
            limit_per_host=max_connections,
            keepalive_timeout=keepalive_expiry,
            enable_cleanup_closed=True
        )
    
    async def get_session(self) -> aiohttp.ClientSession:
        """è·å–HTTPä¼šè¯"""
        self.stats["active_connections"] += 1
        
        return aiohttp.ClientSession(
            connector=self.connector,
            timeout=aiohttp.ClientTimeout(total=30)
        )
    
    def get_stats(self) -> Dict:
        """è·å–è¿æ¥æ± ç»Ÿè®¡"""
        return {
            **self.stats,
            "max_connections": self.max_connections,
            "connector_stats": {
                "total_connections": len(self.connector._conns),
                "available_connections": sum(len(conns) for conns in self.connector._conns.values())
            }
        }
    
    async def close(self):
        """å…³é—­è¿æ¥æ± """
        await self.connector.close()

class RequestPredictor:
    """è¯·æ±‚é¢„æµ‹å™¨"""
    
    def __init__(self, history_size: int = 1000):
        self.history_size = history_size
        self.request_history: List[Dict] = []
        self.pattern_cache: Dict[str, List[str]] = {}
        
    def record_request(self, messages: List[Dict], model: str):
        """è®°å½•è¯·æ±‚"""
        request_info = {
            "timestamp": datetime.now(),
            "messages": messages,
            "model": model,
            "content_hash": self._hash_content(messages)
        }
        
        self.request_history.append(request_info)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.request_history) > self.history_size:
            self.request_history = self.request_history[-self.history_size:]
    
    def _hash_content(self, messages: List[Dict]) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        content = " ".join([msg.get("content", "") for msg in messages])
        return hashlib.md5(content.encode()).hexdigest()[:8]
    
    def predict_next_requests(self, current_messages: List[Dict], limit: int = 5) -> List[Dict]:
        """é¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½çš„è¯·æ±‚"""
        if len(self.request_history) < 2:
            return []
        
        current_hash = self._hash_content(current_messages)
        
        # æŸ¥æ‰¾ç›¸ä¼¼çš„å†å²è¯·æ±‚
        similar_requests = []
        for i, request in enumerate(self.request_history[:-1]):
            if request["content_hash"] == current_hash:
                # æ‰¾åˆ°ç›¸ä¼¼è¯·æ±‚ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯·æ±‚
                next_request = self.request_history[i + 1]
                similar_requests.append(next_request)
        
        # è¿”å›æœ€å¸¸è§çš„åç»­è¯·æ±‚
        if similar_requests:
            # æŒ‰é¢‘ç‡æ’åº
            hash_counts = defaultdict(int)
            hash_to_request = {}
            
            for req in similar_requests:
                req_hash = req["content_hash"]
                hash_counts[req_hash] += 1
                hash_to_request[req_hash] = req
            
            # è¿”å›æœ€é¢‘ç¹çš„è¯·æ±‚
            sorted_hashes = sorted(hash_counts.keys(), key=lambda h: hash_counts[h], reverse=True)
            return [hash_to_request[h] for h in sorted_hashes[:limit]]
        
        return []

class PerformanceOptimizedClient:
    """æ€§èƒ½ä¼˜åŒ–å®¢æˆ·ç«¯"""
    
    def __init__(self,
                 api_key: str,
                 base_url: str = "https://api.harborai.com/v1",
                 cache_size: int = 1000,
                 cache_ttl: float = 3600,
                 max_connections: int = 20,
                 enable_prediction: bool = True):
        
        # åŸºç¡€å®¢æˆ·ç«¯
        self.async_client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        
        # æ€§èƒ½ä¼˜åŒ–ç»„ä»¶
        self.cache = IntelligentCache(max_size=cache_size, default_ttl=cache_ttl)
        self.connection_pool = ConnectionPool(max_connections=max_connections)
        self.predictor = RequestPredictor() if enable_prediction else None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_response_time": 0.0,
            "prediction_hits": 0,
            "preloaded_requests": 0
        }
        
        # é¢„åŠ è½½ä»»åŠ¡
        self.preload_tasks: Dict[str, asyncio.Task] = {}
    
    async def chat_completion(self, 
                            messages: List[Dict], 
                            model: str = "deepseek-chat",
                            use_cache: bool = True,
                            enable_preload: bool = True,
                            **kwargs) -> ChatCompletion:
        """ä¼˜åŒ–çš„èŠå¤©å®Œæˆ"""
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        # 1. å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_response = self.cache.get(messages, model, **kwargs)
            if cached_response:
                self.stats["cache_hits"] += 1
                response_time = time.time() - start_time
                self.stats["total_response_time"] += response_time
                
                logger.debug(f"Cache hit - Response time: {response_time:.3f}s")
                
                # è®°å½•è¯·æ±‚ç”¨äºé¢„æµ‹
                if self.predictor:
                    self.predictor.record_request(messages, model)
                
                # è§¦å‘é¢„åŠ è½½
                if enable_preload:
                    await self._trigger_preload(messages, model, **kwargs)
                
                return cached_response
        
        # 2. å‘é€å®é™…è¯·æ±‚
        try:
            response = await self.async_client.chat.completions.create(
                model=model,
                messages=messages,
                **kwargs
            )
            
            response_time = time.time() - start_time
            self.stats["total_response_time"] += response_time
            self.stats["cache_misses"] += 1
            
            logger.debug(f"API call - Response time: {response_time:.3f}s")
            
            # 3. å­˜å‚¨åˆ°ç¼“å­˜
            if use_cache:
                self.cache.put(messages, model, response, **kwargs)
            
            # 4. è®°å½•è¯·æ±‚ç”¨äºé¢„æµ‹
            if self.predictor:
                self.predictor.record_request(messages, model)
            
            # 5. è§¦å‘é¢„åŠ è½½
            if enable_preload:
                await self._trigger_preload(messages, model, **kwargs)
            
            return response
            
        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            raise e
    
    async def _trigger_preload(self, messages: List[Dict], model: str, **kwargs):
        """è§¦å‘é¢„åŠ è½½"""
        if not self.predictor:
            return
        
        # é¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½çš„è¯·æ±‚
        predicted_requests = self.predictor.predict_next_requests(messages, limit=3)
        
        for predicted in predicted_requests:
            pred_messages = predicted["messages"]
            pred_model = predicted.get("model", model)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ç¼“å­˜ä¸­
            if self.cache.get(pred_messages, pred_model, **kwargs):
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨é¢„åŠ è½½
            pred_key = self.cache._generate_key(pred_messages, pred_model, **kwargs)
            if pred_key in self.preload_tasks:
                continue
            
            # å¯åŠ¨é¢„åŠ è½½ä»»åŠ¡
            task = asyncio.create_task(self._preload_request(pred_messages, pred_model, pred_key, **kwargs))
            self.preload_tasks[pred_key] = task
    
    async def _preload_request(self, messages: List[Dict], model: str, key: str, **kwargs):
        """é¢„åŠ è½½è¯·æ±‚"""
        try:
            logger.debug(f"Preloading request: {key[:8]}...")
            
            response = await self.async_client.chat.completions.create(
                model=model,
                messages=messages,
                **kwargs
            )
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.cache.put(messages, model, response, **kwargs)
            self.stats["preloaded_requests"] += 1
            
            logger.debug(f"Preload completed: {key[:8]}...")
            
        except Exception as e:
            logger.warning(f"Preload failed: {str(e)}")
        finally:
            # æ¸…ç†ä»»åŠ¡
            if key in self.preload_tasks:
                del self.preload_tasks[key]
    
    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        avg_response_time = 0.0
        if self.stats["total_requests"] > 0:
            avg_response_time = self.stats["total_response_time"] / self.stats["total_requests"]
        
        return {
            "requests": self.stats,
            "cache": self.cache.get_stats(),
            "connection_pool": self.connection_pool.get_stats(),
            "average_response_time": avg_response_time,
            "active_preload_tasks": len(self.preload_tasks)
        }
    
    async def warmup_cache(self, warmup_requests: List[Tuple[List[Dict], str]]):
        """é¢„çƒ­ç¼“å­˜"""
        logger.info(f"Warming up cache with {len(warmup_requests)} requests...")
        
        tasks = []
        for messages, model in warmup_requests:
            task = asyncio.create_task(self.chat_completion(messages, model, enable_preload=False))
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œé¢„çƒ­è¯·æ±‚
        await asyncio.gather(*tasks, return_exceptions=True)
        
        logger.info("Cache warmup completed")
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        # å–æ¶ˆæ‰€æœ‰é¢„åŠ è½½ä»»åŠ¡
        for task in self.preload_tasks.values():
            task.cancel()
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        if self.preload_tasks:
            await asyncio.gather(*self.preload_tasks.values(), return_exceptions=True)
        
        # å…³é—­è¿æ¥æ± 
        await self.connection_pool.close()

# æ¼”ç¤ºå‡½æ•°
async def demo_cache_performance():
    """æ¼”ç¤ºç¼“å­˜æ€§èƒ½"""
    print("\nğŸš€ ç¼“å­˜æ€§èƒ½æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºä¼˜åŒ–å®¢æˆ·ç«¯
    client = PerformanceOptimizedClient(
        api_key="your-api-key-here",
        cache_size=100,
        cache_ttl=300  # 5åˆ†é’Ÿ
    )
    
    # æµ‹è¯•è¯·æ±‚
    test_requests = [
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}],
        [{"role": "user", "content": "è§£é‡Šæœºå™¨å­¦ä¹ "}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}],  # é‡å¤è¯·æ±‚
        [{"role": "user", "content": "è§£é‡Šæœºå™¨å­¦ä¹ "}],     # é‡å¤è¯·æ±‚
    ]
    
    print("ğŸ”„ æ‰§è¡Œæµ‹è¯•è¯·æ±‚...")
    
    for i, messages in enumerate(test_requests):
        try:
            start_time = time.time()
            response = await client.chat_completion(messages)
            end_time = time.time()
            
            print(f"âœ… è¯·æ±‚ {i+1}: {end_time - start_time:.3f}s")
            
        except Exception as e:
            print(f"âŒ è¯·æ±‚ {i+1} å¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    stats = client.get_performance_stats()
    print(f"\nğŸ“Š ç¼“å­˜æ€§èƒ½ç»Ÿè®¡:")
    print(f"   - æ€»è¯·æ±‚æ•°: {stats['requests']['total_requests']}")
    print(f"   - ç¼“å­˜å‘½ä¸­: {stats['requests']['cache_hits']}")
    print(f"   - ç¼“å­˜æœªå‘½ä¸­: {stats['requests']['cache_misses']}")
    print(f"   - ç¼“å­˜å‘½ä¸­ç‡: {stats['cache']['hit_rate']:.1%}")
    print(f"   - å¹³å‡å“åº”æ—¶é—´: {stats['average_response_time']:.3f}s")
    
    await client.close()

async def demo_connection_pool():
    """æ¼”ç¤ºè¿æ¥æ± ä¼˜åŒ–"""
    print("\nğŸ”— è¿æ¥æ± ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºä¸åŒé…ç½®çš„å®¢æˆ·ç«¯è¿›è¡Œå¯¹æ¯”
    
    # 1. æ— è¿æ¥æ± ä¼˜åŒ–çš„å®¢æˆ·ç«¯
    normal_client = AsyncOpenAI(api_key="your-api-key-here", base_url="https://api.harborai.com/v1")
    
    # 2. è¿æ¥æ± ä¼˜åŒ–çš„å®¢æˆ·ç«¯
    optimized_client = PerformanceOptimizedClient(
        api_key="your-api-key-here",
        max_connections=10,
        cache_size=0  # ç¦ç”¨ç¼“å­˜ä»¥æµ‹è¯•çº¯è¿æ¥æ± æ€§èƒ½
    )
    
    test_messages = [{"role": "user", "content": f"æµ‹è¯•è¯·æ±‚ {i}"} for i in range(5)]
    
    # æµ‹è¯•æ™®é€šå®¢æˆ·ç«¯
    print("ğŸ”„ æµ‹è¯•æ™®é€šå®¢æˆ·ç«¯...")
    normal_start = time.time()
    normal_tasks = []
    
    for messages in test_messages:
        task = asyncio.create_task(normal_client.chat.completions.create(
            model="deepseek-chat",
            messages=[messages],
            max_tokens=50
        ))
        normal_tasks.append(task)
    
    try:
        await asyncio.gather(*normal_tasks)
        normal_time = time.time() - normal_start
        print(f"âœ… æ™®é€šå®¢æˆ·ç«¯å®Œæˆæ—¶é—´: {normal_time:.3f}s")
    except Exception as e:
        print(f"âŒ æ™®é€šå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: {str(e)}")
        normal_time = float('inf')
    
    # æµ‹è¯•ä¼˜åŒ–å®¢æˆ·ç«¯
    print("ğŸ”„ æµ‹è¯•è¿æ¥æ± ä¼˜åŒ–å®¢æˆ·ç«¯...")
    optimized_start = time.time()
    optimized_tasks = []
    
    for messages in test_messages:
        task = asyncio.create_task(optimized_client.chat_completion(
            [messages],
            model="deepseek-chat",
            max_tokens=50,
            use_cache=False
        ))
        optimized_tasks.append(task)
    
    try:
        await asyncio.gather(*optimized_tasks)
        optimized_time = time.time() - optimized_start
        print(f"âœ… ä¼˜åŒ–å®¢æˆ·ç«¯å®Œæˆæ—¶é—´: {optimized_time:.3f}s")
        
        # æ˜¾ç¤ºè¿æ¥æ± ç»Ÿè®¡
        stats = optimized_client.get_performance_stats()
        print(f"ğŸ“Š è¿æ¥æ± ç»Ÿè®¡:")
        print(f"   - æœ€å¤§è¿æ¥æ•°: {stats['connection_pool']['max_connections']}")
        print(f"   - æ´»è·ƒè¿æ¥æ•°: {stats['connection_pool']['active_connections']}")
        
        if normal_time != float('inf') and optimized_time > 0:
            improvement = (normal_time - optimized_time) / normal_time * 100
            print(f"   - æ€§èƒ½æå‡: {improvement:.1f}%")
        
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–å®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: {str(e)}")
    
    await optimized_client.close()

async def demo_request_prediction():
    """æ¼”ç¤ºè¯·æ±‚é¢„æµ‹"""
    print("\nğŸ§  è¯·æ±‚é¢„æµ‹æ¼”ç¤º")
    print("=" * 50)
    
    client = PerformanceOptimizedClient(
        api_key="your-api-key-here",
        enable_prediction=True
    )
    
    # æ¨¡æ‹Ÿç”¨æˆ·å¯¹è¯æ¨¡å¼
    conversation_patterns = [
        # æ¨¡å¼1ï¼šAIåŸºç¡€é—®é¢˜åºåˆ—
        [
            [{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}],
            [{"role": "user", "content": "AIæœ‰å“ªäº›åº”ç”¨é¢†åŸŸï¼Ÿ"}],
            [{"role": "user", "content": "AIçš„å‘å±•å‰æ™¯å¦‚ä½•ï¼Ÿ"}]
        ],
        # æ¨¡å¼2ï¼šæŠ€æœ¯æ·±å…¥åºåˆ—
        [
            [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}],
            [{"role": "user", "content": "ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«ï¼Ÿ"}],
            [{"role": "user", "content": "å¦‚ä½•é€‰æ‹©æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ"}]
        ]
    ]
    
    # è®­ç»ƒé¢„æµ‹æ¨¡å‹
    print("ğŸ”„ è®­ç»ƒé¢„æµ‹æ¨¡å‹...")
    for pattern in conversation_patterns:
        for messages in pattern:
            try:
                await client.chat_completion(messages, enable_preload=False)
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
            except Exception as e:
                print(f"âŒ è®­ç»ƒè¯·æ±‚å¤±è´¥: {str(e)}")
    
    # æµ‹è¯•é¢„æµ‹æ•ˆæœ
    print("\nğŸ”„ æµ‹è¯•é¢„æµ‹æ•ˆæœ...")
    
    # å‘é€ç¬¬ä¸€ä¸ªè¯·æ±‚ï¼Œåº”è¯¥è§¦å‘é¢„æµ‹å’Œé¢„åŠ è½½
    test_messages = [{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}]
    
    start_time = time.time()
    response1 = await client.chat_completion(test_messages)
    time1 = time.time() - start_time
    print(f"âœ… ç¬¬ä¸€ä¸ªè¯·æ±‚: {time1:.3f}s")
    
    # ç­‰å¾…é¢„åŠ è½½å®Œæˆ
    await asyncio.sleep(2)
    
    # å‘é€é¢„æµ‹çš„ä¸‹ä¸€ä¸ªè¯·æ±‚
    predicted_messages = [{"role": "user", "content": "AIæœ‰å“ªäº›åº”ç”¨é¢†åŸŸï¼Ÿ"}]
    
    start_time = time.time()
    response2 = await client.chat_completion(predicted_messages)
    time2 = time.time() - start_time
    print(f"âœ… é¢„æµ‹è¯·æ±‚: {time2:.3f}s")
    
    # æ˜¾ç¤ºé¢„æµ‹ç»Ÿè®¡
    stats = client.get_performance_stats()
    print(f"\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    print(f"   - é¢„åŠ è½½è¯·æ±‚æ•°: {stats['requests']['preloaded_requests']}")
    print(f"   - æ´»è·ƒé¢„åŠ è½½ä»»åŠ¡: {stats['active_preload_tasks']}")
    print(f"   - ç¼“å­˜å‘½ä¸­ç‡: {stats['cache']['hit_rate']:.1%}")
    
    if time2 < time1:
        speedup = (time1 - time2) / time1 * 100
        print(f"   - é¢„æµ‹åŠ é€Ÿ: {speedup:.1f}%")
    
    await client.close()

async def demo_comprehensive_optimization():
    """æ¼”ç¤ºç»¼åˆä¼˜åŒ–æ•ˆæœ"""
    print("\nâš¡ ç»¼åˆä¼˜åŒ–æ•ˆæœæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºä¸åŒé…ç½®çš„å®¢æˆ·ç«¯
    clients = {
        "åŸºç¡€å®¢æˆ·ç«¯": AsyncOpenAI(api_key="your-api-key-here", base_url="https://api.harborai.com/v1"),
        "ç¼“å­˜ä¼˜åŒ–": PerformanceOptimizedClient(
            api_key="your-api-key-here",
            cache_size=100,
            max_connections=5,
            enable_prediction=False
        ),
        "å…¨é¢ä¼˜åŒ–": PerformanceOptimizedClient(
            api_key="your-api-key-here",
            cache_size=100,
            max_connections=10,
            enable_prediction=True
        )
    }
    
    # æµ‹è¯•åœºæ™¯ï¼šé‡å¤è¯·æ±‚å’Œç›¸å…³è¯·æ±‚
    test_scenarios = [
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}],  # é‡å¤
        [{"role": "user", "content": "AIçš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ"}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}],  # é‡å¤
    ]
    
    results = {}
    
    for client_name, client in clients.items():
        print(f"\nğŸ”„ æµ‹è¯• {client_name}...")
        
        start_time = time.time()
        successful_requests = 0
        
        for i, messages in enumerate(test_scenarios):
            try:
                if client_name == "åŸºç¡€å®¢æˆ·ç«¯":
                    await client.chat.completions.create(
                        model="deepseek-chat",
                        messages=messages,
                        max_tokens=100
                    )
                else:
                    await client.chat_completion(
                        messages,
                        model="deepseek-chat",
                        max_tokens=100
                    )
                successful_requests += 1
                
            except Exception as e:
                print(f"âŒ è¯·æ±‚ {i+1} å¤±è´¥: {str(e)}")
        
        total_time = time.time() - start_time
        
        results[client_name] = {
            "total_time": total_time,
            "successful_requests": successful_requests,
            "avg_time_per_request": total_time / successful_requests if successful_requests > 0 else 0
        }
        
        print(f"âœ… {client_name} å®Œæˆ:")
        print(f"   - æ€»æ—¶é—´: {total_time:.3f}s")
        print(f"   - æˆåŠŸè¯·æ±‚: {successful_requests}")
        print(f"   - å¹³å‡æ—¶é—´: {results[client_name]['avg_time_per_request']:.3f}s/req")
        
        # æ˜¾ç¤ºä¼˜åŒ–å®¢æˆ·ç«¯çš„è¯¦ç»†ç»Ÿè®¡
        if hasattr(client, 'get_performance_stats'):
            stats = client.get_performance_stats()
            print(f"   - ç¼“å­˜å‘½ä¸­ç‡: {stats['cache']['hit_rate']:.1%}")
            if stats['requests']['preloaded_requests'] > 0:
                print(f"   - é¢„åŠ è½½è¯·æ±‚: {stats['requests']['preloaded_requests']}")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    baseline = results["åŸºç¡€å®¢æˆ·ç«¯"]["avg_time_per_request"]
    
    for client_name, result in results.items():
        if client_name != "åŸºç¡€å®¢æˆ·ç«¯" and baseline > 0:
            improvement = (baseline - result["avg_time_per_request"]) / baseline * 100
            print(f"   - {client_name}: {improvement:.1f}% æå‡")
    
    # å…³é—­ä¼˜åŒ–å®¢æˆ·ç«¯
    for client_name, client in clients.items():
        if hasattr(client, 'close'):
            await client.close()

async def demo_memory_optimization():
    """æ¼”ç¤ºå†…å­˜ä¼˜åŒ–"""
    print("\nğŸ§  å†…å­˜ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    # ç›‘æ§å†…å­˜ä½¿ç”¨
    def get_memory_usage():
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024  # MB
    
    initial_memory = get_memory_usage()
    print(f"ğŸ“Š åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.1f}MB")
    
    # åˆ›å»ºå¤§ç¼“å­˜å®¢æˆ·ç«¯
    client = PerformanceOptimizedClient(
        api_key="your-api-key-here",
        cache_size=500,  # è¾ƒå¤§çš„ç¼“å­˜
        cache_ttl=3600
    )
    
    # ç”Ÿæˆå¤§é‡ä¸åŒçš„è¯·æ±‚
    print("ğŸ”„ ç”Ÿæˆå¤§é‡ç¼“å­˜æ•°æ®...")
    for i in range(50):
        messages = [{"role": "user", "content": f"æµ‹è¯•é—®é¢˜ {i}: è¯·è§£é‡Šæ¦‚å¿µ{i}"}]
        try:
            await client.chat_completion(messages, max_tokens=50)
        except Exception as e:
            print(f"âŒ è¯·æ±‚ {i} å¤±è´¥: {str(e)}")
        
        # æ¯10ä¸ªè¯·æ±‚æ£€æŸ¥ä¸€æ¬¡å†…å­˜
        if i % 10 == 0:
            current_memory = get_memory_usage()
            print(f"   è¯·æ±‚ {i}: å†…å­˜ä½¿ç”¨ {current_memory:.1f}MB (+{current_memory - initial_memory:.1f}MB)")
    
    # æœ€ç»ˆå†…å­˜ç»Ÿè®¡
    final_memory = get_memory_usage()
    cache_stats = client.get_performance_stats()["cache"]
    
    print(f"\nğŸ“Š å†…å­˜ä¼˜åŒ–ç»Ÿè®¡:")
    print(f"   - åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
    print(f"   - æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")
    print(f"   - å†…å­˜å¢é•¿: {final_memory - initial_memory:.1f}MB")
    print(f"   - ç¼“å­˜å¤§å°: {cache_stats['cache_size']}")
    print(f"   - ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1%}")
    print(f"   - ç¼“å­˜æ·˜æ±°æ¬¡æ•°: {cache_stats['evictions']}")
    
    # æ¸…ç†ç¼“å­˜å¹¶æ£€æŸ¥å†…å­˜é‡Šæ”¾
    client.cache.clear()
    await asyncio.sleep(1)  # ç­‰å¾…åƒåœ¾å›æ”¶
    
    cleared_memory = get_memory_usage()
    print(f"   - æ¸…ç†åå†…å­˜: {cleared_memory:.1f}MB")
    print(f"   - é‡Šæ”¾å†…å­˜: {final_memory - cleared_memory:.1f}MB")
    
    await client.close()

async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("âš¡ HarborAI æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # ç¼“å­˜æ€§èƒ½æ¼”ç¤º
        await demo_cache_performance()
        
        # è¿æ¥æ± ä¼˜åŒ–æ¼”ç¤º
        await demo_connection_pool()
        
        # è¯·æ±‚é¢„æµ‹æ¼”ç¤º
        await demo_request_prediction()
        
        # ç»¼åˆä¼˜åŒ–æ•ˆæœæ¼”ç¤º
        await demo_comprehensive_optimization()
        
        # å†…å­˜ä¼˜åŒ–æ¼”ç¤º
        await demo_memory_optimization()
        
        print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ç”Ÿäº§ç¯å¢ƒå»ºè®®:")
        print("   1. æ ¹æ®ä¸šåŠ¡ç‰¹ç‚¹è°ƒæ•´ç¼“å­˜ç­–ç•¥å’Œå¤§å°")
        print("   2. ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡å’Œå†…å­˜ä½¿ç”¨æƒ…å†µ")
        print("   3. åˆç†é…ç½®è¿æ¥æ± å‚æ•°")
        print("   4. åˆ©ç”¨è¯·æ±‚æ¨¡å¼è¿›è¡Œæ™ºèƒ½é¢„åŠ è½½")
        print("   5. å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜å’Œç›‘æ§æ€§èƒ½æŒ‡æ ‡")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())