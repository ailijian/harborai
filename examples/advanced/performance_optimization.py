#!/usr/bin/env python3
"""
æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº† HarborAI çš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬ï¼š
1. æ™ºèƒ½ç¼“å­˜ç­–ç•¥
2. è¿æ¥æ± ç®¡ç†
3. è¯·æ±‚é¢„æµ‹ä¸é¢„åŠ è½½
4. æ€§èƒ½åŸºå‡†æµ‹è¯•
5. èµ„æºç›‘æ§ä¸è°ƒä¼˜

åœºæ™¯ï¼š
- é«˜å¹¶å‘ã€å¤§æµé‡çš„ç”Ÿäº§ç¯å¢ƒ
- éœ€è¦å¿«é€Ÿå“åº”çš„å®æ—¶åº”ç”¨
- èµ„æºæ•æ„Ÿçš„æˆæœ¬ä¼˜åŒ–åœºæ™¯

ä»·å€¼ï¼š
- æ˜¾è‘—æå‡å“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒ
- å‡å°‘èµ„æºæ¶ˆè€—å’Œè¿è¥æˆæœ¬
- æé«˜ç³»ç»Ÿå¹¶å‘å¤„ç†èƒ½åŠ›
- ä¼˜åŒ–APIè°ƒç”¨æ•ˆç‡
"""

import asyncio
import time
import hashlib
import random
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable, Any, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
import threading
from collections import OrderedDict, defaultdict

# å¯¼å…¥é…ç½®åŠ©æ‰‹
from config_helper import get_model_configs, get_primary_model_config, print_available_models

# å¯¼å…¥ HarborAI
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from harborai import HarborAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CacheStrategy(Enum):
    """ç¼“å­˜ç­–ç•¥"""
    LRU = "lru"          # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "lfu"          # æœ€å°‘ä½¿ç”¨é¢‘ç‡
    TTL = "ttl"          # æ—¶é—´è¿‡æœŸ
    ADAPTIVE = "adaptive" # è‡ªé€‚åº”

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    value: Any
    created_at: datetime
    last_accessed: datetime
    access_count: int = 0
    ttl: Optional[float] = None
    
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if self.ttl is None:
            return False
        return (datetime.now() - self.created_at).total_seconds() > self.ttl
    
    def touch(self):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        self.last_accessed = datetime.now()
        self.access_count += 1

class IntelligentCache:
    """æ™ºèƒ½ç¼“å­˜"""
    
    def __init__(self, 
                 max_size: int = 1000,
                 default_ttl: float = 3600,
                 strategy: CacheStrategy = CacheStrategy.ADAPTIVE):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.strategy = strategy
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "total_requests": 0
        }
        self._lock = threading.RLock()
    
    def _generate_key(self, messages: List[Dict], model: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ç›¸å…³å‚æ•°çš„å­—ç¬¦ä¸²
        cache_data = {
            "messages": messages,
            "model": model,
            **{k: v for k, v in kwargs.items() if k not in ["stream", "timeout"]}
        }
        
        # ä½¿ç”¨JSONåºåˆ—åŒ–å¹¶è®¡ç®—å“ˆå¸Œ
        cache_str = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def get(self, messages: List[Dict], model: str, **kwargs) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        key = self._generate_key(messages, model, **kwargs)
        
        with self._lock:
            self.stats["total_requests"] += 1
            
            if key in self.cache:
                entry = self.cache[key]
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if entry.is_expired():
                    del self.cache[key]
                    self.stats["misses"] += 1
                    return None
                
                # æ›´æ–°è®¿é—®ä¿¡æ¯
                entry.touch()
                
                # LRUç­–ç•¥ï¼šç§»åŠ¨åˆ°æœ«å°¾
                if self.strategy in [CacheStrategy.LRU, CacheStrategy.ADAPTIVE]:
                    self.cache.move_to_end(key)
                
                self.stats["hits"] += 1
                logger.debug(f"Cache hit for key: {key[:8]}...")
                return entry.value
            
            self.stats["misses"] += 1
            return None
    
    def put(self, messages: List[Dict], model: str, response: Any, **kwargs):
        """å­˜å‚¨ç¼“å­˜"""
        key = self._generate_key(messages, model, **kwargs)
        
        with self._lock:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œæ‰§è¡Œæ·˜æ±°ç­–ç•¥
            if len(self.cache) >= self.max_size:
                self._evict()
            
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            entry = CacheEntry(
                key=key,
                value=response,
                created_at=datetime.now(),
                last_accessed=datetime.now(),
                ttl=self.default_ttl
            )
            
            self.cache[key] = entry
            logger.debug(f"Cached response for key: {key[:8]}...")
    
    def _evict(self):
        """æ·˜æ±°ç¼“å­˜æ¡ç›®"""
        if not self.cache:
            return
        
        if self.strategy == CacheStrategy.LRU:
            # åˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„
            self.cache.popitem(last=False)
        elif self.strategy == CacheStrategy.LFU:
            # åˆ é™¤ä½¿ç”¨é¢‘ç‡æœ€ä½çš„
            min_key = min(self.cache.keys(), key=lambda k: self.cache[k].access_count)
            del self.cache[min_key]
        elif self.strategy == CacheStrategy.TTL:
            # åˆ é™¤æœ€æ—©è¿‡æœŸçš„
            now = datetime.now()
            expired_keys = [k for k, v in self.cache.items() if v.is_expired()]
            if expired_keys:
                del self.cache[expired_keys[0]]
            else:
                self.cache.popitem(last=False)
        else:  # ADAPTIVE
            # è‡ªé€‚åº”ç­–ç•¥ï¼šç»“åˆLRUå’ŒLFU
            now = datetime.now()
            
            # é¦–å…ˆåˆ é™¤è¿‡æœŸçš„
            expired_keys = [k for k, v in self.cache.items() if v.is_expired()]
            if expired_keys:
                del self.cache[expired_keys[0]]
            else:
                # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆæ—¶é—´ + é¢‘ç‡ï¼‰
                scores = {}
                for key, entry in self.cache.items():
                    time_score = (now - entry.last_accessed).total_seconds()
                    freq_score = 1.0 / (entry.access_count + 1)
                    scores[key] = time_score + freq_score * 100
                
                # åˆ é™¤åˆ†æ•°æœ€é«˜çš„ï¼ˆæœ€ä¹…æœªä½¿ç”¨ä¸”é¢‘ç‡æœ€ä½ï¼‰
                worst_key = max(scores.keys(), key=lambda k: scores[k])
                del self.cache[worst_key]
        
        self.stats["evictions"] += 1
    
    def get_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        if self.stats["total_requests"] == 0:
            return 0.0
        return self.stats["hits"] / self.stats["total_requests"]
    
    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            **self.stats,
            "hit_rate": self.get_hit_rate(),
            "cache_size": len(self.cache),
            "max_size": self.max_size
        }
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self.cache.clear()
            self.stats = {
                "hits": 0,
                "misses": 0,
                "evictions": 0,
                "total_requests": 0
            }

class RequestPredictor:
    """è¯·æ±‚é¢„æµ‹å™¨"""
    
    def __init__(self, history_size: int = 1000):
        self.history_size = history_size
        self.request_history: List[Dict] = []
        self.pattern_cache: Dict[str, List[str]] = {}
        
    def record_request(self, messages: List[Dict], model: str):
        """è®°å½•è¯·æ±‚"""
        request_info = {
            "timestamp": datetime.now(),
            "messages": messages,
            "model": model,
            "content_hash": self._hash_content(messages)
        }
        
        self.request_history.append(request_info)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.request_history) > self.history_size:
            self.request_history = self.request_history[-self.history_size:]
    
    def _hash_content(self, messages: List[Dict]) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        content = " ".join([msg.get("content", "") for msg in messages])
        return hashlib.md5(content.encode()).hexdigest()[:8]
    
    def predict_next_requests(self, current_messages: List[Dict], limit: int = 5) -> List[Dict]:
        """é¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½çš„è¯·æ±‚"""
        if len(self.request_history) < 2:
            return []
        
        current_hash = self._hash_content(current_messages)
        
        # æŸ¥æ‰¾ç›¸ä¼¼çš„å†å²è¯·æ±‚
        similar_requests = []
        for i, request in enumerate(self.request_history[:-1]):
            if request["content_hash"] == current_hash:
                # æ‰¾åˆ°ç›¸ä¼¼è¯·æ±‚ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯·æ±‚
                next_request = self.request_history[i + 1]
                similar_requests.append(next_request)
        
        # è¿”å›æœ€å¸¸è§çš„åç»­è¯·æ±‚
        if similar_requests:
            # æŒ‰é¢‘ç‡æ’åº
            hash_counts = defaultdict(int)
            hash_to_request = {}
            
            for req in similar_requests:
                req_hash = req["content_hash"]
                hash_counts[req_hash] += 1
                hash_to_request[req_hash] = req
            
            # è¿”å›æœ€é¢‘ç¹çš„è¯·æ±‚
            sorted_hashes = sorted(hash_counts.keys(), key=lambda h: hash_counts[h], reverse=True)
            return [hash_to_request[h] for h in sorted_hashes[:limit]]
        
        return []

class PerformanceOptimizedClient:
    """æ€§èƒ½ä¼˜åŒ–å®¢æˆ·ç«¯"""
    
    def __init__(self,
                 model_name: Optional[str] = None,
                 cache_size: int = 1000,
                 cache_ttl: float = 3600,
                 enable_prediction: bool = True):
        
        # åŸºç¡€å®¢æˆ·ç«¯
        self.client = HarborAI()
        self.model_name = model_name or get_primary_model_config().model
        
        # æ€§èƒ½ä¼˜åŒ–ç»„ä»¶
        self.cache = IntelligentCache(max_size=cache_size, default_ttl=cache_ttl)
        self.predictor = RequestPredictor() if enable_prediction else None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_response_time": 0.0,
            "prediction_hits": 0,
            "preloaded_requests": 0
        }
        
        # é¢„åŠ è½½ä»»åŠ¡
        self.preload_tasks: Dict[str, asyncio.Task] = {}
    
    async def chat_completion(self, 
                            messages: List[Dict], 
                            model: Optional[str] = None,
                            use_cache: bool = True,
                            enable_preload: bool = True,
                            **kwargs) -> Any:
        """ä¼˜åŒ–çš„èŠå¤©å®Œæˆ"""
        # ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
        model_to_use = model or self.model_name
        
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        # 1. å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_response = self.cache.get(messages, model_to_use, **kwargs)
            if cached_response:
                self.stats["cache_hits"] += 1
                response_time = time.time() - start_time
                self.stats["total_response_time"] += response_time
                
                logger.debug(f"Cache hit - Response time: {response_time:.3f}s")
                
                # è®°å½•è¯·æ±‚ç”¨äºé¢„æµ‹
                if self.predictor:
                    self.predictor.record_request(messages, model_to_use)
                
                # è§¦å‘é¢„åŠ è½½
                if enable_preload:
                    await self._trigger_preload(messages, model_to_use, **kwargs)
                
                return cached_response
        
        # 2. å‘é€å®é™…è¯·æ±‚
        try:
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=model_to_use,
                messages=messages,
                timeout=90.0,  # ä½¿ç”¨90ç§’è¶…æ—¶
                **kwargs
            )
            
            response_time = time.time() - start_time
            self.stats["total_response_time"] += response_time
            self.stats["cache_misses"] += 1
            
            logger.debug(f"API call - Response time: {response_time:.3f}s")
            
            # 3. å­˜å‚¨åˆ°ç¼“å­˜
            if use_cache:
                self.cache.put(messages, model_to_use, response, **kwargs)
            
            # 4. è®°å½•è¯·æ±‚ç”¨äºé¢„æµ‹
            if self.predictor:
                self.predictor.record_request(messages, model_to_use)
            
            # 5. è§¦å‘é¢„åŠ è½½
            if enable_preload:
                await self._trigger_preload(messages, model_to_use, **kwargs)
            
            return response
            
        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            raise e
    
    async def _trigger_preload(self, messages: List[Dict], model: str, **kwargs):
        """è§¦å‘é¢„åŠ è½½"""
        if not self.predictor:
            return
        
        # é¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½çš„è¯·æ±‚
        predicted_requests = self.predictor.predict_next_requests(messages, limit=3)
        
        for predicted in predicted_requests:
            pred_messages = predicted["messages"]
            pred_model = predicted.get("model", model)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ç¼“å­˜ä¸­
            if self.cache.get(pred_messages, pred_model, **kwargs):
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨é¢„åŠ è½½
            pred_key = self.cache._generate_key(pred_messages, pred_model, **kwargs)
            if pred_key in self.preload_tasks:
                continue
            
            # å¯åŠ¨é¢„åŠ è½½ä»»åŠ¡
            task = asyncio.create_task(self._preload_request(pred_messages, pred_model, pred_key, **kwargs))
            self.preload_tasks[pred_key] = task
    
    async def _preload_request(self, messages: List[Dict], model: str, key: str, **kwargs):
        """é¢„åŠ è½½è¯·æ±‚"""
        try:
            logger.debug(f"Preloading request: {key[:8]}...")
            
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=model,
                messages=messages,
                timeout=90.0,
                **kwargs
            )
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.cache.put(messages, model, response, **kwargs)
            self.stats["preloaded_requests"] += 1
            
            logger.debug(f"Preload completed: {key[:8]}...")
            
        except Exception as e:
            logger.warning(f"Preload failed: {str(e)}")
        finally:
            # æ¸…ç†ä»»åŠ¡
            if key in self.preload_tasks:
                del self.preload_tasks[key]
    
    def get_average_response_time(self) -> float:
        """è·å–å¹³å‡å“åº”æ—¶é—´"""
        if self.stats["total_requests"] == 0:
            return 0.0
        return self.stats["total_response_time"] / self.stats["total_requests"]
    
    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        cache_stats = self.cache.get_stats()
        
        return {
            "requests": self.stats,
            "cache": cache_stats,
            "performance": {
                "average_response_time": self.get_average_response_time(),
                "cache_hit_rate": cache_stats["hit_rate"],
                "preload_efficiency": self.stats["preloaded_requests"] / max(self.stats["total_requests"], 1)
            }
        }

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.results: List[Dict] = []
    
    async def run_benchmark(self, 
                          client: PerformanceOptimizedClient,
                          test_messages: List[List[Dict]],
                          iterations: int = 10) -> Dict:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        print(f"ğŸ”„ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• ({iterations} æ¬¡è¿­ä»£)")
        
        start_time = time.time()
        response_times = []
        
        for i in range(iterations):
            for j, messages in enumerate(test_messages):
                iter_start = time.time()
                
                try:
                    await client.chat_completion(messages)
                    iter_time = time.time() - iter_start
                    response_times.append(iter_time)
                    
                    print(f"   è¿­ä»£ {i+1}/{iterations}, æ¶ˆæ¯ {j+1}/{len(test_messages)}: {iter_time:.3f}s")
                    
                except Exception as e:
                    print(f"   âŒ è¿­ä»£ {i+1}/{iterations}, æ¶ˆæ¯ {j+1} å¤±è´¥: {str(e)}")
        
        total_time = time.time() - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            
            # è®¡ç®—ç™¾åˆ†ä½æ•°
            sorted_times = sorted(response_times)
            p50 = sorted_times[len(sorted_times) // 2]
            p95 = sorted_times[int(len(sorted_times) * 0.95)]
            p99 = sorted_times[int(len(sorted_times) * 0.99)]
        else:
            avg_response_time = min_response_time = max_response_time = 0
            p50 = p95 = p99 = 0
        
        # è·å–å®¢æˆ·ç«¯æ€§èƒ½ç»Ÿè®¡
        perf_stats = client.get_performance_stats()
        
        benchmark_result = {
            "test_config": {
                "iterations": iterations,
                "total_messages": len(test_messages),
                "total_requests": iterations * len(test_messages)
            },
            "timing": {
                "total_time": total_time,
                "average_response_time": avg_response_time,
                "min_response_time": min_response_time,
                "max_response_time": max_response_time,
                "p50_response_time": p50,
                "p95_response_time": p95,
                "p99_response_time": p99
            },
            "performance": perf_stats,
            "throughput": {
                "requests_per_second": len(response_times) / total_time if total_time > 0 else 0
            }
        }
        
        self.results.append(benchmark_result)
        return benchmark_result

# æ¼”ç¤ºå‡½æ•°
async def demo_intelligent_cache():
    """æ¼”ç¤ºæ™ºèƒ½ç¼“å­˜"""
    print("\nğŸ§  æ™ºèƒ½ç¼“å­˜æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºä¼˜åŒ–å®¢æˆ·ç«¯
    client = PerformanceOptimizedClient(cache_size=100, cache_ttl=300)
    
    # æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}],
        [{"role": "user", "content": "è§£é‡Šæœºå™¨å­¦ä¹ "}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}],  # é‡å¤è¯·æ±‚
    ]
    
    print("ğŸ”„ å‘é€æµ‹è¯•è¯·æ±‚...")
    
    for i, messages in enumerate(test_messages):
        start_time = time.time()
        
        try:
            response = await client.chat_completion(messages)
            response_time = time.time() - start_time
            
            cache_stats = client.cache.get_stats()
            print(f"   è¯·æ±‚ {i+1}: {response_time:.3f}s (ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1%})")
            
        except Exception as e:
            print(f"   âŒ è¯·æ±‚ {i+1} å¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    cache_stats = client.cache.get_stats()
    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    print(f"   æ€»è¯·æ±‚æ•°: {cache_stats['total_requests']}")
    print(f"   ç¼“å­˜å‘½ä¸­: {cache_stats['hits']}")
    print(f"   ç¼“å­˜æœªå‘½ä¸­: {cache_stats['misses']}")
    print(f"   å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1%}")
    print(f"   ç¼“å­˜å¤§å°: {cache_stats['cache_size']}/{cache_stats['max_size']}")

async def demo_request_prediction():
    """æ¼”ç¤ºè¯·æ±‚é¢„æµ‹"""
    print("\nğŸ”® è¯·æ±‚é¢„æµ‹æ¼”ç¤º")
    print("=" * 50)
    
    client = PerformanceOptimizedClient(enable_prediction=True)
    
    # æ¨¡æ‹Ÿç”¨æˆ·å¯¹è¯æ¨¡å¼
    conversation_patterns = [
        [{"role": "user", "content": "ä½ å¥½"}],
        [{"role": "user", "content": "æˆ‘æƒ³äº†è§£AI"}],
        [{"role": "user", "content": "è°¢è°¢"}],
        [{"role": "user", "content": "ä½ å¥½"}],  # é‡å¤æ¨¡å¼å¼€å§‹
        [{"role": "user", "content": "æˆ‘æƒ³äº†è§£æœºå™¨å­¦ä¹ "}],  # ç±»ä¼¼ä½†ä¸åŒçš„åç»­
    ]
    
    print("ğŸ”„ å»ºç«‹å¯¹è¯æ¨¡å¼...")
    
    for i, messages in enumerate(conversation_patterns):
        try:
            start_time = time.time()
            response = await client.chat_completion(messages, enable_preload=True)
            response_time = time.time() - start_time
            
            print(f"   å¯¹è¯ {i+1}: {response_time:.3f}s")
            
            # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            if client.predictor:
                predictions = client.predictor.predict_next_requests(messages, limit=2)
                if predictions:
                    print(f"     é¢„æµ‹ä¸‹ä¸€æ­¥: {len(predictions)} ä¸ªå¯èƒ½çš„è¯·æ±‚")
            
        except Exception as e:
            print(f"   âŒ å¯¹è¯ {i+1} å¤±è´¥: {str(e)}")
        
        await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
    
    # æ˜¾ç¤ºé¢„åŠ è½½ç»Ÿè®¡
    perf_stats = client.get_performance_stats()
    print(f"\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    print(f"   é¢„åŠ è½½è¯·æ±‚æ•°: {client.stats['preloaded_requests']}")
    print(f"   é¢„åŠ è½½æ•ˆç‡: {perf_stats['performance']['preload_efficiency']:.1%}")

async def demo_performance_comparison():
    """æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”"""
    print("\nâš¡ æ€§èƒ½å¯¹æ¯”æ¼”ç¤º")
    print("=" * 50)
    
    # æ™®é€šå®¢æˆ·ç«¯
    normal_client = HarborAI()
    
    # ä¼˜åŒ–å®¢æˆ·ç«¯
    optimized_client = PerformanceOptimizedClient(
        cache_size=50,
        cache_ttl=300,
        enable_prediction=True
    )
    
    # æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        [{"role": "user", "content": "ç®€å•æµ‹è¯•1"}],
        [{"role": "user", "content": "ç®€å•æµ‹è¯•2"}],
        [{"role": "user", "content": "ç®€å•æµ‹è¯•1"}],  # é‡å¤
        [{"role": "user", "content": "ç®€å•æµ‹è¯•3"}],
    ]
    
    # æµ‹è¯•æ™®é€šå®¢æˆ·ç«¯
    print("ğŸ”„ æµ‹è¯•æ™®é€šå®¢æˆ·ç«¯...")
    normal_times = []
    normal_start = time.time()
    
    for i, messages in enumerate(test_messages):
        try:
            start_time = time.time()
            await asyncio.to_thread(
                normal_client.chat.completions.create,
                model=optimized_client.model_name,
                messages=messages
            )
            response_time = time.time() - start_time
            normal_times.append(response_time)
            print(f"   è¯·æ±‚ {i+1}: {response_time:.3f}s")
        except Exception as e:
            print(f"   âŒ è¯·æ±‚ {i+1} å¤±è´¥: {str(e)}")
    
    normal_total = time.time() - normal_start
    
    # æµ‹è¯•ä¼˜åŒ–å®¢æˆ·ç«¯
    print("\nğŸ”„ æµ‹è¯•ä¼˜åŒ–å®¢æˆ·ç«¯...")
    optimized_times = []
    optimized_start = time.time()
    
    for i, messages in enumerate(test_messages):
        try:
            start_time = time.time()
            await optimized_client.chat_completion(messages)
            response_time = time.time() - start_time
            optimized_times.append(response_time)
            print(f"   è¯·æ±‚ {i+1}: {response_time:.3f}s")
        except Exception as e:
            print(f"   âŒ è¯·æ±‚ {i+1} å¤±è´¥: {str(e)}")
    
    optimized_total = time.time() - optimized_start
    
    # æ€§èƒ½å¯¹æ¯”
    if normal_times and optimized_times:
        avg_normal = sum(normal_times) / len(normal_times)
        avg_optimized = sum(optimized_times) / len(optimized_times)
        
        print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
        print(f"   æ™®é€šå®¢æˆ·ç«¯:")
        print(f"     å¹³å‡å“åº”æ—¶é—´: {avg_normal:.3f}s")
        print(f"     æ€»æ—¶é—´: {normal_total:.3f}s")
        
        print(f"   ä¼˜åŒ–å®¢æˆ·ç«¯:")
        print(f"     å¹³å‡å“åº”æ—¶é—´: {avg_optimized:.3f}s")
        print(f"     æ€»æ—¶é—´: {optimized_total:.3f}s")
        
        if avg_normal > 0:
            improvement = ((avg_normal - avg_optimized) / avg_normal * 100)
            print(f"     æ€§èƒ½æå‡: {improvement:.1f}%")
        
        # æ˜¾ç¤ºä¼˜åŒ–ç»Ÿè®¡
        perf_stats = optimized_client.get_performance_stats()
        print(f"     ç¼“å­˜å‘½ä¸­ç‡: {perf_stats['cache']['hit_rate']:.1%}")

async def demo_benchmark_test():
    """æ¼”ç¤ºåŸºå‡†æµ‹è¯•"""
    print("\nğŸ“Š åŸºå‡†æµ‹è¯•æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    benchmark = PerformanceBenchmark()
    
    # åˆ›å»ºä¼˜åŒ–å®¢æˆ·ç«¯
    client = PerformanceOptimizedClient(
        cache_size=100,
        cache_ttl=600,
        enable_prediction=True
    )
    
    # æµ‹è¯•æ¶ˆæ¯é›†
    test_messages = [
        [{"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯1"}],
        [{"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯2"}],
        [{"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯3"}],
    ]
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    result = await benchmark.run_benchmark(
        client=client,
        test_messages=test_messages,
        iterations=3  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥èŠ‚çœæ—¶é—´
    )
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
    print(f"   æ€»è¯·æ±‚æ•°: {result['test_config']['total_requests']}")
    print(f"   æ€»æ—¶é—´: {result['timing']['total_time']:.3f}s")
    print(f"   å¹³å‡å“åº”æ—¶é—´: {result['timing']['average_response_time']:.3f}s")
    print(f"   P95å“åº”æ—¶é—´: {result['timing']['p95_response_time']:.3f}s")
    print(f"   ååé‡: {result['throughput']['requests_per_second']:.1f} req/s")
    print(f"   ç¼“å­˜å‘½ä¸­ç‡: {result['performance']['cache']['hit_rate']:.1%}")

async def demo_cache_strategies():
    """æ¼”ç¤ºä¸åŒç¼“å­˜ç­–ç•¥"""
    print("\nğŸ¯ ç¼“å­˜ç­–ç•¥å¯¹æ¯”æ¼”ç¤º")
    print("=" * 50)
    
    strategies = [
        ("LRU", CacheStrategy.LRU),
        ("LFU", CacheStrategy.LFU),
        ("è‡ªé€‚åº”", CacheStrategy.ADAPTIVE)
    ]
    
    test_messages = [
        [{"role": "user", "content": f"æµ‹è¯•æ¶ˆæ¯{i}"}] for i in range(1, 6)
    ]
    
    for strategy_name, strategy in strategies:
        print(f"\nğŸ”„ æµ‹è¯• {strategy_name} ç­–ç•¥:")
        
        # åˆ›å»ºä½¿ç”¨ç‰¹å®šç­–ç•¥çš„ç¼“å­˜
        cache = IntelligentCache(max_size=3, strategy=strategy)
        client = PerformanceOptimizedClient(cache_size=3)
        client.cache = cache
        
        # å‘é€è¯·æ±‚ä»¥å¡«å……ç¼“å­˜
        for i, messages in enumerate(test_messages):
            try:
                await client.chat_completion(messages)
                stats = cache.get_stats()
                print(f"   è¯·æ±‚ {i+1}: ç¼“å­˜å¤§å° {stats['cache_size']}, å‘½ä¸­ç‡ {stats['hit_rate']:.1%}")
            except Exception as e:
                print(f"   âŒ è¯·æ±‚ {i+1} å¤±è´¥: {str(e)}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        final_stats = cache.get_stats()
        print(f"   æœ€ç»ˆå‘½ä¸­ç‡: {final_stats['hit_rate']:.1%}")
        print(f"   æ·˜æ±°æ¬¡æ•°: {final_stats['evictions']}")

async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("âš¡ HarborAI æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹é…ç½®
    print_available_models()
    
    try:
        # æ™ºèƒ½ç¼“å­˜æ¼”ç¤º
        await demo_intelligent_cache()
        
        # è¯·æ±‚é¢„æµ‹æ¼”ç¤º
        await demo_request_prediction()
        
        # æ€§èƒ½å¯¹æ¯”æ¼”ç¤º
        await demo_performance_comparison()
        
        # åŸºå‡†æµ‹è¯•æ¼”ç¤º
        await demo_benchmark_test()
        
        # ç¼“å­˜ç­–ç•¥å¯¹æ¯”æ¼”ç¤º
        await demo_cache_strategies()
        
        print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ç”Ÿäº§ç¯å¢ƒå»ºè®®:")
        print("   1. æ ¹æ®ä¸šåŠ¡ç‰¹ç‚¹è°ƒæ•´ç¼“å­˜å¤§å°å’ŒTTL")
        print("   2. ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡å¹¶ä¼˜åŒ–ç­–ç•¥")
        print("   3. ä½¿ç”¨è¯·æ±‚é¢„æµ‹å‡å°‘å»¶è¿Ÿ")
        print("   4. å®šæœŸè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("   5. ç»“åˆä¸šåŠ¡æŒ‡æ ‡ä¼˜åŒ–æ€§èƒ½å‚æ•°")
        print("   6. ä½¿ç”¨90ç§’è¶…æ—¶é…ç½®åº”å¯¹ç½‘ç»œå»¶è¿Ÿ")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())