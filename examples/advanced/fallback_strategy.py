#!/usr/bin/env python3
"""
HarborAI é™çº§ç­–ç•¥æ¼”ç¤º

åœºæ™¯æè¿°:
åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå½“ä¸»è¦AIæœåŠ¡ä¸å¯ç”¨æˆ–æ€§èƒ½ä¸‹é™æ—¶ï¼Œéœ€è¦è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆï¼Œ
ç¡®ä¿æœåŠ¡è¿ç»­æ€§ã€‚æœ¬ç¤ºä¾‹å±•ç¤ºå¤šå±‚çº§é™çº§ç­–ç•¥ã€è‡ªåŠ¨æ•…éšœè½¬ç§»ç­‰æœºåˆ¶ã€‚

åº”ç”¨ä»·å€¼:
- ç¡®ä¿æœåŠ¡è¿ç»­æ€§å’Œå¯ç”¨æ€§
- ä¼˜åŒ–ç”¨æˆ·ä½“éªŒï¼Œé¿å…æœåŠ¡ä¸­æ–­
- é™ä½æœåŠ¡ä¸­æ–­é£é™©
- åœ¨æˆæœ¬å’Œæ€§èƒ½ä¹‹é—´æ‰¾åˆ°å¹³è¡¡

æ ¸å¿ƒåŠŸèƒ½:
1. å¤šå±‚çº§é™çº§ç­–ç•¥
2. æœåŠ¡å¥åº·ç›‘æ§
3. è‡ªåŠ¨æ•…éšœè½¬ç§»
4. æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦
5. ä¼˜é›…é™çº§å¤„ç†
"""

import asyncio
import time
import random
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import json
from openai import OpenAI, AsyncOpenAI
from openai.types.chat import ChatCompletion

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ServiceTier(Enum):
    """æœåŠ¡å±‚çº§"""
    PRIMARY = "primary"      # ä¸»è¦æœåŠ¡
    SECONDARY = "secondary"  # æ¬¡è¦æœåŠ¡
    FALLBACK = "fallback"    # é™çº§æœåŠ¡
    EMERGENCY = "emergency"  # ç´§æ€¥æœåŠ¡

class ServiceStatus(Enum):
    """æœåŠ¡çŠ¶æ€"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    OFFLINE = "offline"

@dataclass
class ServiceConfig:
    """æœåŠ¡é…ç½®"""
    name: str
    model: str
    tier: ServiceTier
    api_key: str
    base_url: str = "https://api.harborai.com/v1"
    max_tokens: int = 1000
    temperature: float = 0.7
    cost_per_token: float = 0.0001
    expected_latency: float = 2.0
    quality_score: float = 1.0

@dataclass
class ServiceMetrics:
    """æœåŠ¡æŒ‡æ ‡"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_latency: float = 0.0
    total_cost: float = 0.0
    last_request_time: Optional[datetime] = None
    last_error_time: Optional[datetime] = None
    consecutive_failures: int = 0
    
    def add_success(self, latency: float, cost: float):
        """æ·»åŠ æˆåŠŸè®°å½•"""
        self.total_requests += 1
        self.successful_requests += 1
        self.total_latency += latency
        self.total_cost += cost
        self.last_request_time = datetime.now()
        self.consecutive_failures = 0
    
    def add_failure(self):
        """æ·»åŠ å¤±è´¥è®°å½•"""
        self.total_requests += 1
        self.failed_requests += 1
        self.last_error_time = datetime.now()
        self.consecutive_failures += 1
    
    def get_success_rate(self) -> float:
        """è·å–æˆåŠŸç‡"""
        if self.total_requests == 0:
            return 0.0
        return self.successful_requests / self.total_requests
    
    def get_average_latency(self) -> float:
        """è·å–å¹³å‡å»¶è¿Ÿ"""
        if self.successful_requests == 0:
            return 0.0
        return self.total_latency / self.successful_requests
    
    def get_average_cost(self) -> float:
        """è·å–å¹³å‡æˆæœ¬"""
        if self.successful_requests == 0:
            return 0.0
        return self.total_cost / self.successful_requests

class ServiceHealthChecker:
    """æœåŠ¡å¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self, config: ServiceConfig):
        self.config = config
        self.client = AsyncOpenAI(api_key=config.api_key, base_url=config.base_url)
        self.metrics = ServiceMetrics()
        self.status = ServiceStatus.HEALTHY
        
    async def health_check(self) -> bool:
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        try:
            start_time = time.time()
            
            # å‘é€ç®€å•çš„å¥åº·æ£€æŸ¥è¯·æ±‚
            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=[{"role": "user", "content": "ping"}],
                max_tokens=10,
                timeout=5.0
            )
            
            latency = time.time() - start_time
            cost = len(response.choices[0].message.content) * self.config.cost_per_token
            
            self.metrics.add_success(latency, cost)
            self._update_status()
            
            logger.info(f"Health check passed for {self.config.name} - Latency: {latency:.2f}s")
            return True
            
        except Exception as e:
            self.metrics.add_failure()
            self._update_status()
            
            logger.warning(f"Health check failed for {self.config.name}: {str(e)}")
            return False
    
    def _update_status(self):
        """æ›´æ–°æœåŠ¡çŠ¶æ€"""
        success_rate = self.metrics.get_success_rate()
        avg_latency = self.metrics.get_average_latency()
        
        if self.metrics.consecutive_failures >= 3:
            self.status = ServiceStatus.OFFLINE
        elif success_rate < 0.5:
            self.status = ServiceStatus.UNHEALTHY
        elif success_rate < 0.8 or avg_latency > self.config.expected_latency * 2:
            self.status = ServiceStatus.DEGRADED
        else:
            self.status = ServiceStatus.HEALTHY
    
    async def make_request(self, messages: List[Dict], **kwargs) -> ChatCompletion:
        """å‘é€è¯·æ±‚"""
        start_time = time.time()
        
        try:
            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=messages,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                **kwargs
            )
            
            latency = time.time() - start_time
            cost = response.usage.total_tokens * self.config.cost_per_token if response.usage else 0
            
            self.metrics.add_success(latency, cost)
            self._update_status()
            
            return response
            
        except Exception as e:
            self.metrics.add_failure()
            self._update_status()
            raise e

class FallbackStrategy:
    """é™çº§ç­–ç•¥"""
    
    def __init__(self, services: List[ServiceConfig]):
        self.services = {config.name: ServiceHealthChecker(config) for config in services}
        self.service_order = sorted(services, key=lambda x: x.tier.value)
        self.current_service = None
        self.fallback_history: List[Dict] = []
        
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        logger.info("Initializing fallback strategy...")
        
        # å¯¹æ‰€æœ‰æœåŠ¡è¿›è¡Œå¥åº·æ£€æŸ¥
        for service_name, checker in self.services.items():
            await checker.health_check()
        
        # é€‰æ‹©æœ€ä½³æœåŠ¡
        self.current_service = self._select_best_service()
        logger.info(f"Selected primary service: {self.current_service}")
    
    def _select_best_service(self) -> Optional[str]:
        """é€‰æ‹©æœ€ä½³æœåŠ¡"""
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œé€‰æ‹©å¥åº·çš„æœåŠ¡
        for config in self.service_order:
            checker = self.services[config.name]
            if checker.status in [ServiceStatus.HEALTHY, ServiceStatus.DEGRADED]:
                return config.name
        
        # å¦‚æœæ²¡æœ‰å¥åº·çš„æœåŠ¡ï¼Œé€‰æ‹©æœ€ä¸å·®çš„
        available_services = [(name, checker) for name, checker in self.services.items()]
        if available_services:
            best_service = max(available_services, key=lambda x: x[1].metrics.get_success_rate())
            return best_service[0]
        
        return None
    
    async def make_request(self, messages: List[Dict], **kwargs) -> ChatCompletion:
        """å‘é€è¯·æ±‚ï¼ˆå¸¦é™çº§ç­–ç•¥ï¼‰"""
        original_service = self.current_service
        
        # å°è¯•æ‰€æœ‰å¯ç”¨æœåŠ¡
        for config in self.service_order:
            service_name = config.name
            checker = self.services[service_name]
            
            # è·³è¿‡ç¦»çº¿æœåŠ¡
            if checker.status == ServiceStatus.OFFLINE:
                continue
            
            try:
                logger.info(f"Attempting request with service: {service_name}")
                response = await checker.make_request(messages, **kwargs)
                
                # å¦‚æœä½¿ç”¨äº†é™çº§æœåŠ¡ï¼Œè®°å½•é™çº§äº‹ä»¶
                if service_name != original_service:
                    self._record_fallback(original_service, service_name, "success")
                    self.current_service = service_name
                
                return response
                
            except Exception as e:
                logger.warning(f"Request failed with {service_name}: {str(e)}")
                
                # è®°å½•é™çº§äº‹ä»¶
                if service_name == original_service:
                    self._record_fallback(original_service, None, "failure")
                
                continue
        
        # æ‰€æœ‰æœåŠ¡éƒ½å¤±è´¥äº†
        raise Exception("All services are unavailable")
    
    def _record_fallback(self, from_service: Optional[str], to_service: Optional[str], reason: str):
        """è®°å½•é™çº§äº‹ä»¶"""
        event = {
            "timestamp": datetime.now().isoformat(),
            "from_service": from_service,
            "to_service": to_service,
            "reason": reason
        }
        self.fallback_history.append(event)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.fallback_history) > 100:
            self.fallback_history = self.fallback_history[-100:]
        
        logger.info(f"Fallback event: {from_service} -> {to_service} ({reason})")
    
    async def periodic_health_check(self, interval: float = 30.0):
        """å®šæœŸå¥åº·æ£€æŸ¥"""
        while True:
            try:
                logger.info("Performing periodic health checks...")
                
                # æ£€æŸ¥æ‰€æœ‰æœåŠ¡å¥åº·çŠ¶æ€
                for service_name, checker in self.services.items():
                    await checker.health_check()
                
                # é‡æ–°é€‰æ‹©æœ€ä½³æœåŠ¡
                best_service = self._select_best_service()
                if best_service and best_service != self.current_service:
                    logger.info(f"Switching to better service: {self.current_service} -> {best_service}")
                    self.current_service = best_service
                
                await asyncio.sleep(interval)
                
            except Exception as e:
                logger.error(f"Health check error: {str(e)}")
                await asyncio.sleep(interval)
    
    def get_service_status(self) -> Dict:
        """è·å–æœåŠ¡çŠ¶æ€"""
        status = {}
        for service_name, checker in self.services.items():
            config = next(c for c in self.service_order if c.name == service_name)
            status[service_name] = {
                "tier": config.tier.value,
                "status": checker.status.value,
                "success_rate": checker.metrics.get_success_rate(),
                "average_latency": checker.metrics.get_average_latency(),
                "average_cost": checker.metrics.get_average_cost(),
                "total_requests": checker.metrics.total_requests,
                "consecutive_failures": checker.metrics.consecutive_failures
            }
        return status
    
    def get_fallback_summary(self) -> Dict:
        """è·å–é™çº§æ‘˜è¦"""
        if not self.fallback_history:
            return {"total_fallbacks": 0, "recent_fallbacks": []}
        
        recent_fallbacks = self.fallback_history[-10:]  # æœ€è¿‘10æ¬¡é™çº§
        
        return {
            "total_fallbacks": len(self.fallback_history),
            "recent_fallbacks": recent_fallbacks,
            "current_service": self.current_service,
            "fallback_rate": len([e for e in recent_fallbacks if e["reason"] == "failure"]) / max(len(recent_fallbacks), 1)
        }

# æ¼”ç¤ºå‡½æ•°
async def demo_basic_fallback():
    """æ¼”ç¤ºåŸºç¡€é™çº§åŠŸèƒ½"""
    print("\nğŸ”„ åŸºç¡€é™çº§æ¼”ç¤º")
    print("=" * 50)
    
    # é…ç½®å¤šä¸ªæœåŠ¡
    services = [
        ServiceConfig(
            name="primary-gpt4",
            model="gpt-4",
            tier=ServiceTier.PRIMARY,
            api_key="your-openai-key",
            cost_per_token=0.03,
            expected_latency=3.0,
            quality_score=1.0
        ),
        ServiceConfig(
            name="secondary-gpt35",
            model="gpt-3.5-turbo",
            tier=ServiceTier.SECONDARY,
            api_key="your-openai-key",
            cost_per_token=0.002,
            expected_latency=2.0,
            quality_score=0.8
        ),
        ServiceConfig(
            name="fallback-deepseek",
            model="deepseek-chat",
            tier=ServiceTier.FALLBACK,
            api_key="your-deepseek-key",
            cost_per_token=0.0001,
            expected_latency=1.5,
            quality_score=0.7
        )
    ]
    
    # åˆ›å»ºé™çº§ç­–ç•¥
    strategy = FallbackStrategy(services)
    await strategy.initialize()
    
    # æµ‹è¯•è¯·æ±‚
    test_messages = [
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿè¯·ç®€è¦è¯´æ˜ã€‚"}
    ]
    
    try:
        response = await strategy.make_request(test_messages)
        print(f"âœ… è¯·æ±‚æˆåŠŸ")
        print(f"ğŸ¯ ä½¿ç”¨æœåŠ¡: {strategy.current_service}")
        print(f"ğŸ“ å›å¤: {response.choices[0].message.content[:100]}...")
        
        # æ˜¾ç¤ºæœåŠ¡çŠ¶æ€
        status = strategy.get_service_status()
        print(f"\nğŸ“Š æœåŠ¡çŠ¶æ€:")
        for service_name, info in status.items():
            print(f"   - {service_name}: {info['status']} (æˆåŠŸç‡: {info['success_rate']:.1%})")
        
    except Exception as e:
        print(f"âŒ æ‰€æœ‰æœåŠ¡éƒ½ä¸å¯ç”¨: {str(e)}")

async def demo_service_monitoring():
    """æ¼”ç¤ºæœåŠ¡ç›‘æ§"""
    print("\nğŸ“Š æœåŠ¡ç›‘æ§æ¼”ç¤º")
    print("=" * 50)
    
    # é…ç½®æœåŠ¡ï¼ˆä½¿ç”¨å¯ç”¨çš„APIå¯†é’¥ï¼‰
    services = [
        ServiceConfig(
            name="primary-deepseek",
            model="deepseek-chat",
            tier=ServiceTier.PRIMARY,
            api_key="your-deepseek-key",
            cost_per_token=0.0001,
            expected_latency=2.0
        ),
        ServiceConfig(
            name="fallback-deepseek-r1",
            model="deepseek-r1",
            tier=ServiceTier.FALLBACK,
            api_key="your-deepseek-key",
            cost_per_token=0.0002,
            expected_latency=3.0
        )
    ]
    
    strategy = FallbackStrategy(services)
    await strategy.initialize()
    
    # å¯åŠ¨å¥åº·ç›‘æ§ï¼ˆåå°ä»»åŠ¡ï¼‰
    monitoring_task = asyncio.create_task(strategy.periodic_health_check(interval=5.0))
    
    try:
        # æ¨¡æ‹Ÿä¸€äº›è¯·æ±‚
        test_messages = [
            {"role": "user", "content": "æµ‹è¯•è¯·æ±‚1"},
            {"role": "user", "content": "æµ‹è¯•è¯·æ±‚2"},
            {"role": "user", "content": "æµ‹è¯•è¯·æ±‚3"}
        ]
        
        print("ğŸ”„ å‘é€æµ‹è¯•è¯·æ±‚...")
        for i, messages in enumerate([test_messages[0]]):  # åªå‘é€ä¸€ä¸ªè¯·æ±‚ç”¨äºæ¼”ç¤º
            try:
                response = await strategy.make_request([messages])
                print(f"âœ… è¯·æ±‚ {i+1} æˆåŠŸ - æœåŠ¡: {strategy.current_service}")
            except Exception as e:
                print(f"âŒ è¯·æ±‚ {i+1} å¤±è´¥: {str(e)}")
            
            await asyncio.sleep(2)
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç›‘æ§è¿è¡Œ
        await asyncio.sleep(10)
        
        # åœæ­¢ç›‘æ§
        monitoring_task.cancel()
        
        # æ˜¾ç¤ºç›‘æ§ç»“æœ
        status = strategy.get_service_status()
        print(f"\nğŸ“Š æœ€ç»ˆæœåŠ¡çŠ¶æ€:")
        for service_name, info in status.items():
            print(f"   - {service_name}:")
            print(f"     çŠ¶æ€: {info['status']}")
            print(f"     æˆåŠŸç‡: {info['success_rate']:.1%}")
            print(f"     å¹³å‡å»¶è¿Ÿ: {info['average_latency']:.2f}s")
            print(f"     å¹³å‡æˆæœ¬: ${info['average_cost']:.6f}")
        
        # æ˜¾ç¤ºé™çº§æ‘˜è¦
        fallback_summary = strategy.get_fallback_summary()
        print(f"\nğŸ”„ é™çº§æ‘˜è¦:")
        print(f"   - æ€»é™çº§æ¬¡æ•°: {fallback_summary['total_fallbacks']}")
        print(f"   - å½“å‰æœåŠ¡: {fallback_summary['current_service']}")
        print(f"   - é™çº§ç‡: {fallback_summary['fallback_rate']:.1%}")
        
    except asyncio.CancelledError:
        pass

async def demo_cost_optimization():
    """æ¼”ç¤ºæˆæœ¬ä¼˜åŒ–"""
    print("\nğŸ’° æˆæœ¬ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    # é…ç½®ä¸åŒæˆæœ¬çš„æœåŠ¡
    services = [
        ServiceConfig(
            name="premium-service",
            model="gpt-4",
            tier=ServiceTier.PRIMARY,
            api_key="your-openai-key",
            cost_per_token=0.03,
            quality_score=1.0
        ),
        ServiceConfig(
            name="standard-service",
            model="gpt-3.5-turbo",
            tier=ServiceTier.SECONDARY,
            api_key="your-openai-key",
            cost_per_token=0.002,
            quality_score=0.8
        ),
        ServiceConfig(
            name="budget-service",
            model="deepseek-chat",
            tier=ServiceTier.FALLBACK,
            api_key="your-deepseek-key",
            cost_per_token=0.0001,
            quality_score=0.7
        )
    ]
    
    strategy = FallbackStrategy(services)
    await strategy.initialize()
    
    # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„è¯·æ±‚
    request_types = [
        ("ç®€å•é—®ç­”", "ä»€ä¹ˆæ˜¯AIï¼Ÿ"),
        ("å¤æ‚åˆ†æ", "è¯·è¯¦ç»†åˆ†æäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æŠ€æœ¯æŒ‘æˆ˜å’Œä¼¦ç†è€ƒè™‘ã€‚"),
        ("åˆ›æ„å†™ä½œ", "å†™ä¸€ä¸ªå…³äºæœªæ¥åŸå¸‚çš„ç§‘å¹»çŸ­æ•…äº‹ã€‚")
    ]
    
    total_cost = 0.0
    results = []
    
    for request_type, content in request_types:
        try:
            messages = [{"role": "user", "content": content}]
            
            start_time = time.time()
            response = await strategy.make_request(messages)
            end_time = time.time()
            
            # ä¼°ç®—æˆæœ¬
            service_config = next(s for s in services if s.name == strategy.current_service)
            estimated_cost = len(response.choices[0].message.content) * service_config.cost_per_token
            total_cost += estimated_cost
            
            result = {
                "type": request_type,
                "service": strategy.current_service,
                "latency": end_time - start_time,
                "cost": estimated_cost,
                "quality": service_config.quality_score
            }
            results.append(result)
            
            print(f"âœ… {request_type}")
            print(f"   æœåŠ¡: {strategy.current_service}")
            print(f"   å»¶è¿Ÿ: {result['latency']:.2f}s")
            print(f"   æˆæœ¬: ${result['cost']:.6f}")
            print(f"   è´¨é‡åˆ†æ•°: {result['quality']}")
            
        except Exception as e:
            print(f"âŒ {request_type} å¤±è´¥: {str(e)}")
    
    # æˆæœ¬åˆ†æ
    print(f"\nğŸ’° æˆæœ¬åˆ†æ:")
    print(f"   - æ€»æˆæœ¬: ${total_cost:.6f}")
    print(f"   - å¹³å‡æˆæœ¬: ${total_cost/len(results):.6f}")
    
    # å¦‚æœå…¨éƒ¨ä½¿ç”¨æœ€è´µæœåŠ¡çš„æˆæœ¬å¯¹æ¯”
    premium_cost = sum(len(r["type"]) * 0.03 for r in results)  # ç®€åŒ–è®¡ç®—
    savings = premium_cost - total_cost
    print(f"   - å¦‚æœå…¨ç”¨é«˜ç«¯æœåŠ¡: ${premium_cost:.6f}")
    print(f"   - èŠ‚çœæˆæœ¬: ${savings:.6f} ({savings/premium_cost*100:.1f}%)")

async def demo_intelligent_routing():
    """æ¼”ç¤ºæ™ºèƒ½è·¯ç”±"""
    print("\nğŸ§  æ™ºèƒ½è·¯ç”±æ¼”ç¤º")
    print("=" * 50)
    
    class IntelligentFallbackStrategy(FallbackStrategy):
        """æ™ºèƒ½é™çº§ç­–ç•¥"""
        
        def _select_service_for_request(self, messages: List[Dict]) -> str:
            """æ ¹æ®è¯·æ±‚å†…å®¹é€‰æ‹©æœ€é€‚åˆçš„æœåŠ¡"""
            content = " ".join([msg.get("content", "") for msg in messages])
            content_length = len(content)
            
            # æ ¹æ®å†…å®¹å¤æ‚åº¦é€‰æ‹©æœåŠ¡
            if content_length > 500 or "åˆ†æ" in content or "è¯¦ç»†" in content:
                # å¤æ‚è¯·æ±‚ï¼Œä¼˜å…ˆä½¿ç”¨é«˜è´¨é‡æœåŠ¡
                for config in self.service_order:
                    checker = self.services[config.name]
                    if checker.status == ServiceStatus.HEALTHY and config.quality_score >= 0.9:
                        return config.name
            elif content_length < 100:
                # ç®€å•è¯·æ±‚ï¼Œä¼˜å…ˆä½¿ç”¨ç»æµæœåŠ¡
                for config in reversed(self.service_order):
                    checker = self.services[config.name]
                    if checker.status == ServiceStatus.HEALTHY:
                        return config.name
            
            # é»˜è®¤é€‰æ‹©
            return self._select_best_service()
        
        async def make_request(self, messages: List[Dict], **kwargs) -> ChatCompletion:
            """æ™ºèƒ½è·¯ç”±è¯·æ±‚"""
            # é€‰æ‹©æœ€é€‚åˆçš„æœåŠ¡
            selected_service = self._select_service_for_request(messages)
            
            if selected_service:
                try:
                    checker = self.services[selected_service]
                    response = await checker.make_request(messages, **kwargs)
                    logger.info(f"Intelligent routing selected: {selected_service}")
                    return response
                except Exception as e:
                    logger.warning(f"Selected service {selected_service} failed, falling back...")
            
            # å¦‚æœé€‰æ‹©çš„æœåŠ¡å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†é™çº§ç­–ç•¥
            return await super().make_request(messages, **kwargs)
    
    # é…ç½®æœåŠ¡
    services = [
        ServiceConfig(
            name="high-quality",
            model="gpt-4",
            tier=ServiceTier.PRIMARY,
            api_key="your-openai-key",
            cost_per_token=0.03,
            quality_score=1.0
        ),
        ServiceConfig(
            name="balanced",
            model="gpt-3.5-turbo",
            tier=ServiceTier.SECONDARY,
            api_key="your-openai-key",
            cost_per_token=0.002,
            quality_score=0.8
        ),
        ServiceConfig(
            name="economical",
            model="deepseek-chat",
            tier=ServiceTier.FALLBACK,
            api_key="your-deepseek-key",
            cost_per_token=0.0001,
            quality_score=0.7
        )
    ]
    
    strategy = IntelligentFallbackStrategy(services)
    await strategy.initialize()
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„è¯·æ±‚
    test_cases = [
        ("ç®€å•é—®ç­”", "ä½ å¥½"),
        ("å¤æ‚åˆ†æ", "è¯·è¯¦ç»†åˆ†æäººå·¥æ™ºèƒ½åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬é£é™©ç®¡ç†ã€ç®—æ³•äº¤æ˜“ã€å®¢æˆ·æœåŠ¡ç­‰æ–¹é¢çš„æŠ€æœ¯å®ç°å’Œå•†ä¸šä»·å€¼ã€‚"),
        ("ä¸­ç­‰å¤æ‚åº¦", "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ")
    ]
    
    for case_type, content in test_cases:
        try:
            messages = [{"role": "user", "content": content}]
            response = await strategy.make_request(messages)
            
            print(f"âœ… {case_type}")
            print(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"   é€‰æ‹©æœåŠ¡: {strategy.current_service}")
            print(f"   å›å¤é•¿åº¦: {len(response.choices[0].message.content)} å­—ç¬¦")
            
        except Exception as e:
            print(f"âŒ {case_type} å¤±è´¥: {str(e)}")

async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ”„ HarborAI é™çº§ç­–ç•¥æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åŸºç¡€é™çº§æ¼”ç¤º
        await demo_basic_fallback()
        
        # æœåŠ¡ç›‘æ§æ¼”ç¤º
        await demo_service_monitoring()
        
        # æˆæœ¬ä¼˜åŒ–æ¼”ç¤º
        await demo_cost_optimization()
        
        # æ™ºèƒ½è·¯ç”±æ¼”ç¤º
        await demo_intelligent_routing()
        
        print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ç”Ÿäº§ç¯å¢ƒå»ºè®®:")
        print("   1. é…ç½®å¤šä¸ªä¸åŒå±‚çº§çš„æœåŠ¡")
        print("   2. è®¾ç½®åˆç†çš„å¥åº·æ£€æŸ¥é—´éš”")
        print("   3. ç›‘æ§é™çº§äº‹ä»¶å’Œæˆæœ¬å˜åŒ–")
        print("   4. æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´è·¯ç”±ç­–ç•¥")
        print("   5. å»ºç«‹é™çº§äº‹ä»¶çš„å‘Šè­¦æœºåˆ¶")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())