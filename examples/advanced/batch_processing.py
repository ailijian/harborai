#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†ä¼˜åŒ–æ¼”ç¤º

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº† HarborAI çš„æ‰¹é‡å¤„ç†ä¼˜åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. æ‰¹é‡è¯·æ±‚èšåˆ
2. å¹¶å‘æ§åˆ¶
3. å†…å­˜ä¼˜åŒ–
4. è¿›åº¦è·Ÿè¸ª
5. ç»“æœåˆ†å‘

åœºæ™¯ï¼š
- å¤§é‡æ–‡æœ¬éœ€è¦æ‰¹é‡å¤„ç†ï¼ˆç¿»è¯‘ã€æ‘˜è¦ã€åˆ†æç­‰ï¼‰
- éœ€è¦æ§åˆ¶å¹¶å‘æ•°é‡é¿å…APIé™åˆ¶
- éœ€è¦ç›‘æ§å†…å­˜ä½¿ç”¨é¿å…OOM
- éœ€è¦å®æ—¶è·Ÿè¸ªå¤„ç†è¿›åº¦

ä»·å€¼ï¼š
- æé«˜å¤„ç†æ•ˆç‡ï¼ˆæ‰¹é‡+å¹¶å‘ï¼‰
- é™ä½APIè°ƒç”¨æˆæœ¬
- æä¾›å¯é çš„é”™è¯¯æ¢å¤æœºåˆ¶
- å®æ—¶ç›‘æ§å’Œè¿›åº¦åé¦ˆ
"""

import asyncio
import time
import psutil
import json
from datetime import datetime
from enum import Enum
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional, Callable, Union
from concurrent.futures import ThreadPoolExecutor
import logging

# å¯¼å…¥é…ç½®åŠ©æ‰‹
from config_helper import get_primary_model_config, get_fallback_models, print_available_models

# å¯¼å…¥ HarborAI
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from harborai import HarborAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BatchStatus(Enum):
    """æ‰¹æ¬¡çŠ¶æ€"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class ProcessingMode(Enum):
    """å¤„ç†æ¨¡å¼"""
    SEQUENTIAL = "sequential"  # é¡ºåºå¤„ç†
    CONCURRENT = "concurrent"  # å¹¶å‘å¤„ç†
    ADAPTIVE = "adaptive"      # è‡ªé€‚åº”å¤„ç†

@dataclass
class BatchConfig:
    """æ‰¹é‡å¤„ç†é…ç½®"""
    batch_size: int = 10           # æ‰¹æ¬¡å¤§å°
    max_concurrent: int = 5        # æœ€å¤§å¹¶å‘æ•°
    memory_limit_mb: int = 1024    # å†…å­˜é™åˆ¶ï¼ˆMBï¼‰
    timeout_seconds: int = 90      # è¯·æ±‚è¶…æ—¶æ—¶é—´
    retry_attempts: int = 3        # é‡è¯•æ¬¡æ•°
    processing_mode: ProcessingMode = ProcessingMode.CONCURRENT
    enable_progress_callback: bool = True

@dataclass
class RequestItem:
    """è¯·æ±‚é¡¹"""
    id: str
    prompt: str
    model: Optional[str] = None
    temperature: Optional[float] = None
    metadata: Optional[Dict[str, Any]] = None

@dataclass
class BatchResult:
    """æ‰¹æ¬¡ç»“æœ"""
    request_id: str
    success: bool
    response: Optional[str] = None
    error: Optional[str] = None
    processing_time: float = 0.0
    model_used: Optional[str] = None
    tokens_used: Optional[int] = None

class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""
    
    def __init__(self, limit_mb: int = 1024):
        self.limit_mb = limit_mb
        self.process = psutil.Process()
    
    def get_memory_usage_mb(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        return self.process.memory_info().rss / 1024 / 1024
    
    def is_memory_available(self, required_mb: float = 100) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜"""
        current_usage = self.get_memory_usage_mb()
        return (current_usage + required_mb) <= self.limit_mb
    
    def get_memory_stats(self) -> Dict[str, float]:
        """è·å–å†…å­˜ç»Ÿè®¡"""
        current = self.get_memory_usage_mb()
        return {
            "current_mb": current,
            "limit_mb": self.limit_mb,
            "usage_percent": (current / self.limit_mb) * 100,
            "available_mb": self.limit_mb - current
        }

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, config: BatchConfig):
        self.config = config
        self.memory_monitor = MemoryMonitor(config.memory_limit_mb)
        self.results: List[BatchResult] = []
        self.failed_requests: List[RequestItem] = []
        self.processing_stats = {
            "total_requests": 0,
            "completed_requests": 0,
            "failed_requests": 0,
            "total_processing_time": 0.0,
            "average_processing_time": 0.0,
            "memory_peak_mb": 0.0
        }
        
        # åˆå§‹åŒ– HarborAI å®¢æˆ·ç«¯
        model_config = get_primary_model_config()
        if not model_config:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹é…ç½®ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®")
        
        self.client = HarborAI()
        self.primary_model = model_config.model
        self.fallback_models = get_fallback_models()
        
        logger.info(f"âœ… æ‰¹é‡å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ä¸»è¦æ¨¡å‹: {self.primary_model}")
        logger.info(f"   é™çº§æ¨¡å‹: {', '.join(self.fallback_models[1:]) if len(self.fallback_models) > 1 else 'æ— '}")
        logger.info(f"   æ‰¹æ¬¡å¤§å°: {config.batch_size}")
        logger.info(f"   æœ€å¤§å¹¶å‘: {config.max_concurrent}")
        logger.info(f"   å†…å­˜é™åˆ¶: {config.memory_limit_mb}MB")
    
    def add_request(self, request: RequestItem) -> None:
        """æ·»åŠ è¯·æ±‚åˆ°å¤„ç†é˜Ÿåˆ—"""
        if not request.model:
            request.model = self.primary_model
        
        self.processing_stats["total_requests"] += 1
        logger.debug(f"æ·»åŠ è¯·æ±‚: {request.id}")
    
    def _create_batch(self, requests: List[RequestItem]) -> List[List[RequestItem]]:
        """åˆ›å»ºæ‰¹æ¬¡"""
        batches = []
        for i in range(0, len(requests), self.config.batch_size):
            batch = requests[i:i + self.config.batch_size]
            batches.append(batch)
        
        logger.info(f"åˆ›å»ºäº† {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ€»å…± {len(requests)} ä¸ªè¯·æ±‚")
        return batches
    
    async def _process_batch(self, batch: List[RequestItem], batch_index: int) -> List[BatchResult]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_index + 1}ï¼ŒåŒ…å« {len(batch)} ä¸ªè¯·æ±‚")
        
        # æ£€æŸ¥å†…å­˜
        if not self.memory_monitor.is_memory_available():
            logger.warning(f"å†…å­˜ä¸è¶³ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_index + 1}")
            return [
                BatchResult(
                    request_id=req.id,
                    success=False,
                    error="å†…å­˜ä¸è¶³",
                    processing_time=0.0
                ) for req in batch
            ]
        
        # å¹¶å‘å¤„ç†æ‰¹æ¬¡ä¸­çš„è¯·æ±‚
        if self.config.processing_mode == ProcessingMode.CONCURRENT:
            semaphore = asyncio.Semaphore(self.config.max_concurrent)
            tasks = [
                self._process_single_request(request, semaphore)
                for request in batch
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        else:
            # é¡ºåºå¤„ç†
            results = []
            for request in batch:
                result = await self._process_single_request(request)
                results.append(result)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        batch_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                batch_results.append(BatchResult(
                    request_id=batch[i].id,
                    success=False,
                    error=str(result),
                    processing_time=0.0
                ))
            else:
                batch_results.append(result)
        
        # æ›´æ–°å†…å­˜å³°å€¼
        current_memory = self.memory_monitor.get_memory_usage_mb()
        if current_memory > self.processing_stats["memory_peak_mb"]:
            self.processing_stats["memory_peak_mb"] = current_memory
        
        logger.info(f"æ‰¹æ¬¡ {batch_index + 1} å¤„ç†å®Œæˆ")
        return batch_results
    
    async def _process_single_request(self, request: RequestItem, semaphore: Optional[asyncio.Semaphore] = None) -> BatchResult:
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        if semaphore:
            async with semaphore:
                return await self._do_process_request(request)
        else:
            return await self._do_process_request(request)
    
    async def _do_process_request(self, request: RequestItem) -> BatchResult:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚å¤„ç†"""
        start_time = time.time()
        
        try:
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": request.model or self.primary_model,
                "messages": [{"role": "user", "content": request.prompt}],
                "temperature": request.temperature or 0.7,
                "timeout": self.config.timeout_seconds
            }
            
            # å‘é€è¯·æ±‚ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥æ–¹æ³•ï¼‰
            response = await asyncio.to_thread(
                self.client.chat.completions.create, 
                **request_params
            )
            
            processing_time = time.time() - start_time
            
            # æå–å“åº”å†…å®¹
            content = response.choices[0].message.content if response.choices else "æ— å“åº”å†…å®¹"
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') and response.usage else 0
            
            result = BatchResult(
                request_id=request.id,
                success=True,
                response=content,
                processing_time=processing_time,
                model_used=request_params["model"],
                tokens_used=tokens_used
            )
            
            self.processing_stats["completed_requests"] += 1
            self.processing_stats["total_processing_time"] += processing_time
            
            logger.debug(f"è¯·æ±‚ {request.id} å¤„ç†æˆåŠŸï¼Œè€—æ—¶ {processing_time:.2f}s")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            result = BatchResult(
                request_id=request.id,
                success=False,
                error=error_msg,
                processing_time=processing_time,
                model_used=request.model
            )
            
            self.processing_stats["failed_requests"] += 1
            self.failed_requests.append(request)
            
            logger.error(f"è¯·æ±‚ {request.id} å¤„ç†å¤±è´¥: {error_msg}")
            return result
    
    async def process_all(self, requests: List[RequestItem], 
                         progress_callback: Optional[Callable[[int, int], None]] = None) -> List[BatchResult]:
        """å¤„ç†æ‰€æœ‰è¯·æ±‚"""
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(requests)} ä¸ªè¯·æ±‚")
        start_time = time.time()
        
        # åˆ›å»ºæ‰¹æ¬¡
        batches = self._create_batch(requests)
        all_results = []
        
        # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
        for i, batch in enumerate(batches):
            batch_results = await self._process_batch(batch, i)
            all_results.extend(batch_results)
            
            # è¿›åº¦å›è°ƒ
            if progress_callback and self.config.enable_progress_callback:
                completed = (i + 1) * self.config.batch_size
                total = len(requests)
                progress_callback(min(completed, total), total)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        if self.processing_stats["completed_requests"] > 0:
            self.processing_stats["average_processing_time"] = (
                self.processing_stats["total_processing_time"] / 
                self.processing_stats["completed_requests"]
            )
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶ {total_time:.2f}s")
        logger.info(f"æˆåŠŸ: {self.processing_stats['completed_requests']}, "
                   f"å¤±è´¥: {self.processing_stats['failed_requests']}")
        
        self.results = all_results
        return all_results
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        memory_stats = self.memory_monitor.get_memory_stats()
        
        return {
            "processing_stats": self.processing_stats,
            "memory_stats": memory_stats,
            "success_rate": (
                self.processing_stats["completed_requests"] / 
                max(self.processing_stats["total_requests"], 1)
            ) * 100,
            "failed_requests_count": len(self.failed_requests)
        }

# æ¼”ç¤ºå‡½æ•°
async def demo_basic_batch_processing():
    """æ¼”ç¤ºåŸºç¡€æ‰¹é‡å¤„ç†"""
    print("\nğŸ“¦ åŸºç¡€æ‰¹é‡å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
    config = BatchConfig(
        batch_size=5,
        max_concurrent=3,
        timeout_seconds=90,
        memory_limit_mb=512
    )
    processor = BatchProcessor(config)
    
    # å‡†å¤‡æµ‹è¯•è¯·æ±‚
    test_requests = [
        RequestItem(id=f"req_{i+1}", prompt=f"ç”¨ä¸€å¥è¯è§£é‡Š{topic}ï¼ˆä¸è¶…è¿‡20å­—ï¼‰")
        for i, topic in enumerate([
            "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", 
            "è®¡ç®—æœºè§†è§‰", "å¼ºåŒ–å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "å¤§è¯­è¨€æ¨¡å‹"
        ])
    ]
    
    print(f"âœ… å‡†å¤‡å¤„ç† {len(test_requests)} ä¸ªè¯·æ±‚")
    
    # è¿›åº¦å›è°ƒå‡½æ•°
    def progress_callback(completed: int, total: int):
        progress = completed / total * 100
        print(f"ğŸ”„ å¤„ç†è¿›åº¦: {progress:.1f}% ({completed}/{total})")
    
    # å¼€å§‹å¤„ç†
    start_time = time.time()
    results = await processor.process_all(test_requests, progress_callback)
    end_time = time.time()
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š å¤„ç†ç»“æœ:")
    print(f"   - æ€»å¤„ç†æ—¶é—´: {end_time - start_time:.2f}s")
    print(f"   - æˆåŠŸè¯·æ±‚: {sum(1 for r in results if r.success)}")
    print(f"   - å¤±è´¥è¯·æ±‚: {sum(1 for r in results if not r.success)}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = processor.get_statistics()
    print(f"   - æˆåŠŸç‡: {stats['success_rate']:.1f}%")
    print(f"   - å¹³å‡å¤„ç†æ—¶é—´: {stats['processing_stats']['average_processing_time']:.2f}s")
    print(f"   - å†…å­˜å³°å€¼: {stats['memory_stats']['current_mb']:.1f}MB")

async def demo_concurrent_vs_sequential():
    """æ¼”ç¤ºå¹¶å‘ä¸é¡ºåºå¤„ç†çš„æ€§èƒ½å¯¹æ¯”"""
    print("\nâš¡ å¹¶å‘ vs é¡ºåºå¤„ç†å¯¹æ¯”")
    print("=" * 50)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_requests = [
        RequestItem(id=f"req_{i+1}", prompt=f"ç®€å•å›ç­”ï¼šä»€ä¹ˆæ˜¯æ¦‚å¿µ{i+1}ï¼Ÿ")
        for i in range(6)
    ]
    
    # 1. é¡ºåºå¤„ç†
    print("ğŸ”„ é¡ºåºå¤„ç†æµ‹è¯•...")
    sequential_config = BatchConfig(
        batch_size=1,
        max_concurrent=1,
        processing_mode=ProcessingMode.SEQUENTIAL
    )
    sequential_processor = BatchProcessor(sequential_config)
    
    sequential_start = time.time()
    sequential_results = await sequential_processor.process_all(test_requests[:3])
    sequential_time = time.time() - sequential_start
    
    # 2. å¹¶å‘å¤„ç†
    print("ğŸ”„ å¹¶å‘å¤„ç†æµ‹è¯•...")
    concurrent_config = BatchConfig(
        batch_size=3,
        max_concurrent=3,
        processing_mode=ProcessingMode.CONCURRENT
    )
    concurrent_processor = BatchProcessor(concurrent_config)
    
    concurrent_start = time.time()
    concurrent_results = await concurrent_processor.process_all(test_requests[:3])
    concurrent_time = time.time() - concurrent_start
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print(f"   é¡ºåºå¤„ç†:")
    print(f"     - å¤„ç†æ—¶é—´: {sequential_time:.2f}s")
    print(f"     - æˆåŠŸæ•°é‡: {sum(1 for r in sequential_results if r.success)}")
    
    print(f"   å¹¶å‘å¤„ç†:")
    print(f"     - å¤„ç†æ—¶é—´: {concurrent_time:.2f}s")
    print(f"     - æˆåŠŸæ•°é‡: {sum(1 for r in concurrent_results if r.success)}")
    
    if sequential_time > 0 and concurrent_time > 0:
        speedup = sequential_time / concurrent_time
        print(f"   æ€§èƒ½æå‡: {speedup:.2f}x")

async def demo_memory_monitoring():
    """æ¼”ç¤ºå†…å­˜ç›‘æ§"""
    print("\nğŸ§  å†…å­˜ç›‘æ§æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºå†…å­˜ç›‘æ§å™¨
    memory_monitor = MemoryMonitor(limit_mb=256)
    
    # æ˜¾ç¤ºåˆå§‹å†…å­˜çŠ¶æ€
    initial_stats = memory_monitor.get_memory_stats()
    print(f"ğŸ“Š åˆå§‹å†…å­˜çŠ¶æ€:")
    print(f"   - å½“å‰ä½¿ç”¨: {initial_stats['current_mb']:.1f}MB")
    print(f"   - å†…å­˜é™åˆ¶: {initial_stats['limit_mb']:.1f}MB")
    print(f"   - ä½¿ç”¨ç‡: {initial_stats['usage_percent']:.1f}%")
    
    # åˆ›å»ºå¤„ç†å™¨
    config = BatchConfig(
        batch_size=3,
        max_concurrent=2,
        memory_limit_mb=256
    )
    processor = BatchProcessor(config)
    
    # æ·»åŠ è¯·æ±‚
    test_requests = [
        RequestItem(id=f"req_{i+1}", prompt=f"ç®€çŸ­å›ç­”é—®é¢˜{i+1}")
        for i in range(6)
    ]
    
    # å¤„ç†å¹¶ç›‘æ§å†…å­˜
    await processor.process_all(test_requests)
    
    # æ˜¾ç¤ºæœ€ç»ˆå†…å­˜çŠ¶æ€
    final_stats = processor.get_statistics()['memory_stats']
    print(f"\nğŸ“Š æœ€ç»ˆå†…å­˜çŠ¶æ€:")
    print(f"   - å½“å‰ä½¿ç”¨: {final_stats['current_mb']:.1f}MB")
    print(f"   - å†…å­˜å³°å€¼: {processor.processing_stats['memory_peak_mb']:.1f}MB")

async def demo_error_handling():
    """æ¼”ç¤ºé”™è¯¯å¤„ç†"""
    print("\nğŸ›¡ï¸ é”™è¯¯å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    config = BatchConfig(
        batch_size=3,
        max_concurrent=2,
        retry_attempts=2
    )
    processor = BatchProcessor(config)
    
    # æ··åˆæ­£å¸¸å’Œå¯èƒ½å‡ºé”™çš„è¯·æ±‚
    test_requests = [
        RequestItem(id="normal_1", prompt="ä»€ä¹ˆæ˜¯AIï¼Ÿ"),
        RequestItem(id="normal_2", prompt="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"),
        RequestItem(id="invalid_model", prompt="æµ‹è¯•è¯·æ±‚", model="invalid-model-name"),
        RequestItem(id="normal_3", prompt="ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"),
    ]
    
    results = await processor.process_all(test_requests)
    
    # åˆ†æç»“æœ
    successful = [r for r in results if r.success]
    failed = [r for r in results if not r.success]
    
    print(f"ğŸ“Š é”™è¯¯å¤„ç†ç»“æœ:")
    print(f"   - æˆåŠŸè¯·æ±‚: {len(successful)}")
    print(f"   - å¤±è´¥è¯·æ±‚: {len(failed)}")
    
    if failed:
        print(f"   å¤±è´¥è¯¦æƒ…:")
        for result in failed:
            print(f"     - {result.request_id}: {result.error}")

async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ“¦ HarborAI æ‰¹é‡å¤„ç†ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹é…ç½®
    print_available_models()
    
    try:
        # åŸºç¡€æ‰¹é‡å¤„ç†æ¼”ç¤º
        await demo_basic_batch_processing()
        
        # å¹¶å‘ vs é¡ºåºå¤„ç†å¯¹æ¯”
        await demo_concurrent_vs_sequential()
        
        # å†…å­˜ç›‘æ§æ¼”ç¤º
        await demo_memory_monitoring()
        
        # é”™è¯¯å¤„ç†æ¼”ç¤º
        await demo_error_handling()
        
        print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ç”Ÿäº§ç¯å¢ƒå»ºè®®:")
        print("   1. æ ¹æ®ç³»ç»Ÿèµ„æºè°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œå¹¶å‘æ•°")
        print("   2. å®æ–½å†…å­˜ç›‘æ§å’Œé™åˆ¶æœºåˆ¶")
        print("   3. å®ç°å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        print("   4. ç›‘æ§å¤„ç†æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Š")
        print("   5. ä½¿ç”¨é€‚å½“çš„è¶…æ—¶é…ç½®ï¼ˆå½“å‰90ç§’ï¼‰")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())