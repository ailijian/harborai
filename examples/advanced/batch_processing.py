#!/usr/bin/env python3
"""
æ‰¹å¤„ç†ä¼˜åŒ–æ¼”ç¤º

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº† HarborAI çš„æ‰¹å¤„ç†ä¼˜åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. åŸç”Ÿå¼‚æ­¥æ‰¹å¤„ç†
2. å¹¶å‘æ§åˆ¶å’Œé™æµ
3. ç»“æ„åŒ–è¾“å‡ºçš„æ‰¹å¤„ç†
4. æ¨ç†æ¨¡å‹çš„æ‰¹å¤„ç†
5. æµå¼æ‰¹å¤„ç†
6. é”™è¯¯å¤„ç†å’Œé‡è¯•

åœºæ™¯ï¼š
- éœ€è¦å¤„ç†å¤§é‡æ–‡æœ¬æ•°æ®
- æ‰¹é‡ç”Ÿæˆç»“æ„åŒ–å†…å®¹
- å¹¶å‘è°ƒç”¨å¤šä¸ªAIæœåŠ¡
- ä¼˜åŒ–å¤„ç†é€Ÿåº¦å’Œèµ„æºä½¿ç”¨

ä»·å€¼ï¼š
- ä½¿ç”¨ HarborAI åŸç”Ÿå¼‚æ­¥æ”¯æŒï¼Œæ€§èƒ½æ›´ä¼˜
- æ™ºèƒ½å¹¶å‘æ§åˆ¶ï¼Œé¿å…APIé™æµ
- ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼çš„æ‰¹å¤„ç†
"""

import asyncio
import time
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import json
import os

# æ­£ç¡®çš„ HarborAI å¯¼å…¥æ–¹å¼
from harborai import HarborAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_client():
    """è·å– HarborAI å®¢æˆ·ç«¯"""
    # ä¼˜å…ˆä½¿ç”¨ DeepSeek
    if os.getenv('DEEPSEEK_API_KEY'):
        return HarborAI(
            api_key=os.getenv('DEEPSEEK_API_KEY'),
            base_url=os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')
        ), "deepseek-chat"
    
    # å…¶æ¬¡ä½¿ç”¨ Ernie
    if os.getenv('ERNIE_API_KEY'):
        return HarborAI(
            api_key=os.getenv('ERNIE_API_KEY'),
            base_url=os.getenv('ERNIE_BASE_URL', 'https://aip.baidubce.com')
        ), "ernie-3.5-8k"
    
    # æœ€åä½¿ç”¨ Doubao
    if os.getenv('DOUBAO_API_KEY'):
        return HarborAI(
            api_key=os.getenv('DOUBAO_API_KEY'),
            base_url=os.getenv('DOUBAO_BASE_URL', 'https://ark.cn-beijing.volces.com')
        ), "doubao-1-5-pro-32k-character-250715"
    
    return None, None

class BatchProcessor:
    """æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, max_concurrent: int = 5, delay_between_batches: float = 1.0):
        self.client, self.model = get_client()
        if not self.client:
            raise ValueError("è¯·è‡³å°‘è®¾ç½®ä¸€ä¸ª API Key (DEEPSEEK_API_KEY, ERNIE_API_KEY, æˆ– DOUBAO_API_KEY)")
        
        self.max_concurrent = max_concurrent
        self.delay_between_batches = delay_between_batches
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.total_tokens = 0
        self.start_time = None
    
    async def process_single_request(self, messages: List[Dict], **kwargs) -> Tuple[bool, Any, str]:
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        async with self.semaphore:
            try:
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    fallback=["deepseek-chat", "ernie-3.5-8k"],
                    retry_policy={
                        "max_attempts": 2,
                        "base_delay": 1.0,
                        "max_delay": 5.0
                    },
                    timeout=30.0,
                    **kwargs
                )
                
                self.successful_requests += 1
                if response.usage:
                    self.total_tokens += response.usage.total_tokens
                
                return True, response, ""
                
            except Exception as e:
                self.failed_requests += 1
                error_msg = str(e)
                logger.warning(f"è¯·æ±‚å¤±è´¥: {error_msg}")
                return False, None, error_msg
            finally:
                self.total_requests += 1
    
    async def process_batch(self, batch_data: List[Dict], **kwargs) -> List[Dict]:
        """å¤„ç†ä¸€æ‰¹è¯·æ±‚"""
        if self.start_time is None:
            self.start_time = time.time()
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = []
        for item in batch_data:
            messages = item.get('messages', [])
            task = self.process_single_request(messages, **kwargs)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        processed_results = []
        for i, (item, result) in enumerate(zip(batch_data, results)):
            if isinstance(result, Exception):
                processed_results.append({
                    'index': i,
                    'input': item,
                    'success': False,
                    'error': str(result),
                    'response': None
                })
            else:
                success, response, error = result
                processed_results.append({
                    'index': i,
                    'input': item,
                    'success': success,
                    'error': error,
                    'response': response
                })
        
        return processed_results
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        elapsed = time.time() - self.start_time if self.start_time else 0
        
        return {
            'total_requests': self.total_requests,
            'successful_requests': self.successful_requests,
            'failed_requests': self.failed_requests,
            'success_rate': self.successful_requests / max(self.total_requests, 1),
            'total_tokens': self.total_tokens,
            'elapsed_time': elapsed,
            'requests_per_second': self.total_requests / max(elapsed, 1),
            'tokens_per_second': self.total_tokens / max(elapsed, 1)
        }

async def demo_basic_batch_processing():
    """æ¼”ç¤ºåŸºæœ¬æ‰¹å¤„ç†"""
    print("\nğŸ”„ æ¼”ç¤ºåŸºæœ¬æ‰¹å¤„ç†")
    print("=" * 50)
    
    # å‡†å¤‡æ‰¹å¤„ç†æ•°æ®
    batch_data = [
        {'messages': [{'role': 'user', 'content': 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ'}]},
        {'messages': [{'role': 'user', 'content': 'è§£é‡Šæœºå™¨å­¦ä¹ çš„æ¦‚å¿µ'}]},
        {'messages': [{'role': 'user', 'content': 'æ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ'}]},
        {'messages': [{'role': 'user', 'content': 'è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨'}]},
        {'messages': [{'role': 'user', 'content': 'è®¡ç®—æœºè§†è§‰æŠ€æœ¯ä»‹ç»'}]}
    ]
    
    processor = BatchProcessor(max_concurrent=3)
    
    print(f"ğŸ“ å¤„ç† {len(batch_data)} ä¸ªè¯·æ±‚...")
    start_time = time.time()
    
    results = await processor.process_batch(batch_data)
    
    elapsed = time.time() - start_time
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nâœ… æ‰¹å¤„ç†å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
    
    for result in results:
        if result['success']:
            content = result['response'].choices[0].message.content[:50] if result['response'] else "æ— å†…å®¹"
            print(f"   âœ… è¯·æ±‚ {result['index'] + 1}: {content}...")
        else:
            print(f"   âŒ è¯·æ±‚ {result['index'] + 1}: {result['error']}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = processor.get_statistics()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æˆåŠŸç‡: {stats['success_rate']:.1%}")
    print(f"   æ€»Token: {stats['total_tokens']}")
    print(f"   è¯·æ±‚/ç§’: {stats['requests_per_second']:.2f}")

async def demo_structured_batch_processing():
    """æ¼”ç¤ºç»“æ„åŒ–è¾“å‡ºçš„æ‰¹å¤„ç†"""
    print("\nğŸ“Š æ¼”ç¤ºç»“æ„åŒ–è¾“å‡ºæ‰¹å¤„ç†")
    print("=" * 50)
    
    # å®šä¹‰ç»“æ„åŒ–è¾“å‡º schema
    schema = {
        "type": "object",
        "properties": {
            "topic": {"type": "string"},
            "summary": {"type": "string"},
            "key_points": {
                "type": "array",
                "items": {"type": "string"}
            },
            "difficulty": {
                "type": "string",
                "enum": ["åˆçº§", "ä¸­çº§", "é«˜çº§"]
            }
        },
        "required": ["topic", "summary", "key_points", "difficulty"],
        "additionalProperties": False
    }
    
    # å‡†å¤‡æ‰¹å¤„ç†æ•°æ®
    topics = [
        "Pythonç¼–ç¨‹åŸºç¡€",
        "æ•°æ®ç»“æ„ä¸ç®—æ³•",
        "Webå¼€å‘æ¡†æ¶",
        "æ•°æ®åº“è®¾è®¡",
        "äº‘è®¡ç®—æ¶æ„"
    ]
    
    batch_data = []
    for topic in topics:
        batch_data.append({
            'messages': [
                {'role': 'user', 'content': f'è¯·åˆ†æ"{topic}"è¿™ä¸ªæŠ€æœ¯ä¸»é¢˜'}
            ]
        })
    
    processor = BatchProcessor(max_concurrent=2)
    
    print(f"ğŸ“ å¤„ç† {len(batch_data)} ä¸ªç»“æ„åŒ–è¾“å‡ºè¯·æ±‚...")
    
    results = await processor.process_batch(
        batch_data,
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "TopicAnalysis",
                "schema": schema,
                "strict": True
            }
        }
    )
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nâœ… ç»“æ„åŒ–æ‰¹å¤„ç†å®Œæˆ")
    
    for result in results:
        if result['success'] and result['response']:
            parsed = result['response'].parsed
            if parsed:
                print(f"\n   ğŸ“‹ ä¸»é¢˜: {parsed.get('topic', 'N/A')}")
                print(f"      éš¾åº¦: {parsed.get('difficulty', 'N/A')}")
                print(f"      è¦ç‚¹: {len(parsed.get('key_points', []))} ä¸ª")
            else:
                print(f"   âš ï¸ è¯·æ±‚ {result['index'] + 1}: è§£æå¤±è´¥")
        else:
            print(f"   âŒ è¯·æ±‚ {result['index'] + 1}: {result['error']}")

async def demo_reasoning_batch_processing():
    """æ¼”ç¤ºæ¨ç†æ¨¡å‹çš„æ‰¹å¤„ç†"""
    print("\nğŸ§  æ¼”ç¤ºæ¨ç†æ¨¡å‹æ‰¹å¤„ç†")
    print("=" * 50)
    
    # å‡†å¤‡éœ€è¦æ·±åº¦æ€è€ƒçš„é—®é¢˜
    complex_questions = [
        "å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜å¯ç”¨çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Ÿ",
        "äººå·¥æ™ºèƒ½å¯¹å°±ä¸šå¸‚åœºçš„é•¿æœŸå½±å“æ˜¯ä»€ä¹ˆï¼Ÿ",
        "åŒºå—é“¾æŠ€æœ¯åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨å‰æ™¯å¦‚ä½•ï¼Ÿ"
    ]
    
    batch_data = []
    for question in complex_questions:
        batch_data.append({
            'messages': [
                {'role': 'user', 'content': question}
            ]
        })
    
    # åˆ›å»ºæ”¯æŒæ¨ç†æ¨¡å‹çš„å¤„ç†å™¨
    processor = BatchProcessor(max_concurrent=2)
    
    # å°è¯•ä½¿ç”¨æ¨ç†æ¨¡å‹
    if os.getenv('DEEPSEEK_API_KEY'):
        processor.model = "deepseek-reasoner"
    
    print(f"ğŸ“ å¤„ç† {len(batch_data)} ä¸ªå¤æ‚æ¨ç†é—®é¢˜...")
    
    results = await processor.process_batch(batch_data)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nâœ… æ¨ç†æ‰¹å¤„ç†å®Œæˆ")
    
    for i, result in enumerate(results):
        if result['success'] and result['response']:
            response = result['response']
            content = response.choices[0].message.content[:100] if response.choices else "æ— å†…å®¹"
            
            print(f"\n   ğŸ¤” é—®é¢˜ {i + 1}: {complex_questions[i]}")
            print(f"      ç­”æ¡ˆ: {content}...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ€è€ƒè¿‡ç¨‹
            if hasattr(response.choices[0].message, 'reasoning_content'):
                reasoning = response.choices[0].message.reasoning_content
                if reasoning:
                    print(f"      æ€è€ƒ: {reasoning[:80]}...")
                else:
                    print("      æ€è€ƒ: ä½¿ç”¨äº†æ™®é€šæ¨¡å‹")
            else:
                print("      æ€è€ƒ: æ— æ€è€ƒè¿‡ç¨‹è®°å½•")
        else:
            print(f"   âŒ é—®é¢˜ {i + 1}: {result['error']}")

async def demo_stream_batch_processing():
    """æ¼”ç¤ºæµå¼æ‰¹å¤„ç†"""
    print("\nğŸŒŠ æ¼”ç¤ºæµå¼æ‰¹å¤„ç†")
    print("=" * 50)
    
    client, model = get_client()
    if not client:
        print("âŒ è¯·è‡³å°‘è®¾ç½®ä¸€ä¸ª API Key")
        return
    
    # å‡†å¤‡éœ€è¦é•¿å›ç­”çš„é—®é¢˜
    questions = [
        "è¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ çš„å·¥ä½œåŸç†",
        "åˆ†æäº‘è®¡ç®—çš„å‘å±•è¶‹åŠ¿å’ŒæŒ‘æˆ˜",
        "ä»‹ç»åŒºå—é“¾æŠ€æœ¯çš„æ ¸å¿ƒæ¦‚å¿µ"
    ]
    
    print(f"ğŸ“ å¼€å§‹ {len(questions)} ä¸ªæµå¼è¯·æ±‚...")
    
    async def process_stream_request(question: str, index: int):
        """å¤„ç†å•ä¸ªæµå¼è¯·æ±‚"""
        print(f"\nğŸŒŠ æµå¼è¯·æ±‚ {index + 1}: {question}")
        
        try:
            stream = await client.chat.completions.create(
                model=model,
                messages=[{'role': 'user', 'content': question}],
                stream=True,
                fallback=["deepseek-chat", "ernie-3.5-8k"],
                retry_policy={
                    "max_attempts": 2,
                    "base_delay": 1.0,
                    "max_delay": 5.0
                },
                timeout=60.0
            )
            
            content_parts = []
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    content_parts.append(content)
                    print(content, end="", flush=True)
            
            print(f"\n   âœ… å®Œæˆï¼Œå…± {len(content_parts)} ä¸ªç‰‡æ®µ")
            return True, len(content_parts)
            
        except Exception as e:
            print(f"\n   âŒ å¤±è´¥: {e}")
            return False, 0
    
    # å¹¶å‘å¤„ç†æµå¼è¯·æ±‚
    tasks = [
        process_stream_request(question, i) 
        for i, question in enumerate(questions)
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # ç»Ÿè®¡ç»“æœ
    successful = sum(1 for result in results if isinstance(result, tuple) and result[0])
    total_chunks = sum(result[1] for result in results if isinstance(result, tuple) and result[0])
    
    print(f"\nğŸ“Š æµå¼æ‰¹å¤„ç†ç»Ÿè®¡:")
    print(f"   æˆåŠŸ: {successful}/{len(questions)}")
    print(f"   æ€»ç‰‡æ®µ: {total_chunks}")

async def demo_large_scale_batch():
    """æ¼”ç¤ºå¤§è§„æ¨¡æ‰¹å¤„ç†"""
    print("\nğŸš€ æ¼”ç¤ºå¤§è§„æ¨¡æ‰¹å¤„ç†")
    print("=" * 50)
    
    # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
    batch_size = 20
    questions = [
        f"è¯·ç®€å•ä»‹ç»ç¬¬{i+1}ä¸ªäººå·¥æ™ºèƒ½æ¦‚å¿µ" 
        for i in range(batch_size)
    ]
    
    batch_data = [
        {'messages': [{'role': 'user', 'content': question}]}
        for question in questions
    ]
    
    # ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
    processor = BatchProcessor(max_concurrent=8, delay_between_batches=0.5)
    
    print(f"ğŸ“ å¤„ç† {len(batch_data)} ä¸ªå¤§è§„æ¨¡è¯·æ±‚...")
    start_time = time.time()
    
    # åˆ†æ‰¹å¤„ç†
    chunk_size = 10
    all_results = []
    
    for i in range(0, len(batch_data), chunk_size):
        chunk = batch_data[i:i + chunk_size]
        print(f"   å¤„ç†æ‰¹æ¬¡ {i//chunk_size + 1}/{(len(batch_data) + chunk_size - 1)//chunk_size}")
        
        chunk_results = await processor.process_batch(chunk)
        all_results.extend(chunk_results)
        
        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        if i + chunk_size < len(batch_data):
            await asyncio.sleep(processor.delay_between_batches)
    
    elapsed = time.time() - start_time
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = processor.get_statistics()
    
    print(f"\nâœ… å¤§è§„æ¨¡æ‰¹å¤„ç†å®Œæˆ")
    print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print(f"   æ€»è¯·æ±‚: {stats['total_requests']}")
    print(f"   æˆåŠŸç‡: {stats['success_rate']:.1%}")
    print(f"   æ€»è€—æ—¶: {elapsed:.2f}ç§’")
    print(f"   å¹³å‡QPS: {stats['requests_per_second']:.2f}")
    print(f"   æ€»Token: {stats['total_tokens']}")
    print(f"   Token/ç§’: {stats['tokens_per_second']:.2f}")

async def demo_error_handling_batch():
    """æ¼”ç¤ºé”™è¯¯å¤„ç†å’Œé‡è¯•"""
    print("\nğŸ›¡ï¸ æ¼”ç¤ºé”™è¯¯å¤„ç†å’Œé‡è¯•")
    print("=" * 50)
    
    # å‡†å¤‡åŒ…å«å¯èƒ½å¤±è´¥çš„è¯·æ±‚
    batch_data = [
        {'messages': [{'role': 'user', 'content': 'æ­£å¸¸è¯·æ±‚ï¼šä»€ä¹ˆæ˜¯AIï¼Ÿ'}]},
        {'messages': [{'role': 'user', 'content': 'è¶…é•¿è¯·æ±‚ï¼š' + 'x' * 10000}]},  # å¯èƒ½å¤±è´¥
        {'messages': [{'role': 'user', 'content': 'æ­£å¸¸è¯·æ±‚ï¼šæœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ'}]},
        {'messages': [{'role': 'user', 'content': ''}]},  # ç©ºè¯·æ±‚ï¼Œå¯èƒ½å¤±è´¥
        {'messages': [{'role': 'user', 'content': 'æ­£å¸¸è¯·æ±‚ï¼šæ·±åº¦å­¦ä¹ çš„åº”ç”¨'}]}
    ]
    
    processor = BatchProcessor(max_concurrent=3)
    
    print(f"ğŸ“ å¤„ç† {len(batch_data)} ä¸ªåŒ…å«é”™è¯¯çš„è¯·æ±‚...")
    
    results = await processor.process_batch(batch_data)
    
    # åˆ†æç»“æœ
    successful_results = [r for r in results if r['success']]
    failed_results = [r for r in results if not r['success']]
    
    print(f"\nğŸ“Š é”™è¯¯å¤„ç†ç»“æœ:")
    print(f"   æˆåŠŸ: {len(successful_results)}")
    print(f"   å¤±è´¥: {len(failed_results)}")
    
    if failed_results:
        print(f"   å¤±è´¥è¯¦æƒ…:")
        for result in failed_results:
            print(f"     - {result.request_id}: {result.error}")

async def demo_mixed_format_batch():
    """æ¼”ç¤ºæ··åˆæ ¼å¼æ‰¹å¤„ç†"""
    print("\nğŸ­ æ¼”ç¤ºæ··åˆæ ¼å¼æ‰¹å¤„ç†")
    print("=" * 50)
    
    client, model = get_client()
    if not client:
        print("âŒ è¯·è‡³å°‘è®¾ç½®ä¸€ä¸ª API Key")
        return
    
    # å®šä¹‰ä¸åŒç±»å‹çš„è¯·æ±‚
    requests = [
        {
            'type': 'normal',
            'messages': [{'role': 'user', 'content': 'ä»€ä¹ˆæ˜¯äº‘è®¡ç®—ï¼Ÿ'}],
            'params': {}
        },
        {
            'type': 'structured',
            'messages': [{'role': 'user', 'content': 'åˆ†æPythonç¼–ç¨‹è¯­è¨€'}],
            'params': {
                'response_format': {
                    "type": "json_schema",
                    "json_schema": {
                        "name": "LanguageAnalysis",
                        "schema": {
                            "type": "object",
                            "properties": {
                                "language": {"type": "string"},
                                "strengths": {"type": "array", "items": {"type": "string"}},
                                "use_cases": {"type": "array", "items": {"type": "string"}}
                            },
                            "required": ["language", "strengths", "use_cases"]
                        }
                    }
                }
            }
        },
        {
            'type': 'stream',
            'messages': [{'role': 'user', 'content': 'è¯¦ç»†è§£é‡ŠåŒºå—é“¾æŠ€æœ¯'}],
            'params': {'stream': True}
        }
    ]
    
    print(f"ğŸ“ å¤„ç† {len(requests)} ä¸ªæ··åˆæ ¼å¼è¯·æ±‚...")
    
    async def process_mixed_request(request: Dict, index: int):
        """å¤„ç†æ··åˆæ ¼å¼è¯·æ±‚"""
        try:
            if request['type'] == 'stream':
                print(f"\nğŸŒŠ æµå¼è¯·æ±‚ {index + 1}:")
                
                stream = await client.chat.completions.create(
                    model=model,
                    messages=request['messages'],
                    fallback=["deepseek-chat", "ernie-3.5-8k"],
                    **request['params']
                )
                
                content_parts = []
                async for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        content_parts.append(content)
                        print(content, end="", flush=True)
                
                print(f"\n   âœ… æµå¼å®Œæˆï¼Œ{len(content_parts)} ç‰‡æ®µ")
                return {'type': 'stream', 'success': True, 'chunks': len(content_parts)}
            
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=request['messages'],
                    fallback=["deepseek-chat", "ernie-3.5-8k"],
                    retry_policy={
                        "max_attempts": 2,
                        "base_delay": 1.0,
                        "max_delay": 5.0
                    },
                    **request['params']
                )
                
                if request['type'] == 'structured':
                    return {
                        'type': 'structured', 
                        'success': True, 
                        'parsed': response.parsed
                    }
                else:
                    return {
                        'type': 'normal', 
                        'success': True, 
                        'content': response.choices[0].message.content[:100]
                    }
        
        except Exception as e:
            return {'type': request['type'], 'success': False, 'error': str(e)}
    
    # å¹¶å‘å¤„ç†
    tasks = [
        process_mixed_request(request, i) 
        for i, request in enumerate(requests)
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š æ··åˆæ ¼å¼æ‰¹å¤„ç†ç»“æœ:")
    for i, result in enumerate(results):
        if isinstance(result, dict) and result['success']:
            if result['type'] == 'normal':
                print(f"   âœ… æ™®é€šè¯·æ±‚ {i + 1}: {result['content']}...")
            elif result['type'] == 'structured':
                print(f"   âœ… ç»“æ„åŒ–è¯·æ±‚ {i + 1}: {result['parsed']}")
            elif result['type'] == 'stream':
                print(f"   âœ… æµå¼è¯·æ±‚ {i + 1}: {result['chunks']} ç‰‡æ®µ")
        else:
            error = result.get('error', str(result)) if isinstance(result, dict) else str(result)
            print(f"   âŒ è¯·æ±‚ {i + 1}: {error[:50]}...")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ HarborAI æ‰¹å¤„ç†ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    client, model = get_client()
    if not client:
        print("âš ï¸ è­¦å‘Š: æœªè®¾ç½®ä»»ä½• API Key")
        print("è¯·è®¾ç½® DEEPSEEK_API_KEY, ERNIE_API_KEY, æˆ– DOUBAO_API_KEY")
        return
    
    print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: {model}")
    
    demos = [
        ("åŸºæœ¬æ‰¹å¤„ç†", demo_basic_batch_processing),
        ("ç»“æ„åŒ–è¾“å‡ºæ‰¹å¤„ç†", demo_structured_batch_processing),
        ("æ¨ç†æ¨¡å‹æ‰¹å¤„ç†", demo_reasoning_batch_processing),
        ("æµå¼æ‰¹å¤„ç†", demo_stream_batch_processing),
        ("å¤§è§„æ¨¡æ‰¹å¤„ç†", demo_large_scale_batch),
        ("é”™è¯¯å¤„ç†å’Œé‡è¯•", demo_error_handling_batch),
        ("æ··åˆæ ¼å¼æ‰¹å¤„ç†", demo_mixed_format_batch)
    ]
    
    for name, demo_func in demos:
        try:
            await demo_func()
            await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        except Exception as e:
            print(f"âŒ {name} æ¼”ç¤ºå¤±è´¥: {e}")
    
    print("\nğŸ‰ æ‰¹å¤„ç†æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
    print("1. ä½¿ç”¨åŸç”Ÿå¼‚æ­¥æ”¯æŒï¼Œé¿å… asyncio.to_thread")
    print("2. é€šè¿‡ Semaphore æ§åˆ¶å¹¶å‘æ•°ï¼Œé¿å…APIé™æµ")
    print("3. æ”¯æŒæ™®é€šã€ç»“æ„åŒ–ã€æµå¼ç­‰å¤šç§æ ¼å¼çš„æ‰¹å¤„ç†")
    print("4. å†…ç½®é‡è¯•å’Œé™çº§æœºåˆ¶ï¼Œæé«˜æˆåŠŸç‡")
    print("5. è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä¾¿äºæ€§èƒ½ç›‘æ§")
    print("6. çµæ´»çš„é”™è¯¯å¤„ç†ï¼Œç¡®ä¿æ‰¹å¤„ç†çš„å¥å£®æ€§")

if __name__ == "__main__":
    asyncio.run(main())