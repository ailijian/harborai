#!/usr/bin/env python3
"""
HarborAI æ‰¹é‡å¤„ç†ä¼˜åŒ–æ¼”ç¤º

åœºæ™¯æè¿°:
åœ¨éœ€è¦å¤„ç†å¤§é‡AIè¯·æ±‚çš„åœºæ™¯ä¸­ï¼Œå¦‚æ‰¹é‡æ–‡æ¡£åˆ†æã€å¤§è§„æ¨¡æ•°æ®å¤„ç†ç­‰ï¼Œ
é€šè¿‡æ™ºèƒ½æ‰¹é‡èšåˆã€å¹¶å‘æ§åˆ¶ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡å¤„ç†æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡ã€‚

åº”ç”¨ä»·å€¼:
- å¤§å¹…æå‡å¤„ç†æ•ˆç‡å’Œååé‡
- é™ä½APIè°ƒç”¨æˆæœ¬
- ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡
- æ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†åœºæ™¯

æ ¸å¿ƒåŠŸèƒ½:
1. æ™ºèƒ½æ‰¹é‡èšåˆ
2. å¹¶å‘æ§åˆ¶ä¸é™æµ
3. å†…å­˜ç®¡ç†ä¼˜åŒ–
4. è¿›åº¦è¿½è¸ªä¸ç›‘æ§
5. ç»“æœåˆ†å‘æœºåˆ¶
"""

import asyncio
import time
import random
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable, Any, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
import psutil
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from harborai import HarborAI
from harborai.types.chat import ChatCompletion

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BatchStatus(Enum):
    """æ‰¹æ¬¡çŠ¶æ€"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class ProcessingMode(Enum):
    """å¤„ç†æ¨¡å¼"""
    SEQUENTIAL = "sequential"    # é¡ºåºå¤„ç†
    CONCURRENT = "concurrent"    # å¹¶å‘å¤„ç†
    BATCH = "batch"             # æ‰¹é‡å¤„ç†

@dataclass
class BatchConfig:
    """æ‰¹é‡å¤„ç†é…ç½®"""
    batch_size: int = 10
    max_concurrent_batches: int = 5
    max_wait_time: float = 5.0
    memory_limit_mb: int = 1024
    retry_attempts: int = 3
    timeout_per_request: float = 30.0

@dataclass
class RequestItem:
    """è¯·æ±‚é¡¹"""
    id: str
    messages: List[Dict]
    model: str = "deepseek-chat"
    kwargs: Dict = field(default_factory=dict)
    priority: int = 0
    created_at: datetime = field(default_factory=datetime.now)
    
@dataclass
class BatchResult:
    """æ‰¹æ¬¡ç»“æœ"""
    batch_id: str
    status: BatchStatus
    items: List[RequestItem]
    results: List[Optional[ChatCompletion]] = field(default_factory=list)
    errors: List[Optional[Exception]] = field(default_factory=list)
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    total_cost: float = 0.0
    
    def get_processing_time(self) -> float:
        """è·å–å¤„ç†æ—¶é—´"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0
    
    def get_success_rate(self) -> float:
        """è·å–æˆåŠŸç‡"""
        if not self.results:
            return 0.0
        successful = sum(1 for result in self.results if result is not None)
        return successful / len(self.results)

class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""
    
    def __init__(self, limit_mb: int = 1024):
        self.limit_mb = limit_mb
        self.limit_bytes = limit_mb * 1024 * 1024
        
    def get_memory_usage(self) -> Dict:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024,
            "percent": process.memory_percent(),
            "available_mb": psutil.virtual_memory().available / 1024 / 1024
        }
    
    def is_memory_available(self, estimated_usage_mb: float = 0) -> bool:
        """æ£€æŸ¥å†…å­˜æ˜¯å¦å¯ç”¨"""
        current_usage = self.get_memory_usage()
        projected_usage = current_usage["rss_mb"] + estimated_usage_mb
        return projected_usage < self.limit_mb
    
    def wait_for_memory(self, required_mb: float = 100, timeout: float = 60.0):
        """ç­‰å¾…å†…å­˜å¯ç”¨"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            if self.is_memory_available(required_mb):
                return True
            
            logger.info(f"Waiting for memory... Current usage: {self.get_memory_usage()['rss_mb']:.1f}MB")
            time.sleep(1.0)
        
        return False

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, 
                 api_key: str,
                 base_url: str = "https://api.deepseek.com/v1",
                 config: Optional[BatchConfig] = None):
        self.client = HarborAI(api_key=api_key, base_url=base_url)
        self.config = config or BatchConfig()
        self.memory_monitor = MemoryMonitor(self.config.memory_limit_mb)
        
        # æ‰¹æ¬¡ç®¡ç†
        self.pending_requests: List[RequestItem] = []
        self.active_batches: Dict[str, BatchResult] = {}
        self.completed_batches: List[BatchResult] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_processed = 0
        self.total_cost = 0.0
        self.start_time = datetime.now()
        
        # å¹¶å‘æ§åˆ¶
        self.semaphore = asyncio.Semaphore(self.config.max_concurrent_batches)
        self.batch_counter = 0
        
    def add_request(self, messages: List[Dict], model: str = "deepseek-chat", priority: int = 0, **kwargs) -> str:
        """æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—"""
        request_id = f"req_{int(time.time() * 1000)}_{random.randint(1000, 9999)}"
        
        request_item = RequestItem(
            id=request_id,
            messages=messages,
            model=model,
            priority=priority,
            kwargs=kwargs
        )
        
        self.pending_requests.append(request_item)
        logger.info(f"Added request {request_id} to queue (queue size: {len(self.pending_requests)})")
        
        return request_id
    
    def _create_batch(self) -> Optional[BatchResult]:
        """åˆ›å»ºæ‰¹æ¬¡"""
        if not self.pending_requests:
            return None
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.pending_requests.sort(key=lambda x: x.priority, reverse=True)
        
        # å–å‡ºä¸€æ‰¹è¯·æ±‚
        batch_size = min(self.config.batch_size, len(self.pending_requests))
        batch_items = self.pending_requests[:batch_size]
        self.pending_requests = self.pending_requests[batch_size:]
        
        # åˆ›å»ºæ‰¹æ¬¡
        self.batch_counter += 1
        batch_id = f"batch_{self.batch_counter}_{int(time.time())}"
        
        batch = BatchResult(
            batch_id=batch_id,
            status=BatchStatus.PENDING,
            items=batch_items,
            results=[None] * len(batch_items),
            errors=[None] * len(batch_items)
        )
        
        logger.info(f"Created batch {batch_id} with {len(batch_items)} items")
        return batch
    
    async def _process_batch(self, batch: BatchResult) -> BatchResult:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        async with self.semaphore:
            batch.status = BatchStatus.PROCESSING
            batch.start_time = datetime.now()
            
            logger.info(f"Processing batch {batch.batch_id}...")
            
            # æ£€æŸ¥å†…å­˜
            if not self.memory_monitor.is_memory_available(100):  # é¢„ä¼°100MB
                logger.warning("Memory limit reached, waiting...")
                if not self.memory_monitor.wait_for_memory(100, 30.0):
                    batch.status = BatchStatus.FAILED
                    return batch
            
            # å¹¶å‘å¤„ç†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰è¯·æ±‚
            tasks = []
            for i, item in enumerate(batch.items):
                task = self._process_single_request(item, i, batch.batch_id)
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            total_cost = 0.0
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    batch.errors[i] = result
                    logger.error(f"Request {batch.items[i].id} failed: {str(result)}")
                else:
                    batch.results[i] = result
                    # ä¼°ç®—æˆæœ¬
                    if result and hasattr(result, 'usage') and result.usage:
                        total_cost += result.usage.total_tokens * 0.0001  # ç®€åŒ–æˆæœ¬è®¡ç®—
            
            batch.total_cost = total_cost
            batch.end_time = datetime.now()
            batch.status = BatchStatus.COMPLETED
            
            # æ›´æ–°ç»Ÿè®¡
            self.total_processed += len(batch.items)
            self.total_cost += total_cost
            
            logger.info(f"Batch {batch.batch_id} completed in {batch.get_processing_time():.2f}s")
            logger.info(f"Success rate: {batch.get_success_rate():.1%}, Cost: ${batch.total_cost:.6f}")
            
            return batch
    
    async def _process_single_request(self, item: RequestItem, index: int, batch_id: str) -> ChatCompletion:
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        for attempt in range(self.config.retry_attempts):
            try:
                response = await self.client.chat.completions.create(
                    model=item.model,
                    messages=item.messages,
                    timeout=self.config.timeout_per_request,
                    **item.kwargs
                )
                
                logger.debug(f"Request {item.id} in batch {batch_id} completed (attempt {attempt + 1})")
                return response
                
            except Exception as e:
                logger.warning(f"Request {item.id} attempt {attempt + 1} failed: {str(e)}")
                if attempt == self.config.retry_attempts - 1:
                    raise e
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    
    async def process_all(self, progress_callback: Optional[Callable] = None) -> List[BatchResult]:
        """å¤„ç†æ‰€æœ‰å¾…å¤„ç†è¯·æ±‚"""
        logger.info(f"Starting batch processing of {len(self.pending_requests)} requests...")
        
        all_batches = []
        
        while self.pending_requests or self.active_batches:
            # åˆ›å»ºæ–°æ‰¹æ¬¡
            while len(self.active_batches) < self.config.max_concurrent_batches and self.pending_requests:
                batch = self._create_batch()
                if batch:
                    self.active_batches[batch.batch_id] = batch
                    
                    # å¯åŠ¨æ‰¹æ¬¡å¤„ç†ä»»åŠ¡
                    task = asyncio.create_task(self._process_batch(batch))
                    
                    # æ·»åŠ å®Œæˆå›è°ƒ
                    def batch_completed(batch_id):
                        def callback(task):
                            completed_batch = task.result()
                            if batch_id in self.active_batches:
                                del self.active_batches[batch_id]
                            self.completed_batches.append(completed_batch)
                            all_batches.append(completed_batch)
                            
                            if progress_callback:
                                progress_callback(completed_batch)
                        return callback
                    
                    task.add_done_callback(batch_completed(batch.batch_id))
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´
            await asyncio.sleep(0.1)
        
        # ç­‰å¾…æ‰€æœ‰æ´»è·ƒæ‰¹æ¬¡å®Œæˆ
        while self.active_batches:
            await asyncio.sleep(0.1)
        
        logger.info(f"All batches completed. Total processed: {self.total_processed}")
        return all_batches
    
    def get_statistics(self) -> Dict:
        """è·å–å¤„ç†ç»Ÿè®¡"""
        total_time = (datetime.now() - self.start_time).total_seconds()
        
        return {
            "total_processed": self.total_processed,
            "total_cost": self.total_cost,
            "total_time": total_time,
            "throughput": self.total_processed / total_time if total_time > 0 else 0,
            "average_cost_per_request": self.total_cost / self.total_processed if self.total_processed > 0 else 0,
            "completed_batches": len(self.completed_batches),
            "active_batches": len(self.active_batches),
            "pending_requests": len(self.pending_requests),
            "memory_usage": self.memory_monitor.get_memory_usage()
        }

# æ¼”ç¤ºå‡½æ•°
async def demo_basic_batch_processing():
    """æ¼”ç¤ºåŸºç¡€æ‰¹é‡å¤„ç†"""
    print("\nğŸ“¦ åŸºç¡€æ‰¹é‡å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
    processor = BatchProcessor(
        api_key="your-deepseek-key",
        config=BatchConfig(
            batch_size=5,
            max_concurrent_batches=2,
            max_wait_time=3.0
        )
    )
    
    # æ·»åŠ æµ‹è¯•è¯·æ±‚
    test_questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
        "æ·±åº¦å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
        "è®¡ç®—æœºè§†è§‰çš„ä¸»è¦æŠ€æœ¯æœ‰å“ªäº›ï¼Ÿ",
        "å¼ºåŒ–å­¦ä¹ çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ",
        "AIåœ¨åŒ»ç–—é¢†åŸŸæœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
        "è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„æ ¸å¿ƒæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    # æ·»åŠ è¯·æ±‚åˆ°å¤„ç†å™¨
    request_ids = []
    for i, question in enumerate(test_questions):
        messages = [{"role": "user", "content": question}]
        request_id = processor.add_request(messages, priority=random.randint(1, 5))
        request_ids.append(request_id)
    
    print(f"âœ… å·²æ·»åŠ  {len(request_ids)} ä¸ªè¯·æ±‚åˆ°å¤„ç†é˜Ÿåˆ—")
    
    # è¿›åº¦å›è°ƒå‡½æ•°
    def progress_callback(batch: BatchResult):
        print(f"ğŸ”„ æ‰¹æ¬¡ {batch.batch_id} å®Œæˆ:")
        print(f"   - å¤„ç†æ—¶é—´: {batch.get_processing_time():.2f}s")
        print(f"   - æˆåŠŸç‡: {batch.get_success_rate():.1%}")
        print(f"   - æˆæœ¬: ${batch.total_cost:.6f}")
    
    # å¼€å§‹å¤„ç†
    start_time = time.time()
    batches = await processor.process_all(progress_callback)
    end_time = time.time()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = processor.get_statistics()
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   - æ€»å¤„ç†æ•°é‡: {stats['total_processed']}")
    print(f"   - æ€»å¤„ç†æ—¶é—´: {end_time - start_time:.2f}s")
    print(f"   - ååé‡: {stats['throughput']:.2f} req/s")
    print(f"   - æ€»æˆæœ¬: ${stats['total_cost']:.6f}")
    print(f"   - å¹³å‡æˆæœ¬: ${stats['average_cost_per_request']:.6f}")
    print(f"   - å†…å­˜ä½¿ç”¨: {stats['memory_usage']['rss_mb']:.1f}MB")

async def demo_performance_comparison():
    """æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”"""
    print("\nâš¡ æ€§èƒ½å¯¹æ¯”æ¼”ç¤º")
    print("=" * 50)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_questions = [
        f"è¯·è§£é‡Šæ¦‚å¿µ{i}: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é—®é¢˜ï¼Œç”¨äºæ€§èƒ½å¯¹æ¯”ã€‚" 
        for i in range(20)
    ]
    
    # 1. é¡ºåºå¤„ç†
    print("ğŸ”„ é¡ºåºå¤„ç†æµ‹è¯•...")
    client = HarborAI(api_key="your-deepseek-key", base_url="https://api.deepseek.com/v1")
    
    sequential_start = time.time()
    sequential_results = []
    
    for question in test_questions[:5]:  # åªæµ‹è¯•5ä¸ªè¯·æ±‚
        try:
            response = await client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": question}],
                max_tokens=100
            )
            sequential_results.append(response)
        except Exception as e:
            print(f"âŒ é¡ºåºå¤„ç†å¤±è´¥: {str(e)}")
            sequential_results.append(None)
    
    sequential_time = time.time() - sequential_start
    sequential_success = sum(1 for r in sequential_results if r is not None)
    
    # 2. æ‰¹é‡å¤„ç†
    print("ğŸ”„ æ‰¹é‡å¤„ç†æµ‹è¯•...")
    processor = BatchProcessor(
        api_key="your-deepseek-key",
        config=BatchConfig(
            batch_size=5,
            max_concurrent_batches=3
        )
    )
    
    # æ·»åŠ ç›¸åŒçš„è¯·æ±‚
    for question in test_questions[:5]:
        messages = [{"role": "user", "content": question}]
        processor.add_request(messages, max_tokens=100)
    
    batch_start = time.time()
    batches = await processor.process_all()
    batch_time = time.time() - batch_start
    
    batch_success = sum(batch.get_success_rate() * len(batch.items) for batch in batches)
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print(f"   é¡ºåºå¤„ç†:")
    print(f"     - å¤„ç†æ—¶é—´: {sequential_time:.2f}s")
    print(f"     - æˆåŠŸæ•°é‡: {sequential_success}")
    print(f"     - ååé‡: {sequential_success/sequential_time:.2f} req/s")
    
    print(f"   æ‰¹é‡å¤„ç†:")
    print(f"     - å¤„ç†æ—¶é—´: {batch_time:.2f}s")
    print(f"     - æˆåŠŸæ•°é‡: {int(batch_success)}")
    print(f"     - ååé‡: {batch_success/batch_time:.2f} req/s")
    
    if sequential_time > 0 and batch_time > 0:
        speedup = sequential_time / batch_time
        print(f"   æ€§èƒ½æå‡: {speedup:.2f}x")

async def demo_memory_management():
    """æ¼”ç¤ºå†…å­˜ç®¡ç†"""
    print("\nğŸ§  å†…å­˜ç®¡ç†æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºå†…å­˜ç›‘æ§å™¨
    memory_monitor = MemoryMonitor(limit_mb=512)  # è®¾ç½®è¾ƒä½çš„é™åˆ¶ç”¨äºæ¼”ç¤º
    
    # æ˜¾ç¤ºåˆå§‹å†…å­˜çŠ¶æ€
    initial_memory = memory_monitor.get_memory_usage()
    print(f"ğŸ“Š åˆå§‹å†…å­˜çŠ¶æ€:")
    print(f"   - RSS: {initial_memory['rss_mb']:.1f}MB")
    print(f"   - å¯ç”¨å†…å­˜: {initial_memory['available_mb']:.1f}MB")
    print(f"   - å†…å­˜é™åˆ¶: {memory_monitor.limit_mb}MB")
    
    # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨ï¼ˆè¾ƒå°çš„æ‰¹æ¬¡å¤§å°ï¼‰
    processor = BatchProcessor(
        api_key="your-deepseek-key",
        config=BatchConfig(
            batch_size=3,
            max_concurrent_batches=2,
            memory_limit_mb=512
        )
    )
    
    # æ·»åŠ ä¸€äº›è¯·æ±‚
    for i in range(10):
        messages = [{"role": "user", "content": f"æµ‹è¯•è¯·æ±‚ {i+1}"}]
        processor.add_request(messages)
    
    # ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
    def memory_progress_callback(batch: BatchResult):
        current_memory = memory_monitor.get_memory_usage()
        print(f"ğŸ”„ æ‰¹æ¬¡ {batch.batch_id} å®Œæˆ - å†…å­˜ä½¿ç”¨: {current_memory['rss_mb']:.1f}MB")
    
    # å¤„ç†è¯·æ±‚
    await processor.process_all(memory_progress_callback)
    
    # æ˜¾ç¤ºæœ€ç»ˆå†…å­˜çŠ¶æ€
    final_memory = memory_monitor.get_memory_usage()
    print(f"\nğŸ“Š æœ€ç»ˆå†…å­˜çŠ¶æ€:")
    print(f"   - RSS: {final_memory['rss_mb']:.1f}MB")
    print(f"   - å†…å­˜å¢é•¿: {final_memory['rss_mb'] - initial_memory['rss_mb']:.1f}MB")

async def demo_priority_processing():
    """æ¼”ç¤ºä¼˜å…ˆçº§å¤„ç†"""
    print("\nğŸ¯ ä¼˜å…ˆçº§å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    processor = BatchProcessor(
        api_key="your-deepseek-key",
        config=BatchConfig(batch_size=3, max_concurrent_batches=1)
    )
    
    # æ·»åŠ ä¸åŒä¼˜å…ˆçº§çš„è¯·æ±‚
    requests_data = [
        ("ç´§æ€¥é—®é¢˜", "è¿™æ˜¯ä¸€ä¸ªç´§æ€¥é—®é¢˜ï¼Œéœ€è¦ç«‹å³å¤„ç†", 10),
        ("æ™®é€šé—®é¢˜1", "è¿™æ˜¯ä¸€ä¸ªæ™®é€šé—®é¢˜", 5),
        ("ä½ä¼˜å…ˆçº§é—®é¢˜", "è¿™æ˜¯ä¸€ä¸ªä½ä¼˜å…ˆçº§é—®é¢˜", 1),
        ("é«˜ä¼˜å…ˆçº§é—®é¢˜", "è¿™æ˜¯ä¸€ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜", 8),
        ("æ™®é€šé—®é¢˜2", "è¿™æ˜¯å¦ä¸€ä¸ªæ™®é€šé—®é¢˜", 5),
        ("æœ€é«˜ä¼˜å…ˆçº§", "è¿™æ˜¯æœ€é«˜ä¼˜å…ˆçº§é—®é¢˜", 15)
    ]
    
    # éšæœºé¡ºåºæ·»åŠ è¯·æ±‚
    import random
    random.shuffle(requests_data)
    
    for name, content, priority in requests_data:
        messages = [{"role": "user", "content": content}]
        request_id = processor.add_request(messages, priority=priority)
        print(f"ğŸ“ æ·»åŠ è¯·æ±‚: {name} (ä¼˜å…ˆçº§: {priority})")
    
    # å¤„ç†è¯·æ±‚å¹¶è§‚å¯Ÿå¤„ç†é¡ºåº
    def priority_progress_callback(batch: BatchResult):
        print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch.batch_id}:")
        for item in batch.items:
            print(f"   - è¯·æ±‚å†…å®¹: {item.messages[0]['content'][:20]}... (ä¼˜å…ˆçº§: {item.priority})")
    
    await processor.process_all(priority_progress_callback)

async def demo_large_scale_processing():
    """æ¼”ç¤ºå¤§è§„æ¨¡å¤„ç†"""
    print("\nğŸš€ å¤§è§„æ¨¡å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºå¤§è§„æ¨¡å¤„ç†å™¨
    processor = BatchProcessor(
        api_key="your-deepseek-key",
        config=BatchConfig(
            batch_size=10,
            max_concurrent_batches=5,
            memory_limit_mb=1024
        )
    )
    
    # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
    print("ğŸ“ ç”Ÿæˆæµ‹è¯•æ•°æ®...")
    categories = ["ç§‘æŠ€", "åŒ»ç–—", "æ•™è‚²", "é‡‘è", "ç¯å¢ƒ"]
    
    for i in range(50):  # ç”Ÿæˆ50ä¸ªè¯·æ±‚
        category = random.choice(categories)
        content = f"è¯·åˆ†æ{category}é¢†åŸŸçš„å‘å±•è¶‹åŠ¿å’ŒæŒ‘æˆ˜ - é—®é¢˜{i+1}"
        messages = [{"role": "user", "content": content}]
        processor.add_request(messages, priority=random.randint(1, 10))
    
    print(f"âœ… å·²ç”Ÿæˆ 50 ä¸ªæµ‹è¯•è¯·æ±‚")
    
    # å¤„ç†ç»Ÿè®¡
    processed_count = 0
    total_cost = 0.0
    
    def large_scale_progress_callback(batch: BatchResult):
        nonlocal processed_count, total_cost
        processed_count += len(batch.items)
        total_cost += batch.total_cost
        
        progress = processed_count / 50 * 100
        print(f"ğŸ”„ è¿›åº¦: {progress:.1f}% ({processed_count}/50) - æ‰¹æ¬¡æˆåŠŸç‡: {batch.get_success_rate():.1%}")
    
    # å¼€å§‹å¤§è§„æ¨¡å¤„ç†
    start_time = time.time()
    batches = await processor.process_all(large_scale_progress_callback)
    end_time = time.time()
    
    # æœ€ç»ˆç»Ÿè®¡
    stats = processor.get_statistics()
    print(f"\nğŸ“Š å¤§è§„æ¨¡å¤„ç†ç»Ÿè®¡:")
    print(f"   - æ€»å¤„ç†æ—¶é—´: {end_time - start_time:.2f}s")
    print(f"   - å¹³å‡ååé‡: {stats['throughput']:.2f} req/s")
    print(f"   - æ€»æˆæœ¬: ${stats['total_cost']:.6f}")
    print(f"   - å†…å­˜å³°å€¼: {stats['memory_usage']['rss_mb']:.1f}MB")
    print(f"   - æ‰¹æ¬¡æ•°é‡: {len(batches)}")
    
    # æˆåŠŸç‡åˆ†æ
    overall_success_rate = sum(batch.get_success_rate() * len(batch.items) for batch in batches) / 50
    print(f"   - æ•´ä½“æˆåŠŸç‡: {overall_success_rate:.1%}")

async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ“¦ HarborAI æ‰¹é‡å¤„ç†ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åŸºç¡€æ‰¹é‡å¤„ç†æ¼”ç¤º
        await demo_basic_batch_processing()
        
        # æ€§èƒ½å¯¹æ¯”æ¼”ç¤º
        await demo_performance_comparison()
        
        # å†…å­˜ç®¡ç†æ¼”ç¤º
        await demo_memory_management()
        
        # ä¼˜å…ˆçº§å¤„ç†æ¼”ç¤º
        await demo_priority_processing()
        
        # å¤§è§„æ¨¡å¤„ç†æ¼”ç¤º
        await demo_large_scale_processing()
        
        print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ç”Ÿäº§ç¯å¢ƒå»ºè®®:")
        print("   1. æ ¹æ®ç³»ç»Ÿèµ„æºè°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œå¹¶å‘æ•°")
        print("   2. å®æ–½å†…å­˜ç›‘æ§å’Œé™åˆ¶æœºåˆ¶")
        print("   3. ä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—å¤„ç†é‡è¦è¯·æ±‚")
        print("   4. ç›‘æ§å¤„ç†æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Š")
        print("   5. å®ç°æ•…éšœæ¢å¤å’Œé‡è¯•æœºåˆ¶")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())