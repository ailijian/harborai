#!/usr/bin/env python3
"""
HarborAI æ•°æ®åˆ†æåŠ©æ‰‹

åœºæ™¯æè¿°:
æ„å»ºæ™ºèƒ½æ•°æ®åˆ†æå¹³å°ï¼Œæ”¯æŒè‡ªåŠ¨åŒ–æ•°æ®å¤„ç†ã€ç»Ÿè®¡åˆ†æã€å¯è§†åŒ–ç”Ÿæˆå’Œæ´å¯Ÿå‘ç°ã€‚
é€‚ç”¨äºä¸šåŠ¡åˆ†æã€å¸‚åœºç ”ç©¶ã€è¿è¥ä¼˜åŒ–ç­‰å¤šç§æ•°æ®é©±åŠ¨çš„å†³ç­–åœºæ™¯ã€‚

åº”ç”¨ä»·å€¼:
- è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
- æ™ºèƒ½ç»Ÿè®¡åˆ†æå’Œæ¨¡å¼è¯†åˆ«
- è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’ŒæŠ¥å‘Šç”Ÿæˆ
- å¯è§†åŒ–å›¾è¡¨è‡ªåŠ¨ç”Ÿæˆ
- ä¸šåŠ¡æ´å¯Ÿå’Œå»ºè®®æä¾›

æ ¸å¿ƒåŠŸèƒ½:
1. è‡ªç„¶è¯­è¨€æ•°æ®æŸ¥è¯¢å’Œåˆ†æ
2. è‡ªåŠ¨åŒ–ç»Ÿè®¡åˆ†æå’Œå‡è®¾æ£€éªŒ
3. æ•°æ®å¯è§†åŒ–å’Œå›¾è¡¨ç”Ÿæˆ
4. å¼‚å¸¸æ£€æµ‹å’Œè¶‹åŠ¿åˆ†æ
5. æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆå’Œæ´å¯Ÿå‘ç°
"""

import asyncio
import json
import time
import uuid
import logging
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field, asdict
from enum import Enum
import io
import base64
from collections import defaultdict
import scipy.stats as stats
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ æœ¬åœ°æºç è·¯å¾„
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from harborai import HarborAI
    from harborai.core.base_plugin import ChatCompletion
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥ HarborAIï¼Œè¯·æ£€æŸ¥è·¯å¾„é…ç½®")
    exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class AnalysisType(Enum):
    """åˆ†æç±»å‹"""
    DESCRIPTIVE = "descriptive"           # æè¿°æ€§åˆ†æ
    DIAGNOSTIC = "diagnostic"             # è¯Šæ–­æ€§åˆ†æ
    PREDICTIVE = "predictive"             # é¢„æµ‹æ€§åˆ†æ
    PRESCRIPTIVE = "prescriptive"         # è§„èŒƒæ€§åˆ†æ
    EXPLORATORY = "exploratory"           # æ¢ç´¢æ€§åˆ†æ

class DataType(Enum):
    """æ•°æ®ç±»å‹"""
    NUMERICAL = "numerical"               # æ•°å€¼å‹
    CATEGORICAL = "categorical"           # åˆ†ç±»å‹
    TEMPORAL = "temporal"                 # æ—¶é—´åºåˆ—
    TEXT = "text"                        # æ–‡æœ¬å‹
    MIXED = "mixed"                      # æ··åˆå‹

class ChartType(Enum):
    """å›¾è¡¨ç±»å‹"""
    LINE = "line"                        # æŠ˜çº¿å›¾
    BAR = "bar"                          # æŸ±çŠ¶å›¾
    SCATTER = "scatter"                  # æ•£ç‚¹å›¾
    HISTOGRAM = "histogram"              # ç›´æ–¹å›¾
    BOX = "box"                          # ç®±çº¿å›¾
    HEATMAP = "heatmap"                  # çƒ­åŠ›å›¾
    PIE = "pie"                          # é¥¼å›¾
    VIOLIN = "violin"                    # å°æç´å›¾

class InsightLevel(Enum):
    """æ´å¯Ÿçº§åˆ«"""
    CRITICAL = "critical"                # å…³é”®
    IMPORTANT = "important"              # é‡è¦
    MODERATE = "moderate"                # ä¸­ç­‰
    MINOR = "minor"                      # æ¬¡è¦

@dataclass
class DataSource:
    """æ•°æ®æº"""
    id: str
    name: str
    description: str
    data_type: DataType
    file_path: Optional[str] = None
    connection_string: Optional[str] = None
    query: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    last_updated: Optional[datetime] = None

@dataclass
class AnalysisRequest:
    """åˆ†æè¯·æ±‚"""
    id: str
    query: str                           # è‡ªç„¶è¯­è¨€æŸ¥è¯¢
    data_source_id: str
    analysis_type: AnalysisType
    parameters: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    priority: int = 3                    # 1-5, 5æœ€é«˜

@dataclass
class StatisticalResult:
    """ç»Ÿè®¡ç»“æœ"""
    metric: str
    value: float
    confidence_interval: Optional[Tuple[float, float]] = None
    p_value: Optional[float] = None
    significance: Optional[bool] = None
    interpretation: str = ""

@dataclass
class Insight:
    """æ•°æ®æ´å¯Ÿ"""
    id: str
    title: str
    description: str
    level: InsightLevel
    confidence: float                    # 0-1
    supporting_data: Dict[str, Any]
    recommendations: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    id: str
    request: AnalysisRequest
    summary: str
    statistical_results: List[StatisticalResult]
    insights: List[Insight]
    visualizations: List[str] = field(default_factory=list)  # å›¾è¡¨æ–‡ä»¶è·¯å¾„
    raw_data: Optional[pd.DataFrame] = None
    execution_time: float = 0.0
    created_at: datetime = field(default_factory=datetime.now)

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.data_cache: Dict[str, pd.DataFrame] = {}
    
    def load_data(self, data_source: DataSource) -> pd.DataFrame:
        """åŠ è½½æ•°æ®"""
        try:
            if data_source.id in self.data_cache:
                return self.data_cache[data_source.id]
            
            if data_source.file_path:
                # ä»æ–‡ä»¶åŠ è½½
                if data_source.file_path.endswith('.csv'):
                    df = pd.read_csv(data_source.file_path)
                elif data_source.file_path.endswith('.xlsx'):
                    df = pd.read_excel(data_source.file_path)
                elif data_source.file_path.endswith('.json'):
                    df = pd.read_json(data_source.file_path)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_source.file_path}")
            
            elif data_source.connection_string and data_source.query:
                # ä»æ•°æ®åº“åŠ è½½
                conn = sqlite3.connect(data_source.connection_string)
                df = pd.read_sql_query(data_source.query, conn)
                conn.close()
            
            else:
                # ç”Ÿæˆç¤ºä¾‹æ•°æ®
                df = self._generate_sample_data(data_source.data_type)
            
            # ç¼“å­˜æ•°æ®
            self.data_cache[data_source.id] = df
            
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {data_source.name}, å½¢çŠ¶: {df.shape}")
            return df
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise e
    
    def _generate_sample_data(self, data_type: DataType) -> pd.DataFrame:
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
        np.random.seed(42)
        
        if data_type == DataType.NUMERICAL:
            # ç”Ÿæˆæ•°å€¼å‹æ•°æ®
            n = 1000
            data = {
                'id': range(1, n+1),
                'value1': np.random.normal(100, 15, n),
                'value2': np.random.exponential(50, n),
                'value3': np.random.uniform(0, 100, n),
                'category': np.random.choice(['A', 'B', 'C', 'D'], n),
                'date': pd.date_range('2023-01-01', periods=n, freq='D')
            }
            
        elif data_type == DataType.TEMPORAL:
            # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
            dates = pd.date_range('2023-01-01', periods=365, freq='D')
            trend = np.linspace(100, 200, len(dates))
            seasonal = 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)
            noise = np.random.normal(0, 5, len(dates))
            
            data = {
                'date': dates,
                'value': trend + seasonal + noise,
                'category': np.random.choice(['äº§å“A', 'äº§å“B', 'äº§å“C'], len(dates))
            }
            
        elif data_type == DataType.CATEGORICAL:
            # ç”Ÿæˆåˆ†ç±»æ•°æ®
            n = 500
            data = {
                'customer_id': range(1, n+1),
                'age_group': np.random.choice(['18-25', '26-35', '36-45', '46-55', '55+'], n),
                'gender': np.random.choice(['ç”·', 'å¥³'], n),
                'city': np.random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·'], n),
                'purchase_amount': np.random.lognormal(4, 1, n),
                'satisfaction': np.random.choice(['å¾ˆæ»¡æ„', 'æ»¡æ„', 'ä¸€èˆ¬', 'ä¸æ»¡æ„'], n, p=[0.3, 0.4, 0.2, 0.1])
            }
            
        else:
            # é»˜è®¤æ··åˆæ•°æ®
            n = 800
            data = {
                'id': range(1, n+1),
                'sales': np.random.lognormal(5, 0.5, n),
                'profit_margin': np.random.normal(0.15, 0.05, n),
                'region': np.random.choice(['ååŒ—', 'åä¸œ', 'åå—', 'è¥¿å—'], n),
                'product_type': np.random.choice(['ç”µå­äº§å“', 'æœè£…', 'é£Ÿå“', 'å®¶å…·'], n),
                'quarter': np.random.choice(['Q1', 'Q2', 'Q3', 'Q4'], n),
                'customer_rating': np.random.uniform(1, 5, n)
            }
        
        return pd.DataFrame(data)
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        # å¤„ç†ç¼ºå¤±å€¼
        for col in df.columns:
            if df[col].dtype in ['float64', 'int64']:
                # æ•°å€¼å‹ç”¨å‡å€¼å¡«å……
                df[col].fillna(df[col].mean(), inplace=True)
            else:
                # åˆ†ç±»å‹ç”¨ä¼—æ•°å¡«å……
                df[col].fillna(df[col].mode().iloc[0] if not df[col].mode().empty else 'Unknown', inplace=True)
        
        # å¤„ç†å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
        for col in df.select_dtypes(include=[np.number]).columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # å°†å¼‚å¸¸å€¼æ›¿æ¢ä¸ºè¾¹ç•Œå€¼
            df[col] = df[col].clip(lower_bound, upper_bound)
        
        return df
    
    def get_data_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦"""
        summary = {
            "shape": df.shape,
            "columns": list(df.columns),
            "dtypes": df.dtypes.to_dict(),
            "missing_values": df.isnull().sum().to_dict(),
            "memory_usage": df.memory_usage(deep=True).sum(),
            "numerical_summary": {},
            "categorical_summary": {}
        }
        
        # æ•°å€¼å‹åˆ—æ‘˜è¦
        numerical_cols = df.select_dtypes(include=[np.number]).columns
        if len(numerical_cols) > 0:
            summary["numerical_summary"] = df[numerical_cols].describe().to_dict()
        
        # åˆ†ç±»å‹åˆ—æ‘˜è¦
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            summary["categorical_summary"][col] = {
                "unique_count": df[col].nunique(),
                "top_values": df[col].value_counts().head().to_dict()
            }
        
        return summary

class StatisticalAnalyzer:
    """ç»Ÿè®¡åˆ†æå™¨"""
    
    def __init__(self):
        pass
    
    def descriptive_analysis(self, df: pd.DataFrame, columns: List[str] = None) -> List[StatisticalResult]:
        """æè¿°æ€§ç»Ÿè®¡åˆ†æ"""
        results = []
        
        if columns is None:
            columns = df.select_dtypes(include=[np.number]).columns
        
        for col in columns:
            if col in df.columns and df[col].dtype in [np.number]:
                data = df[col].dropna()
                
                # åŸºæœ¬ç»Ÿè®¡é‡
                results.extend([
                    StatisticalResult("å‡å€¼", data.mean(), interpretation=f"{col}çš„å¹³å‡å€¼"),
                    StatisticalResult("ä¸­ä½æ•°", data.median(), interpretation=f"{col}çš„ä¸­ä½æ•°"),
                    StatisticalResult("æ ‡å‡†å·®", data.std(), interpretation=f"{col}çš„æ ‡å‡†å·®"),
                    StatisticalResult("ååº¦", stats.skew(data), interpretation=f"{col}çš„ååº¦"),
                    StatisticalResult("å³°åº¦", stats.kurtosis(data), interpretation=f"{col}çš„å³°åº¦")
                ])
                
                # ç½®ä¿¡åŒºé—´
                confidence_interval = stats.t.interval(0.95, len(data)-1, 
                                                     loc=data.mean(), 
                                                     scale=stats.sem(data))
                results.append(
                    StatisticalResult("95%ç½®ä¿¡åŒºé—´", data.mean(), 
                                    confidence_interval=confidence_interval,
                                    interpretation=f"{col}å‡å€¼çš„95%ç½®ä¿¡åŒºé—´")
                )
        
        return results
    
    def correlation_analysis(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[StatisticalResult]]:
        """ç›¸å…³æ€§åˆ†æ"""
        numerical_cols = df.select_dtypes(include=[np.number]).columns
        
        if len(numerical_cols) < 2:
            return pd.DataFrame(), []
        
        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        corr_matrix = df[numerical_cols].corr()
        
        # æ‰¾å‡ºå¼ºç›¸å…³å…³ç³»
        results = []
        for i in range(len(numerical_cols)):
            for j in range(i+1, len(numerical_cols)):
                col1, col2 = numerical_cols[i], numerical_cols[j]
                corr_value = corr_matrix.loc[col1, col2]
                
                # è®¡ç®—æ˜¾è‘—æ€§
                n = len(df[[col1, col2]].dropna())
                if n > 2:
                    t_stat = corr_value * np.sqrt((n-2) / (1-corr_value**2))
                    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n-2))
                    
                    interpretation = f"{col1}ä¸{col2}çš„ç›¸å…³ç³»æ•°"
                    if abs(corr_value) > 0.7:
                        interpretation += " (å¼ºç›¸å…³)"
                    elif abs(corr_value) > 0.3:
                        interpretation += " (ä¸­ç­‰ç›¸å…³)"
                    else:
                        interpretation += " (å¼±ç›¸å…³)"
                    
                    results.append(
                        StatisticalResult(
                            f"{col1} vs {col2}",
                            corr_value,
                            p_value=p_value,
                            significance=p_value < 0.05,
                            interpretation=interpretation
                        )
                    )
        
        return corr_matrix, results
    
    def hypothesis_testing(self, df: pd.DataFrame, group_col: str, value_col: str) -> List[StatisticalResult]:
        """å‡è®¾æ£€éªŒ"""
        results = []
        
        if group_col not in df.columns or value_col not in df.columns:
            return results
        
        groups = df.groupby(group_col)[value_col].apply(list)
        group_names = list(groups.index)
        
        if len(group_names) == 2:
            # ä¸¤ç»„tæ£€éªŒ
            group1, group2 = groups.iloc[0], groups.iloc[1]
            t_stat, p_value = stats.ttest_ind(group1, group2)
            
            results.append(
                StatisticalResult(
                    "ä¸¤æ ·æœ¬tæ£€éªŒ",
                    t_stat,
                    p_value=p_value,
                    significance=p_value < 0.05,
                    interpretation=f"{group_names[0]}ä¸{group_names[1]}åœ¨{value_col}ä¸Šçš„å·®å¼‚æ£€éªŒ"
                )
            )
            
        elif len(group_names) > 2:
            # æ–¹å·®åˆ†æ
            group_data = [groups.iloc[i] for i in range(len(group_names))]
            f_stat, p_value = stats.f_oneway(*group_data)
            
            results.append(
                StatisticalResult(
                    "å•å› ç´ æ–¹å·®åˆ†æ",
                    f_stat,
                    p_value=p_value,
                    significance=p_value < 0.05,
                    interpretation=f"å„ç»„åœ¨{value_col}ä¸Šçš„å·®å¼‚æ£€éªŒ"
                )
            )
        
        return results
    
    def anomaly_detection(self, df: pd.DataFrame, columns: List[str] = None) -> Tuple[pd.DataFrame, List[StatisticalResult]]:
        """å¼‚å¸¸æ£€æµ‹"""
        if columns is None:
            columns = df.select_dtypes(include=[np.number]).columns
        
        # ä½¿ç”¨Isolation Forestè¿›è¡Œå¼‚å¸¸æ£€æµ‹
        data = df[columns].dropna()
        
        if len(data) < 10:
            return pd.DataFrame(), []
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data)
        
        # å¼‚å¸¸æ£€æµ‹
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_labels = iso_forest.fit_predict(data_scaled)
        
        # æ·»åŠ å¼‚å¸¸æ ‡ç­¾
        anomaly_df = data.copy()
        anomaly_df['is_anomaly'] = anomaly_labels == -1
        
        # ç»Ÿè®¡ç»“æœ
        anomaly_count = sum(anomaly_labels == -1)
        anomaly_rate = anomaly_count / len(data)
        
        results = [
            StatisticalResult(
                "å¼‚å¸¸ç‚¹æ•°é‡",
                anomaly_count,
                interpretation=f"æ£€æµ‹åˆ°{anomaly_count}ä¸ªå¼‚å¸¸ç‚¹"
            ),
            StatisticalResult(
                "å¼‚å¸¸ç‡",
                anomaly_rate,
                interpretation=f"å¼‚å¸¸ç‡ä¸º{anomaly_rate:.2%}"
            )
        ]
        
        return anomaly_df, results

class VisualizationGenerator:
    """å¯è§†åŒ–ç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "visualizations"):
        self.output_dir = output_dir
        import os
        os.makedirs(output_dir, exist_ok=True)
    
    def generate_chart(self, df: pd.DataFrame, chart_type: ChartType, 
                      x_col: str = None, y_col: str = None, 
                      group_col: str = None, title: str = None) -> str:
        """ç”Ÿæˆå›¾è¡¨"""
        plt.figure(figsize=(10, 6))
        
        if chart_type == ChartType.LINE:
            if x_col and y_col:
                if group_col:
                    for group in df[group_col].unique():
                        group_data = df[df[group_col] == group]
                        plt.plot(group_data[x_col], group_data[y_col], label=group, marker='o')
                    plt.legend()
                else:
                    plt.plot(df[x_col], df[y_col], marker='o')
                plt.xlabel(x_col)
                plt.ylabel(y_col)
        
        elif chart_type == ChartType.BAR:
            if x_col and y_col:
                if group_col:
                    df_pivot = df.pivot_table(values=y_col, index=x_col, columns=group_col, aggfunc='mean')
                    df_pivot.plot(kind='bar', ax=plt.gca())
                else:
                    df.groupby(x_col)[y_col].mean().plot(kind='bar')
                plt.xlabel(x_col)
                plt.ylabel(y_col)
                plt.xticks(rotation=45)
        
        elif chart_type == ChartType.SCATTER:
            if x_col and y_col:
                if group_col:
                    for group in df[group_col].unique():
                        group_data = df[df[group_col] == group]
                        plt.scatter(group_data[x_col], group_data[y_col], label=group, alpha=0.6)
                    plt.legend()
                else:
                    plt.scatter(df[x_col], df[y_col], alpha=0.6)
                plt.xlabel(x_col)
                plt.ylabel(y_col)
        
        elif chart_type == ChartType.HISTOGRAM:
            if x_col:
                if group_col:
                    for group in df[group_col].unique():
                        group_data = df[df[group_col] == group]
                        plt.hist(group_data[x_col], alpha=0.6, label=group, bins=20)
                    plt.legend()
                else:
                    plt.hist(df[x_col], bins=20, alpha=0.7)
                plt.xlabel(x_col)
                plt.ylabel('é¢‘æ¬¡')
        
        elif chart_type == ChartType.BOX:
            if y_col:
                if group_col:
                    df.boxplot(column=y_col, by=group_col, ax=plt.gca())
                else:
                    df.boxplot(column=y_col, ax=plt.gca())
        
        elif chart_type == ChartType.HEATMAP:
            numerical_cols = df.select_dtypes(include=[np.number]).columns
            if len(numerical_cols) > 1:
                corr_matrix = df[numerical_cols].corr()
                sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
        
        elif chart_type == ChartType.PIE:
            if x_col:
                value_counts = df[x_col].value_counts()
                plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')
        
        if title:
            plt.title(title)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        filename = f"{chart_type.value}_{int(time.time())}.png"
        filepath = f"{self.output_dir}/{filename}"
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        return filepath
    
    def generate_dashboard(self, df: pd.DataFrame, title: str = "æ•°æ®åˆ†æä»ªè¡¨æ¿") -> str:
        """ç”Ÿæˆç»¼åˆä»ªè¡¨æ¿"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(title, fontsize=16)
        
        numerical_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=['object']).columns
        
        # 1. æ•°å€¼å‹å˜é‡åˆ†å¸ƒ
        if len(numerical_cols) > 0:
            col = numerical_cols[0]
            axes[0, 0].hist(df[col].dropna(), bins=20, alpha=0.7)
            axes[0, 0].set_title(f'{col} åˆ†å¸ƒ')
            axes[0, 0].set_xlabel(col)
            axes[0, 0].set_ylabel('é¢‘æ¬¡')
        
        # 2. ç›¸å…³æ€§çƒ­åŠ›å›¾
        if len(numerical_cols) > 1:
            corr_matrix = df[numerical_cols].corr()
            im = axes[0, 1].imshow(corr_matrix, cmap='coolwarm', aspect='auto')
            axes[0, 1].set_title('ç›¸å…³æ€§çŸ©é˜µ')
            axes[0, 1].set_xticks(range(len(numerical_cols)))
            axes[0, 1].set_yticks(range(len(numerical_cols)))
            axes[0, 1].set_xticklabels(numerical_cols, rotation=45)
            axes[0, 1].set_yticklabels(numerical_cols)
            plt.colorbar(im, ax=axes[0, 1])
        
        # 3. åˆ†ç±»å˜é‡åˆ†å¸ƒ
        if len(categorical_cols) > 0:
            col = categorical_cols[0]
            value_counts = df[col].value_counts().head(10)
            axes[1, 0].bar(range(len(value_counts)), value_counts.values)
            axes[1, 0].set_title(f'{col} åˆ†å¸ƒ')
            axes[1, 0].set_xlabel(col)
            axes[1, 0].set_ylabel('è®¡æ•°')
            axes[1, 0].set_xticks(range(len(value_counts)))
            axes[1, 0].set_xticklabels(value_counts.index, rotation=45)
        
        # 4. æ—¶é—´åºåˆ—ï¼ˆå¦‚æœæœ‰æ—¥æœŸåˆ—ï¼‰
        date_cols = df.select_dtypes(include=['datetime64']).columns
        if len(date_cols) > 0 and len(numerical_cols) > 0:
            date_col = date_cols[0]
            value_col = numerical_cols[0]
            df_sorted = df.sort_values(date_col)
            axes[1, 1].plot(df_sorted[date_col], df_sorted[value_col])
            axes[1, 1].set_title(f'{value_col} æ—¶é—´è¶‹åŠ¿')
            axes[1, 1].set_xlabel(date_col)
            axes[1, 1].set_ylabel(value_col)
            axes[1, 1].tick_params(axis='x', rotation=45)
        else:
            # ç®±çº¿å›¾
            if len(numerical_cols) > 0:
                col = numerical_cols[0]
                axes[1, 1].boxplot(df[col].dropna())
                axes[1, 1].set_title(f'{col} ç®±çº¿å›¾')
                axes[1, 1].set_ylabel(col)
        
        plt.tight_layout()
        
        # ä¿å­˜ä»ªè¡¨æ¿
        filename = f"dashboard_{int(time.time())}.png"
        filepath = f"{self.output_dir}/{filename}"
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        return filepath

class InsightGenerator:
    """æ´å¯Ÿç”Ÿæˆå™¨"""
    
    def __init__(self, client: HarborAI):
        self.client = client
    
    async def generate_insights(self, df: pd.DataFrame, 
                              statistical_results: List[StatisticalResult],
                              analysis_context: str = "") -> List[Insight]:
        """ç”Ÿæˆæ•°æ®æ´å¯Ÿ"""
        try:
            # å‡†å¤‡æ•°æ®æ‘˜è¦
            data_summary = self._prepare_data_summary(df)
            stats_summary = self._prepare_stats_summary(statistical_results)
            
            insight_prompt = f"""
ä½œä¸ºæ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æ•°æ®å’Œç»Ÿè®¡åˆ†æç»“æœï¼Œç”Ÿæˆæ·±å…¥çš„ä¸šåŠ¡æ´å¯Ÿï¼š

åˆ†æèƒŒæ™¯: {analysis_context}

æ•°æ®æ‘˜è¦:
{data_summary}

ç»Ÿè®¡åˆ†æç»“æœ:
{stats_summary}

è¯·ç”Ÿæˆ3-5ä¸ªå…³é”®æ´å¯Ÿï¼Œæ¯ä¸ªæ´å¯ŸåŒ…å«ï¼š
1. æ´å¯Ÿæ ‡é¢˜ï¼ˆç®€æ´æ˜äº†ï¼‰
2. è¯¦ç»†æè¿°ï¼ˆåŒ…å«å…·ä½“æ•°æ®æ”¯æ’‘ï¼‰
3. é‡è¦æ€§çº§åˆ«ï¼ˆcritical/important/moderate/minorï¼‰
4. ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
5. ä¸šåŠ¡å»ºè®®ï¼ˆå…·ä½“å¯æ‰§è¡Œçš„å»ºè®®ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "insights": [
        {{
            "title": "æ´å¯Ÿæ ‡é¢˜",
            "description": "è¯¦ç»†æè¿°",
            "level": "important",
            "confidence": 0.85,
            "recommendations": ["å»ºè®®1", "å»ºè®®2"],
            "supporting_data": {{"key": "value"}}
        }}
    ]
}}
"""
            
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ä»æ•°æ®ä¸­å‘ç°æœ‰ä»·å€¼çš„ä¸šåŠ¡æ´å¯Ÿã€‚"},
                    {"role": "user", "content": insight_prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            result_text = response.choices[0].message.content
            
            # è§£æJSONç»“æœ
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                result_data = json.loads(json_match.group())
                insights = []
                
                for insight_data in result_data.get("insights", []):
                    insight = Insight(
                        id=str(uuid.uuid4()),
                        title=insight_data.get("title", ""),
                        description=insight_data.get("description", ""),
                        level=InsightLevel(insight_data.get("level", "moderate")),
                        confidence=insight_data.get("confidence", 0.5),
                        supporting_data=insight_data.get("supporting_data", {}),
                        recommendations=insight_data.get("recommendations", [])
                    )
                    insights.append(insight)
                
                return insights
            
        except Exception as e:
            logger.error(f"æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
        
        # è¿”å›é»˜è®¤æ´å¯Ÿ
        return [
            Insight(
                id=str(uuid.uuid4()),
                title="æ•°æ®æ¦‚è§ˆ",
                description=f"æ•°æ®é›†åŒ…å«{df.shape[0]}è¡Œ{df.shape[1]}åˆ—ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æä»¥å‘ç°æ¨¡å¼ã€‚",
                level=InsightLevel.MODERATE,
                confidence=0.7,
                supporting_data={"rows": df.shape[0], "columns": df.shape[1]},
                recommendations=["è¿›è¡Œæ›´æ·±å…¥çš„æ¢ç´¢æ€§æ•°æ®åˆ†æ", "æ£€æŸ¥æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§"]
            )
        ]
    
    def _prepare_data_summary(self, df: pd.DataFrame) -> str:
        """å‡†å¤‡æ•°æ®æ‘˜è¦"""
        summary_parts = [
            f"æ•°æ®å½¢çŠ¶: {df.shape[0]}è¡Œ x {df.shape[1]}åˆ—",
            f"æ•°å€¼å‹åˆ—: {len(df.select_dtypes(include=[np.number]).columns)}ä¸ª",
            f"åˆ†ç±»å‹åˆ—: {len(df.select_dtypes(include=['object']).columns)}ä¸ª"
        ]
        
        # ç¼ºå¤±å€¼æƒ…å†µ
        missing_info = df.isnull().sum()
        if missing_info.sum() > 0:
            summary_parts.append(f"ç¼ºå¤±å€¼: {missing_info.sum()}ä¸ª")
        
        # æ•°å€¼å‹åˆ—çš„åŸºæœ¬ç»Ÿè®¡
        numerical_cols = df.select_dtypes(include=[np.number]).columns
        if len(numerical_cols) > 0:
            for col in numerical_cols[:3]:  # åªæ˜¾ç¤ºå‰3åˆ—
                mean_val = df[col].mean()
                std_val = df[col].std()
                summary_parts.append(f"{col}: å‡å€¼={mean_val:.2f}, æ ‡å‡†å·®={std_val:.2f}")
        
        return "\n".join(summary_parts)
    
    def _prepare_stats_summary(self, statistical_results: List[StatisticalResult]) -> str:
        """å‡†å¤‡ç»Ÿè®¡æ‘˜è¦"""
        if not statistical_results:
            return "æš‚æ— ç»Ÿè®¡åˆ†æç»“æœ"
        
        summary_parts = []
        for result in statistical_results[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªç»“æœ
            summary_parts.append(f"{result.metric}: {result.value:.4f}")
            if result.p_value is not None:
                summary_parts.append(f"  på€¼: {result.p_value:.4f}")
            if result.interpretation:
                summary_parts.append(f"  è§£é‡Š: {result.interpretation}")
        
        return "\n".join(summary_parts)

class QueryProcessor:
    """æŸ¥è¯¢å¤„ç†å™¨"""
    
    def __init__(self, client: HarborAI):
        self.client = client
    
    async def parse_query(self, query: str, data_summary: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æè‡ªç„¶è¯­è¨€æŸ¥è¯¢"""
        try:
            parse_prompt = f"""
è¯·è§£æä»¥ä¸‹è‡ªç„¶è¯­è¨€æ•°æ®åˆ†ææŸ¥è¯¢ï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„åˆ†æå‚æ•°ï¼š

ç”¨æˆ·æŸ¥è¯¢: {query}

å¯ç”¨æ•°æ®åˆ—: {', '.join(data_summary.get('columns', []))}
æ•°æ®ç±»å‹: {data_summary.get('dtypes', {})}

è¯·åˆ†æç”¨æˆ·æƒ³è¦è¿›è¡Œä»€ä¹ˆç±»å‹çš„åˆ†æï¼Œå¹¶è¿”å›JSONæ ¼å¼çš„å‚æ•°ï¼š
{{
    "analysis_type": "descriptive/diagnostic/predictive/exploratory",
    "target_columns": ["åˆ—å1", "åˆ—å2"],
    "group_by_column": "åˆ†ç»„åˆ—åæˆ–null",
    "filter_conditions": {{"åˆ—å": "æ¡ä»¶"}},
    "chart_type": "line/bar/scatter/histogram/box/heatmap/pie",
    "specific_operations": ["æ“ä½œ1", "æ“ä½œ2"],
    "interpretation": "æŸ¥è¯¢æ„å›¾çš„è§£é‡Š"
}}
"""
            
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†ææŸ¥è¯¢è§£æä¸“å®¶ã€‚"},
                    {"role": "user", "content": parse_prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )
            
            result_text = response.choices[0].message.content
            
            # è§£æJSONç»“æœ
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢è§£æå¤±è´¥: {str(e)}")
        
        # è¿”å›é»˜è®¤è§£æç»“æœ
        return {
            "analysis_type": "exploratory",
            "target_columns": [],
            "group_by_column": None,
            "filter_conditions": {},
            "chart_type": "histogram",
            "specific_operations": ["descriptive_statistics"],
            "interpretation": "è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æ"
        }

class DataAnalysisAssistant:
    """æ•°æ®åˆ†æåŠ©æ‰‹"""
    
    def __init__(self, 
                 api_key: str,
                 base_url: str = "https://api.deepseek.com/v1",
                 db_path: str = "analysis_assistant.db"):
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.client = HarborAI(api_key=api_key, base_url=base_url)
        self.data_processor = DataProcessor()
        self.statistical_analyzer = StatisticalAnalyzer()
        self.visualization_generator = VisualizationGenerator()
        self.insight_generator = InsightGenerator(self.client)
        self.query_processor = QueryProcessor(self.client)
        
        # æ•°æ®æºç®¡ç†
        self.data_sources: Dict[str, DataSource] = {}
        
        # åˆ†æå†å²
        self.analysis_history: List[AnalysisResult] = []
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºåˆ†æç»“æœè¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS analysis_results (
                id TEXT PRIMARY KEY,
                query TEXT NOT NULL,
                data_source_id TEXT NOT NULL,
                analysis_type TEXT NOT NULL,
                summary TEXT,
                execution_time REAL,
                created_at TIMESTAMP NOT NULL,
                result_data TEXT
            )
        """)
        
        # åˆ›å»ºæ´å¯Ÿè¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS insights (
                id TEXT PRIMARY KEY,
                analysis_id TEXT NOT NULL,
                title TEXT NOT NULL,
                description TEXT NOT NULL,
                level TEXT NOT NULL,
                confidence REAL NOT NULL,
                created_at TIMESTAMP NOT NULL,
                FOREIGN KEY (analysis_id) REFERENCES analysis_results (id)
            )
        """)
        
        conn.commit()
        conn.close()
    
    def register_data_source(self, data_source: DataSource):
        """æ³¨å†Œæ•°æ®æº"""
        self.data_sources[data_source.id] = data_source
        logger.info(f"æ•°æ®æºå·²æ³¨å†Œ: {data_source.name}")
    
    async def analyze(self, query: str, data_source_id: str) -> AnalysisResult:
        """æ‰§è¡Œæ•°æ®åˆ†æ"""
        start_time = time.time()
        
        try:
            # è·å–æ•°æ®æº
            if data_source_id not in self.data_sources:
                raise ValueError(f"æ•°æ®æºä¸å­˜åœ¨: {data_source_id}")
            
            data_source = self.data_sources[data_source_id]
            
            # åŠ è½½å’Œæ¸…æ´—æ•°æ®
            df = self.data_processor.load_data(data_source)
            df_clean = self.data_processor.clean_data(df)
            
            # è·å–æ•°æ®æ‘˜è¦
            data_summary = self.data_processor.get_data_summary(df_clean)
            
            # è§£ææŸ¥è¯¢
            query_params = await self.query_processor.parse_query(query, data_summary)
            
            # åˆ›å»ºåˆ†æè¯·æ±‚
            request = AnalysisRequest(
                id=str(uuid.uuid4()),
                query=query,
                data_source_id=data_source_id,
                analysis_type=AnalysisType(query_params.get("analysis_type", "exploratory")),
                parameters=query_params
            )
            
            # æ‰§è¡Œç»Ÿè®¡åˆ†æ
            statistical_results = []
            
            # æè¿°æ€§ç»Ÿè®¡
            if query_params.get("target_columns"):
                desc_results = self.statistical_analyzer.descriptive_analysis(
                    df_clean, query_params["target_columns"]
                )
                statistical_results.extend(desc_results)
            
            # ç›¸å…³æ€§åˆ†æ
            corr_matrix, corr_results = self.statistical_analyzer.correlation_analysis(df_clean)
            statistical_results.extend(corr_results)
            
            # å‡è®¾æ£€éªŒï¼ˆå¦‚æœæœ‰åˆ†ç»„åˆ—ï¼‰
            if query_params.get("group_by_column") and query_params.get("target_columns"):
                group_col = query_params["group_by_column"]
                value_col = query_params["target_columns"][0]
                if group_col in df_clean.columns and value_col in df_clean.columns:
                    hyp_results = self.statistical_analyzer.hypothesis_testing(
                        df_clean, group_col, value_col
                    )
                    statistical_results.extend(hyp_results)
            
            # å¼‚å¸¸æ£€æµ‹
            anomaly_df, anomaly_results = self.statistical_analyzer.anomaly_detection(df_clean)
            statistical_results.extend(anomaly_results)
            
            # ç”Ÿæˆå¯è§†åŒ–
            visualizations = []
            chart_type = ChartType(query_params.get("chart_type", "histogram"))
            
            if query_params.get("target_columns"):
                target_cols = query_params["target_columns"]
                group_col = query_params.get("group_by_column")
                
                if len(target_cols) >= 1:
                    chart_path = self.visualization_generator.generate_chart(
                        df_clean,
                        chart_type,
                        x_col=target_cols[0] if len(target_cols) > 0 else None,
                        y_col=target_cols[1] if len(target_cols) > 1 else target_cols[0],
                        group_col=group_col,
                        title=f"{query} - {chart_type.value}å›¾"
                    )
                    visualizations.append(chart_path)
            
            # ç”Ÿæˆç»¼åˆä»ªè¡¨æ¿
            dashboard_path = self.visualization_generator.generate_dashboard(
                df_clean, f"æ•°æ®åˆ†æä»ªè¡¨æ¿ - {query}"
            )
            visualizations.append(dashboard_path)
            
            # ç”Ÿæˆæ´å¯Ÿ
            insights = await self.insight_generator.generate_insights(
                df_clean, statistical_results, query
            )
            
            # ç”Ÿæˆåˆ†ææ‘˜è¦
            summary = await self._generate_analysis_summary(
                query, statistical_results, insights
            )
            
            # åˆ›å»ºåˆ†æç»“æœ
            execution_time = time.time() - start_time
            result = AnalysisResult(
                id=str(uuid.uuid4()),
                request=request,
                summary=summary,
                statistical_results=statistical_results,
                insights=insights,
                visualizations=visualizations,
                raw_data=df_clean,
                execution_time=execution_time
            )
            
            # ä¿å­˜ç»“æœ
            self._save_analysis_result(result)
            self.analysis_history.append(result)
            
            logger.info(f"åˆ†æå®Œæˆ: {result.id} (è€—æ—¶: {execution_time:.3f}s)")
            
            return result
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
            raise e
    
    async def _generate_analysis_summary(self, 
                                       query: str,
                                       statistical_results: List[StatisticalResult],
                                       insights: List[Insight]) -> str:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        try:
            # å‡†å¤‡ç»Ÿè®¡ç»“æœæ‘˜è¦
            stats_summary = []
            for result in statistical_results[:5]:  # åªå–å‰5ä¸ªç»“æœ
                stats_summary.append(f"- {result.metric}: {result.value:.4f}")
                if result.interpretation:
                    stats_summary.append(f"  {result.interpretation}")
            
            # å‡†å¤‡æ´å¯Ÿæ‘˜è¦
            insights_summary = []
            for insight in insights[:3]:  # åªå–å‰3ä¸ªæ´å¯Ÿ
                insights_summary.append(f"- {insight.title}: {insight.description[:100]}...")
            
            summary_prompt = f"""
è¯·åŸºäºä»¥ä¸‹åˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´æ˜äº†çš„åˆ†ææ‘˜è¦ï¼š

ç”¨æˆ·æŸ¥è¯¢: {query}

ç»Ÿè®¡åˆ†æç»“æœ:
{chr(10).join(stats_summary)}

å…³é”®æ´å¯Ÿ:
{chr(10).join(insights_summary)}

è¯·ç”Ÿæˆä¸€ä¸ª200å­—ä»¥å†…çš„åˆ†ææ‘˜è¦ï¼ŒåŒ…å«ï¼š
1. æ•°æ®çš„ä¸»è¦ç‰¹å¾
2. å…³é”®å‘ç°
3. é‡è¦ç»“è®º
4. å»ºè®®çš„åç»­è¡ŒåŠ¨

æ‘˜è¦åº”è¯¥ç®€æ´ã€ä¸“ä¸šä¸”æ˜“äºç†è§£ã€‚
"""
            
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿æ€»ç»“åˆ†æç»“æœã€‚"},
                    {"role": "user", "content": summary_prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"é’ˆå¯¹æŸ¥è¯¢'{query}'çš„æ•°æ®åˆ†æå·²å®Œæˆï¼Œå‘ç°äº†{len(statistical_results)}ä¸ªç»Ÿè®¡ç»“æœå’Œ{len(insights)}ä¸ªå…³é”®æ´å¯Ÿã€‚"
    
    def _save_analysis_result(self, result: AnalysisResult):
        """ä¿å­˜åˆ†æç»“æœ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ä¿å­˜åˆ†æç»“æœ
        cursor.execute("""
            INSERT INTO analysis_results 
            (id, query, data_source_id, analysis_type, summary, execution_time, created_at, result_data)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            result.id,
            result.request.query,
            result.request.data_source_id,
            result.request.analysis_type.value,
            result.summary,
            result.execution_time,
            result.created_at,
            json.dumps({
                "statistical_results": [asdict(sr) for sr in result.statistical_results],
                "visualizations": result.visualizations
            })
        ))
        
        # ä¿å­˜æ´å¯Ÿ
        for insight in result.insights:
            cursor.execute("""
                INSERT INTO insights 
                (id, analysis_id, title, description, level, confidence, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                insight.id,
                result.id,
                insight.title,
                insight.description,
                insight.level.value,
                insight.confidence,
                insight.created_at
            ))
        
        conn.commit()
        conn.close()
    
    def get_analysis_history(self, limit: int = 10) -> List[AnalysisResult]:
        """è·å–åˆ†æå†å²"""
        return self.analysis_history[-limit:]
    
    def get_data_sources(self) -> List[DataSource]:
        """è·å–æ•°æ®æºåˆ—è¡¨"""
        return list(self.data_sources.values())

# æ¼”ç¤ºå‡½æ•°
async def demo_basic_analysis():
    """æ¼”ç¤ºåŸºç¡€æ•°æ®åˆ†æ"""
    print("\nğŸ“Š åŸºç¡€æ•°æ®åˆ†ææ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®åˆ†æåŠ©æ‰‹
    assistant = DataAnalysisAssistant(api_key="your-deepseek-key")
    
    # æ³¨å†Œæ•°æ®æº
    data_source = DataSource(
        id="sales_data",
        name="é”€å”®æ•°æ®",
        description="å…¬å¸é”€å”®ä¸šç»©æ•°æ®",
        data_type=DataType.MIXED
    )
    assistant.register_data_source(data_source)
    
    print(f"ğŸ“‹ æ•°æ®æºå·²æ³¨å†Œ: {data_source.name}")
    
    # æ‰§è¡Œåˆ†æ
    query = "åˆ†æé”€å”®æ•°æ®çš„åˆ†å¸ƒæƒ…å†µå’Œå„åœ°åŒºçš„ä¸šç»©å·®å¼‚"
    
    print(f"ğŸ” åˆ†ææŸ¥è¯¢: {query}")
    print(f"ğŸ”„ æ­£åœ¨æ‰§è¡Œåˆ†æ...")
    
    start_time = time.time()
    result = await assistant.analyze(query, data_source.id)
    analysis_time = time.time() - start_time
    
    print(f"âœ… åˆ†æå®Œæˆ!")
    print(f"   åˆ†æID: {result.id}")
    print(f"   æ‰§è¡Œæ—¶é—´: {analysis_time:.3f}s")
    print(f"   ç»Ÿè®¡ç»“æœæ•°: {len(result.statistical_results)}")
    print(f"   æ´å¯Ÿæ•°: {len(result.insights)}")
    print(f"   å¯è§†åŒ–å›¾è¡¨æ•°: {len(result.visualizations)}")
    
    # æ˜¾ç¤ºåˆ†ææ‘˜è¦
    print(f"\nğŸ“„ åˆ†ææ‘˜è¦:")
    print(f"   {result.summary}")
    
    # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“ˆ å…³é”®ç»Ÿè®¡ç»“æœ:")
    for i, stat in enumerate(result.statistical_results[:5]):
        print(f"   {i+1}. {stat.metric}: {stat.value:.4f}")
        if stat.interpretation:
            print(f"      {stat.interpretation}")
    
    # æ˜¾ç¤ºæ´å¯Ÿ
    print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
    for i, insight in enumerate(result.insights):
        print(f"   {i+1}. {insight.title} ({insight.level.value})")
        print(f"      {insight.description[:100]}...")
        if insight.recommendations:
            print(f"      å»ºè®®: {', '.join(insight.recommendations[:2])}")
    
    # æ˜¾ç¤ºå¯è§†åŒ–æ–‡ä»¶
    print(f"\nğŸ“Š ç”Ÿæˆçš„å¯è§†åŒ–:")
    for i, viz_path in enumerate(result.visualizations):
        print(f"   {i+1}. {viz_path}")
    
    return assistant, result

async def demo_natural_language_query():
    """æ¼”ç¤ºè‡ªç„¶è¯­è¨€æŸ¥è¯¢"""
    print("\nğŸ—£ï¸ è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ¼”ç¤º")
    print("=" * 50)
    
    assistant = DataAnalysisAssistant(api_key="your-deepseek-key")
    
    # æ³¨å†Œæ—¶é—´åºåˆ—æ•°æ®æº
    data_source = DataSource(
        id="time_series_data",
        name="æ—¶é—´åºåˆ—æ•°æ®",
        description="äº§å“é”€é‡æ—¶é—´åºåˆ—æ•°æ®",
        data_type=DataType.TEMPORAL
    )
    assistant.register_data_source(data_source)
    
    # å¤šä¸ªè‡ªç„¶è¯­è¨€æŸ¥è¯¢
    queries = [
        "æ˜¾ç¤ºé”€é‡çš„æ—¶é—´è¶‹åŠ¿ï¼Œå¹¶æ‰¾å‡ºå­£èŠ‚æ€§æ¨¡å¼",
        "æ¯”è¾ƒä¸åŒäº§å“ç±»åˆ«çš„é”€å”®è¡¨ç°",
        "æ£€æµ‹é”€é‡æ•°æ®ä¸­çš„å¼‚å¸¸å€¼",
        "åˆ†æé”€é‡ä¸å…¶ä»–å˜é‡çš„ç›¸å…³æ€§"
    ]
    
    results = []
    
    for i, query in enumerate(queries):
        print(f"\nğŸ” æŸ¥è¯¢ {i+1}: {query}")
        print(f"ğŸ”„ æ­£åœ¨åˆ†æ...")
        
        result = await assistant.analyze(query, data_source.id)
        results.append(result)
        
        print(f"âœ… åˆ†æå®Œæˆ (è€—æ—¶: {result.execution_time:.3f}s)")
        print(f"   æ‘˜è¦: {result.summary[:100]}...")
        
        # æ˜¾ç¤ºæœ€é‡è¦çš„æ´å¯Ÿ
        if result.insights:
            top_insight = max(result.insights, key=lambda x: x.confidence)
            print(f"   å…³é”®æ´å¯Ÿ: {top_insight.title}")
    
    # æ˜¾ç¤ºåˆ†æå†å²
    print(f"\nğŸ“š åˆ†æå†å²:")
    history = assistant.get_analysis_history()
    for i, result in enumerate(history):
        print(f"   {i+1}. {result.request.query[:50]}... (è€—æ—¶: {result.execution_time:.3f}s)")
    
    return results

async def demo_advanced_analytics():
    """æ¼”ç¤ºé«˜çº§åˆ†æåŠŸèƒ½"""
    print("\nğŸ”¬ é«˜çº§åˆ†æåŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    assistant = DataAnalysisAssistant(api_key="your-deepseek-key")
    
    # æ³¨å†Œå¤æ‚æ•°æ®æº
    data_source = DataSource(
        id="complex_data",
        name="å¤æ‚ä¸šåŠ¡æ•°æ®",
        description="åŒ…å«å¤šç»´åº¦çš„å¤æ‚ä¸šåŠ¡æ•°æ®",
        data_type=DataType.MIXED
    )
    assistant.register_data_source(data_source)
    
    # é«˜çº§åˆ†ææŸ¥è¯¢
    advanced_queries = [
        "è¿›è¡Œå®¢æˆ·ç»†åˆ†åˆ†æï¼Œè¯†åˆ«ä¸åŒçš„å®¢æˆ·ç¾¤ä½“",
        "æ‰§è¡ŒA/Bæµ‹è¯•åˆ†æï¼Œæ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬çš„æ•ˆæœå·®å¼‚",
        "é¢„æµ‹æœªæ¥3ä¸ªæœˆçš„é”€å”®è¶‹åŠ¿",
        "è¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼Œæ‰¾å‡ºå¯èƒ½çš„æ•°æ®è´¨é‡é—®é¢˜"
    ]
    
    print(f"ğŸ“‹ é«˜çº§åˆ†æä»»åŠ¡:")
    for i, query in enumerate(advanced_queries):
        print(f"   {i+1}. {query}")
    
    # æ‰¹é‡æ‰§è¡Œåˆ†æ
    print(f"\nğŸ”„ æ­£åœ¨æ‰§è¡Œæ‰¹é‡åˆ†æ...")
    start_time = time.time()
    
    tasks = [assistant.analyze(query, data_source.id) for query in advanced_queries]
    results = await asyncio.gather(*tasks)
    
    total_time = time.time() - start_time
    
    print(f"âœ… æ‰¹é‡åˆ†æå®Œæˆ!")
    print(f"   æ€»è€—æ—¶: {total_time:.3f}s")
    print(f"   å¹³å‡è€—æ—¶: {total_time/len(results):.3f}s/ä¸ª")
    
    # åˆ†æç»“æœæ±‡æ€»
    total_insights = sum(len(result.insights) for result in results)
    total_stats = sum(len(result.statistical_results) for result in results)
    total_visualizations = sum(len(result.visualizations) for result in results)
    
    print(f"\nğŸ“Š åˆ†æç»“æœæ±‡æ€»:")
    print(f"   æ€»æ´å¯Ÿæ•°: {total_insights}")
    print(f"   æ€»ç»Ÿè®¡ç»“æœæ•°: {total_stats}")
    print(f"   æ€»å¯è§†åŒ–æ•°: {total_visualizations}")
    
    # æ˜¾ç¤ºæ¯ä¸ªåˆ†æçš„å…³é”®å‘ç°
    print(f"\nğŸ” å…³é”®å‘ç°:")
    for i, result in enumerate(results):
        print(f"   åˆ†æ {i+1}: {advanced_queries[i][:30]}...")
        if result.insights:
            top_insight = max(result.insights, key=lambda x: x.confidence)
            print(f"     å…³é”®æ´å¯Ÿ: {top_insight.title}")
            print(f"     ç½®ä¿¡åº¦: {top_insight.confidence:.2f}")
    
    return results

async def demo_interactive_analysis():
    """æ¼”ç¤ºäº¤äº’å¼åˆ†æ"""
    print("\nğŸ¯ äº¤äº’å¼åˆ†ææ¼”ç¤º")
    print("=" * 50)
    
    assistant = DataAnalysisAssistant(api_key="your-deepseek-key")
    
    # æ³¨å†Œæ•°æ®æº
    data_source = DataSource(
        id="interactive_data",
        name="äº¤äº’å¼åˆ†ææ•°æ®",
        description="ç”¨äºäº¤äº’å¼åˆ†æçš„ç¤ºä¾‹æ•°æ®",
        data_type=DataType.MIXED
    )
    assistant.register_data_source(data_source)
    
    # æ¨¡æ‹Ÿäº¤äº’å¼åˆ†æä¼šè¯
    conversation = [
        "é¦–å…ˆç»™æˆ‘ä¸€ä¸ªæ•°æ®çš„æ•´ä½“æ¦‚è§ˆ",
        "é‡ç‚¹åˆ†æé”€å”®é¢æœ€é«˜çš„å‡ ä¸ªåœ°åŒº",
        "è¿™äº›åœ°åŒºæœ‰ä»€ä¹ˆå…±åŒç‰¹å¾ï¼Ÿ",
        "é¢„æµ‹è¿™äº›åœ°åŒºä¸‹ä¸ªå­£åº¦çš„è¡¨ç°",
        "ç»™å‡ºå…·ä½“çš„ä¸šåŠ¡å»ºè®®"
    ]
    
    print(f"ğŸ’¬ äº¤äº’å¼åˆ†æä¼šè¯:")
    
    context_results = []
    
    for i, query in enumerate(conversation):
        print(f"\nğŸ‘¤ ç”¨æˆ·: {query}")
        print(f"ğŸ¤– åŠ©æ‰‹: æ­£åœ¨åˆ†æ...")
        
        # åœ¨æŸ¥è¯¢ä¸­åŠ å…¥ä¸Šä¸‹æ–‡
        if context_results:
            contextual_query = f"åŸºäºä¹‹å‰çš„åˆ†æç»“æœï¼Œ{query}"
        else:
            contextual_query = query
        
        result = await assistant.analyze(contextual_query, data_source.id)
        context_results.append(result)
        
        print(f"ğŸ¤– åŠ©æ‰‹: {result.summary}")
        
        # æ˜¾ç¤ºå…³é”®æ´å¯Ÿ
        if result.insights:
            top_insight = result.insights[0]
            print(f"ğŸ’¡ å…³é”®å‘ç°: {top_insight.title}")
            if top_insight.recommendations:
                print(f"ğŸ“‹ å»ºè®®: {top_insight.recommendations[0]}")
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print(f"\nğŸ“„ ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š...")
    
    final_summary = await assistant._generate_analysis_summary(
        "äº¤äº’å¼åˆ†æä¼šè¯æ€»ç»“",
        [stat for result in context_results for stat in result.statistical_results],
        [insight for result in context_results for insight in result.insights]
    )
    
    print(f"ğŸ“Š æœ€ç»ˆåˆ†ææŠ¥å‘Š:")
    print(f"   {final_summary}")
    
    return context_results

async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ“Š HarborAI æ•°æ®åˆ†æåŠ©æ‰‹æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åŸºç¡€æ•°æ®åˆ†ææ¼”ç¤º
        assistant, basic_result = await demo_basic_analysis()
        
        # è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ¼”ç¤º
        await demo_natural_language_query()
        
        # é«˜çº§åˆ†æåŠŸèƒ½æ¼”ç¤º
        await demo_advanced_analytics()
        
        # äº¤äº’å¼åˆ†ææ¼”ç¤º
        await demo_interactive_analysis()
        
        print("\nğŸ“ˆ ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡:")
        history = assistant.get_analysis_history()
        if history:
            avg_time = sum(r.execution_time for r in history) / len(history)
            total_insights = sum(len(r.insights) for r in history)
            total_visualizations = sum(len(r.visualizations) for r in history)
            
            print(f"   æ€»åˆ†ææ¬¡æ•°: {len(history)}")
            print(f"   å¹³å‡åˆ†ææ—¶é—´: {avg_time:.3f}s")
            print(f"   æ€»ç”Ÿæˆæ´å¯Ÿ: {total_insights}")
            print(f"   æ€»ç”Ÿæˆå›¾è¡¨: {total_visualizations}")
        
        print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å»ºè®®:")
        print("   1. é›†æˆæ›´å¤šæ•°æ®æºï¼ˆæ•°æ®åº“ã€APIã€æ–‡ä»¶ç³»ç»Ÿï¼‰")
        print("   2. å®ç°å®æ—¶æ•°æ®æµåˆ†æå’Œç›‘æ§")
        print("   3. æ·»åŠ æ›´å¤šé«˜çº§åˆ†æç®—æ³•ï¼ˆæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ï¼‰")
        print("   4. æ„å»ºäº¤äº’å¼ä»ªè¡¨æ¿å’ŒæŠ¥å‘Šç³»ç»Ÿ")
        print("   5. å®ç°åˆ†æç»“æœçš„è‡ªåŠ¨åŒ–åˆ†å‘å’Œå‘Šè­¦")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())