# -*- coding: utf-8 -*-
"""
è¯¦ç»†æ€§èƒ½åˆ†ææµ‹è¯• - HarborAI vs ç›´æ¥Agentlyè°ƒç”¨
ç›®æ ‡ï¼šè§‚æµ‹æ¯ä¸ªæ­¥éª¤çš„æ€§èƒ½å¼€é”€ï¼Œç¡®ä¿æ ·æœ¬ä¸€è‡´æ€§ï¼Œåˆ†æé‡æ„å¿…è¦æ€§
"""

import os
import sys
import time
import json
import statistics
import tracemalloc
import psutil
import threading
from contextlib import contextmanager
from functools import wraps
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

# è®¾ç½®æ§åˆ¶å°ç¼–ç 
if sys.platform == "win32":
    import codecs
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
    sys.stderr = codecs.getwriter("utf-8")(sys.stderr.detach())

# å¯¼å…¥ä¾èµ–
from dotenv import load_dotenv
from Agently.agently import Agently
from harborai import HarborAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_env_result = load_dotenv()
print(f"ğŸ”§ ç¯å¢ƒå˜é‡åŠ è½½ç»“æœ: {load_env_result}")

# è®¾ç½®FASTæ¨¡å¼ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥HarborAIä¹‹å‰è®¾ç½®ï¼‰
os.environ["HARBORAI_PERFORMANCE_MODE"] = "fast"
os.environ["HARBORAI_ENABLE_FAST_PATH"] = "true"
print(f"ğŸš€ è®¾ç½®æ€§èƒ½æ¨¡å¼: FAST")
print(f"ğŸš€ å¯ç”¨å¿«é€Ÿè·¯å¾„: true")

# æ¸…é™¤settingsç¼“å­˜ä»¥ç¡®ä¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
from harborai.config.settings import get_settings
from harborai.config.performance import reset_performance_config, PerformanceMode

# æ¸…é™¤ç¼“å­˜
get_settings.cache_clear()
print(f"ğŸ”„ æ¸…é™¤settingsç¼“å­˜")

# é‡ç½®æ€§èƒ½é…ç½®
reset_performance_config(PerformanceMode.FAST)
print(f"ğŸ”„ é‡ç½®æ€§èƒ½é…ç½®ä¸ºFASTæ¨¡å¼")

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    step_name: str
    start_time: float
    end_time: float
    duration: float
    memory_before: float
    memory_after: float
    memory_delta: float
    cpu_percent: float
    thread_count: int
    additional_info: Dict[str, Any] = None

    def __post_init__(self):
        if self.additional_info is None:
            self.additional_info = {}

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics: List[PerformanceMetrics] = []
        self.process = psutil.Process()
        
    def start_monitoring(self):
        """å¼€å§‹å†…å­˜ç›‘æ§"""
        tracemalloc.start()
        
    def stop_monitoring(self):
        """åœæ­¢å†…å­˜ç›‘æ§"""
        if tracemalloc.is_tracing():
            tracemalloc.stop()
    
    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        try:
            return self.process.memory_info().rss / 1024 / 1024
        except:
            return 0.0
    
    def get_cpu_percent(self) -> float:
        """è·å–CPUä½¿ç”¨ç‡"""
        try:
            return self.process.cpu_percent()
        except:
            return 0.0
    
    def get_thread_count(self) -> int:
        """è·å–çº¿ç¨‹æ•°"""
        try:
            return self.process.num_threads()
        except:
            return 0
    
    @contextmanager
    def monitor_step(self, step_name: str, additional_info: Dict[str, Any] = None):
        """ç›‘æ§å•ä¸ªæ­¥éª¤çš„æ€§èƒ½"""
        print(f"ğŸ“Š å¼€å§‹ç›‘æ§æ­¥éª¤: {step_name}")
        
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.perf_counter()
        memory_before = self.get_memory_usage()
        cpu_before = self.get_cpu_percent()
        thread_count = self.get_thread_count()
        
        try:
            yield
        finally:
            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = time.perf_counter()
            memory_after = self.get_memory_usage()
            duration = end_time - start_time
            memory_delta = memory_after - memory_before
            
            # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡
            metrics = PerformanceMetrics(
                step_name=step_name,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                memory_before=memory_before,
                memory_after=memory_after,
                memory_delta=memory_delta,
                cpu_percent=cpu_before,
                thread_count=thread_count,
                additional_info=additional_info or {}
            )
            
            self.metrics.append(metrics)
            
            print(f"âœ… æ­¥éª¤å®Œæˆ: {step_name}")
            print(f"   â±ï¸  è€—æ—¶: {duration:.4f}ç§’")
            print(f"   ğŸ’¾ å†…å­˜å˜åŒ–: {memory_delta:+.2f}MB ({memory_before:.2f} â†’ {memory_after:.2f})")
            print(f"   ğŸ–¥ï¸  CPU: {cpu_before:.1f}%")
            print(f"   ğŸ§µ çº¿ç¨‹æ•°: {thread_count}")
            if additional_info:
                print(f"   ğŸ“ é¢å¤–ä¿¡æ¯: {additional_info}")
            print()

def performance_decorator(step_name: str):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            monitor = getattr(wrapper, '_monitor', None)
            if monitor:
                with monitor.monitor_step(step_name, {"function": func.__name__, "args_count": len(args), "kwargs_count": len(kwargs)}):
                    return func(*args, **kwargs)
            else:
                return func(*args, **kwargs)
        return wrapper
    return decorator

class TestConfiguration:
    """æµ‹è¯•é…ç½®ç±» - ç¡®ä¿æ ·æœ¬ä¸€è‡´æ€§"""
    
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
        self.deepseek_base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        
        if not self.deepseek_api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        
        print(f"ğŸ”‘ DeepSeek API Key: {self.deepseek_api_key[:10]}...")
        print(f"ğŸŒ DeepSeek Base URL: {self.deepseek_base_url}")
        
        # ç»Ÿä¸€çš„æµ‹è¯•å‚æ•°
        self.model_name = "deepseek-chat"
        self.temperature = 0.1  # ä½æ¸©åº¦ç¡®ä¿ç»“æœä¸€è‡´æ€§
        self.max_tokens = 1000
        self.test_rounds = 5
        
        # ç»Ÿä¸€çš„æµ‹è¯•è¾“å…¥
        self.test_prompt = "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼š'ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œæˆ‘å¿ƒæƒ…å¾ˆæ„‰å¿«ï¼Œå·¥ä½œä¹Ÿå¾ˆé¡ºåˆ©ã€‚'"
        
        # ç»Ÿä¸€çš„JSON Schema
        self.json_schema = {
            "type": "object",
            "properties": {
                "analysis": {
                    "type": "string",
                    "description": "è¯¦ç»†çš„æƒ…æ„Ÿåˆ†æ"
                },
                "sentiment": {
                    "type": "string",
                    "enum": ["positive", "negative", "neutral"],
                    "description": "æƒ…æ„Ÿå€¾å‘åˆ†ç±»"
                },
                "confidence": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 1,
                    "description": "ç½®ä¿¡åº¦åˆ†æ•°"
                },
                "keywords": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "å…³é”®è¯åˆ—è¡¨"
                },
                "metadata": {
                    "type": "object",
                    "properties": {
                        "processed_at": {"type": "string"},
                        "model_used": {"type": "string"}
                    }
                }
            },
            "required": ["analysis", "sentiment", "confidence", "keywords"]
        }
        
        print(f"ğŸ“‹ æµ‹è¯•é…ç½®:")
        print(f"   ğŸ¤– æ¨¡å‹: {self.model_name}")
        print(f"   ğŸŒ¡ï¸  æ¸©åº¦: {self.temperature}")
        print(f"   ğŸ“ æœ€å¤§tokens: {self.max_tokens}")
        print(f"   ğŸ”„ æµ‹è¯•è½®æ•°: {self.test_rounds}")
        print(f"   ğŸ’¬ æµ‹è¯•æç¤º: {self.test_prompt}")
        print()

class HarborAITester:
    """HarborAIæµ‹è¯•å™¨ - å¸¦è¯¦ç»†æ€§èƒ½ç›‘æ§"""
    
    def __init__(self, config: TestConfiguration, monitor: PerformanceMonitor):
        self.config = config
        self.monitor = monitor
        self.client = None
        
        # ä¸ºè£…é¥°å™¨è®¾ç½®ç›‘æ§å™¨
        self._setup_decorators()
    
    def _setup_decorators(self):
        """è®¾ç½®è£…é¥°å™¨çš„ç›‘æ§å™¨"""
        for attr_name in dir(self):
            attr = getattr(self, attr_name)
            if hasattr(attr, '_monitor'):
                attr._monitor = self.monitor
    
    @performance_decorator("HarborAI-å®¢æˆ·ç«¯åˆ›å»º")
    def create_client(self):
        """åˆ›å»ºHarborAIå®¢æˆ·ç«¯ - é…ç½®ä¸ºFASTæ¨¡å¼"""
        print("ğŸš€ åˆ›å»ºHarborAIå®¢æˆ·ç«¯ï¼ˆFASTæ¨¡å¼ï¼‰...")
        
        self.client = HarborAI(
            api_key=self.config.deepseek_api_key,
            base_url=self.config.deepseek_base_url,
            model=self.config.model_name,
            performance_mode="fast"  # æ˜¾å¼è®¾ç½®FASTæ¨¡å¼
        )
        
        print(f"âœ… HarborAIå®¢æˆ·ç«¯åˆ›å»ºå®Œæˆï¼ˆFASTæ¨¡å¼ï¼‰")
        print(f"ğŸš€ æ€§èƒ½æ¨¡å¼: FAST - å¯ç”¨å¿«é€Ÿè·¯å¾„ä¼˜åŒ–")
        return self.client
    
    @performance_decorator("HarborAI-å‚æ•°å‡†å¤‡")
    def prepare_parameters(self):
        """å‡†å¤‡è°ƒç”¨å‚æ•°"""
        print("ğŸ“‹ å‡†å¤‡HarborAIè°ƒç”¨å‚æ•°...")
        
        params = {
            "model": self.config.model_name,
            "messages": [{"role": "user", "content": self.config.test_prompt}],
            "response_format": {
                "type": "json_schema",
                "json_schema": {
                    "name": "sentiment_analysis",
                    "schema": self.config.json_schema,
                    "strict": True
                }
            },
            "structured_provider": "agently",  # å…³é”®ï¼šæŒ‡å®šä½¿ç”¨Agently
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }
        
        print(f"âœ… å‚æ•°å‡†å¤‡å®Œæˆ: {len(params)} ä¸ªå‚æ•°")
        print(f"ğŸ”§ ä½¿ç”¨ç»“æ„åŒ–æä¾›è€…: agently")
        print(f"ğŸ“‹ Schemaåç§°: sentiment_analysis")
        return params
    
    @performance_decorator("HarborAI-ç»“æ„åŒ–è¾“å‡ºè°ƒç”¨")
    def call_structured_output(self, params):
        """è°ƒç”¨HarborAIçš„ç»“æ„åŒ–è¾“å‡º"""
        print("ğŸ¯ è°ƒç”¨HarborAIç»“æ„åŒ–è¾“å‡º...")
        print(f"ğŸ“‹ è°ƒç”¨å‚æ•°: {list(params.keys())}")
        print(f"ğŸš€ æ€§èƒ½æ¨¡å¼: {params.get('structured_provider', 'unknown')}")
        print(f"ğŸ“ å“åº”æ ¼å¼: {params.get('response_format', {}).get('type', 'unknown')}")
        
        # æ£€æŸ¥æ€§èƒ½é…ç½®
        from harborai.config.performance import get_performance_config
        perf_config = get_performance_config()
        print(f"ğŸ”§ æ€§èƒ½é…ç½®æ¨¡å¼: {perf_config.mode.value}")
        print(f"ğŸš€ å¿«é€Ÿè·¯å¾„å¯ç”¨: {perf_config.feature_flags.enable_fast_path}")
        
        # æ£€æŸ¥å¿«é€Ÿè·¯å¾„æ¡ä»¶
        response_format = params.get('response_format', {})
        structured_provider = params.get('structured_provider')
        stream = params.get('stream', False)
        
        fast_structured_conditions = [
            f"FASTæ¨¡å¼: {perf_config.mode.value == 'fast'}",
            f"æœ‰ç»“æ„åŒ–è¾“å‡º: {response_format and response_format.get('type') == 'json_schema'}",
            f"ä½¿ç”¨agently: {structured_provider == 'agently'}",
            f"éæµå¼: {not stream}"
        ]
        print(f"ğŸ” å¿«é€Ÿç»“æ„åŒ–è·¯å¾„æ¡ä»¶: {fast_structured_conditions}")
        
        response = self.client.chat.completions.create(**params)
        
        print(f"âœ… HarborAIè°ƒç”¨å®Œæˆ")
        print(f"ğŸ” å“åº”ID: {getattr(response, 'id', 'unknown')}")
        print(f"ğŸ¯ æ˜¯å¦å¿«é€Ÿè·¯å¾„: {'fast-structured' in getattr(response, 'id', '')}")
        return response
    
    @performance_decorator("HarborAI-å“åº”è§£æ")
    def parse_response(self, response):
        """è§£æå“åº”"""
        print("ğŸ” è§£æHarborAIå“åº”...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰parsedç»“æœï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰
        if hasattr(response.choices[0].message, 'parsed') and response.choices[0].message.parsed:
            parsed_content = response.choices[0].message.parsed
            print(f"âœ… è·å¾—ç»“æ„åŒ–è¾“å‡ºç»“æœ: {len(str(parsed_content))} å­—ç¬¦")
            print(f"ğŸ¯ ç»“æ„åŒ–ç»“æœç±»å‹: {type(parsed_content)}")
            print(f"ğŸ“ ç»“æ„åŒ–ç»“æœé¢„è§ˆ: {json.dumps(parsed_content, ensure_ascii=False, indent=2)[:200]}...")
            return parsed_content
        else:
            # å›é€€åˆ°contentè§£æ
            content = response.choices[0].message.content
            print(f"âš ï¸ æœªè·å¾—ç»“æ„åŒ–è¾“å‡ºï¼Œå°è¯•è§£æcontent: {content[:100]}...")
            
            if isinstance(content, str):
                try:
                    parsed_content = json.loads(content)
                    print(f"âœ… Content JSONè§£ææˆåŠŸ: {len(str(parsed_content))} å­—ç¬¦")
                    return parsed_content
                except json.JSONDecodeError as e:
                    print(f"âŒ Content JSONè§£æå¤±è´¥: {e}")
                    raise ValueError(f"æ— æ³•è§£æå“åº”å†…å®¹ä¸ºJSON: {e}")
            else:
                print(f"âœ… Contentç›´æ¥è¿”å›: {len(str(content))} å­—ç¬¦")
                return content
    
    def run_single_test(self) -> Tuple[Dict[str, Any], float]:
        """è¿è¡Œå•æ¬¡æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹HarborAIå•æ¬¡æµ‹è¯•...")
        
        start_time = time.perf_counter()
        
        try:
            # åˆ›å»ºå®¢æˆ·ç«¯
            client = self.create_client()
            
            # å‡†å¤‡å‚æ•°
            params = self.prepare_parameters()
            
            # è°ƒç”¨API
            response = self.call_structured_output(params)
            
            # è§£æå“åº”
            result = self.parse_response(response)
            
            end_time = time.perf_counter()
            total_time = end_time - start_time
            
            print(f"âœ… HarborAIå•æ¬¡æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.4f}ç§’")
            return result, total_time
            
        except Exception as e:
            end_time = time.perf_counter()
            total_time = end_time - start_time
            print(f"âŒ HarborAIæµ‹è¯•å¤±è´¥: {str(e)}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            print(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            return None, total_time

class AgentlyTester:
    """Agentlyæµ‹è¯•å™¨ - å¸¦è¯¦ç»†æ€§èƒ½ç›‘æ§"""
    
    def __init__(self, config: TestConfiguration, monitor: PerformanceMonitor):
        self.config = config
        self.monitor = monitor
        self.agent = None
        
        # ä¸ºè£…é¥°å™¨è®¾ç½®ç›‘æ§å™¨
        self._setup_decorators()
    
    def _setup_decorators(self):
        """è®¾ç½®è£…é¥°å™¨çš„ç›‘æ§å™¨"""
        for attr_name in dir(self):
            attr = getattr(self, attr_name)
            if hasattr(attr, '_monitor'):
                attr._monitor = self.monitor
    
    @performance_decorator("Agently-å…¨å±€é…ç½®")
    def configure_agently(self):
        """é…ç½®Agentlyå…¨å±€è®¾ç½®"""
        print("âš™ï¸ é…ç½®Agentlyå…¨å±€è®¾ç½®...")
        
        # ä½¿ç”¨æ­£ç¡®çš„Agentlyé…ç½®æ–¹å¼
        Agently.set_settings(
            "OpenAICompatible",
            {
                "base_url": self.config.deepseek_base_url,
                "model": self.config.model_name,
                "model_type": "chat",
                "auth": self.config.deepseek_api_key,
            },
        )
        
        print(f"âœ… Agentlyå…¨å±€é…ç½®å®Œæˆ")
    
    @performance_decorator("Agently-Agentåˆ›å»º")
    def create_agent(self):
        """åˆ›å»ºAgently Agent"""
        print("ğŸ¤– åˆ›å»ºAgently Agent...")
        
        # ä½¿ç”¨æ­£ç¡®çš„Agentlyåˆ›å»ºæ–¹å¼
        self.agent = Agently.create_agent()
        
        print(f"âœ… Agently Agentåˆ›å»ºå®Œæˆ")
        return self.agent
    
    @performance_decorator("Agently-Schemaè½¬æ¢")
    def convert_schema_to_agently(self):
        """å°†JSON Schemaè½¬æ¢ä¸ºAgentlyæ ¼å¼"""
        print("ğŸ”„ è½¬æ¢JSON Schemaä¸ºAgentlyæ ¼å¼...")
        
        # è½¬æ¢JSON Schemaä¸ºAgentlyè¾“å‡ºæ ¼å¼
        agently_output = {
            "analysis": ("str", "è¯¦ç»†çš„æƒ…æ„Ÿåˆ†æ"),
            "sentiment": ("str", "æƒ…æ„Ÿå€¾å‘åˆ†ç±»: positive/negative/neutral"),
            "confidence": ("float", "ç½®ä¿¡åº¦åˆ†æ•°(0-1)"),
            "keywords": (["str"], "å…³é”®è¯åˆ—è¡¨"),
            "metadata": ({
                "processed_at": ("str", "å¤„ç†æ—¶é—´"),
                "model_used": ("str", "ä½¿ç”¨çš„æ¨¡å‹")
            }, "å…ƒæ•°æ®ä¿¡æ¯")
        }
        
        print(f"âœ… Schemaè½¬æ¢å®Œæˆ: {len(agently_output)} ä¸ªå­—æ®µ")
        return agently_output
    
    @performance_decorator("Agently-è¾“å‡ºæ ¼å¼è®¾ç½®")
    def set_output_format(self, agently_output):
        """è®¾ç½®è¾“å‡ºæ ¼å¼"""
        print("ğŸ“ è®¾ç½®Agentlyè¾“å‡ºæ ¼å¼...")
        
        self.agent.output(agently_output)
        
        print(f"âœ… è¾“å‡ºæ ¼å¼è®¾ç½®å®Œæˆ")
    
    @performance_decorator("Agently-ç»“æ„åŒ–è¾“å‡ºè°ƒç”¨")
    def call_structured_output(self):
        """è°ƒç”¨ç»“æ„åŒ–è¾“å‡º"""
        print("ğŸ¯ è°ƒç”¨Agentlyç»“æ„åŒ–è¾“å‡º...")
        
        result = self.agent.input(self.config.test_prompt).start()
        
        print(f"âœ… Agentlyè°ƒç”¨å®Œæˆ")
        return result
    
    def run_single_test(self) -> Tuple[Dict[str, Any], float]:
        """è¿è¡Œå•æ¬¡æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹Agentlyå•æ¬¡æµ‹è¯•...")
        
        start_time = time.perf_counter()
        
        try:
            # é…ç½®Agently
            self.configure_agently()
            
            # åˆ›å»ºAgent
            agent = self.create_agent()
            
            # è½¬æ¢Schema
            agently_output = self.convert_schema_to_agently()
            
            # è®¾ç½®è¾“å‡ºæ ¼å¼
            self.set_output_format(agently_output)
            
            # è°ƒç”¨API
            result = self.call_structured_output()
            
            end_time = time.perf_counter()
            total_time = end_time - start_time
            
            print(f"âœ… Agentlyå•æ¬¡æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.4f}ç§’")
            return result, total_time
            
        except Exception as e:
            end_time = time.perf_counter()
            total_time = end_time - start_time
            print(f"âŒ Agentlyæµ‹è¯•å¤±è´¥: {str(e)}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            print(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            return None, total_time

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.harborai_results = []
        self.agently_results = []
        self.harborai_metrics = []
        self.agently_metrics = []
    
    def analyze_step_performance(self, metrics: List[PerformanceMetrics]) -> Dict[str, Any]:
        """åˆ†ææ­¥éª¤æ€§èƒ½"""
        if not metrics:
            return {}
        
        # æŒ‰æ­¥éª¤åˆ†ç»„
        step_groups = {}
        for metric in metrics:
            if metric.step_name not in step_groups:
                step_groups[metric.step_name] = []
            step_groups[metric.step_name].append(metric)
        
        # åˆ†ææ¯ä¸ªæ­¥éª¤
        step_analysis = {}
        for step_name, step_metrics in step_groups.items():
            durations = [m.duration for m in step_metrics]
            memory_deltas = [m.memory_delta for m in step_metrics]
            
            step_analysis[step_name] = {
                "count": len(step_metrics),
                "total_duration": sum(durations),
                "avg_duration": statistics.mean(durations),
                "min_duration": min(durations),
                "max_duration": max(durations),
                "std_duration": statistics.stdev(durations) if len(durations) > 1 else 0,
                "total_memory_delta": sum(memory_deltas),
                "avg_memory_delta": statistics.mean(memory_deltas),
                "percentage": (sum(durations) / sum(m.duration for m in metrics)) * 100
            }
        
        return step_analysis
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆè¯¦ç»†æ€§èƒ½æŠ¥å‘Š...")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        harborai_times = [t for _, t in self.harborai_results if t > 0]
        agently_times = [t for _, t in self.agently_results if t > 0]
        
        harborai_success_count = len([r for r, _ in self.harborai_results if r is not None])
        agently_success_count = len([r for r, _ in self.agently_results if r is not None])
        
        # åˆ†ææ­¥éª¤æ€§èƒ½
        harborai_step_analysis = self.analyze_step_performance(self.harborai_metrics)
        agently_step_analysis = self.analyze_step_performance(self.agently_metrics)
        
        report = {
            "test_summary": {
                "test_time": datetime.now().isoformat(),
                "total_rounds": len(self.harborai_results),
                "harborai_success_rate": harborai_success_count / len(self.harborai_results) * 100,
                "agently_success_rate": agently_success_count / len(self.agently_results) * 100
            },
            "harborai_performance": {
                "total_time": sum(harborai_times),
                "avg_time": statistics.mean(harborai_times) if harborai_times else 0,
                "min_time": min(harborai_times) if harborai_times else 0,
                "max_time": max(harborai_times) if harborai_times else 0,
                "std_time": statistics.stdev(harborai_times) if len(harborai_times) > 1 else 0,
                "step_analysis": harborai_step_analysis
            },
            "agently_performance": {
                "total_time": sum(agently_times),
                "avg_time": statistics.mean(agently_times) if agently_times else 0,
                "min_time": min(agently_times) if agently_times else 0,
                "max_time": max(agently_times) if agently_times else 0,
                "std_time": statistics.stdev(agently_times) if len(agently_times) > 1 else 0,
                "step_analysis": agently_step_analysis
            }
        }
        
        # è®¡ç®—æ€§èƒ½å¯¹æ¯”
        if harborai_times and agently_times:
            harborai_avg = statistics.mean(harborai_times)
            agently_avg = statistics.mean(agently_times)
            
            report["performance_comparison"] = {
                "harborai_avg_time": harborai_avg,
                "agently_avg_time": agently_avg,
                "time_difference": harborai_avg - agently_avg,
                "harborai_slower_by_factor": harborai_avg / agently_avg if agently_avg > 0 else 0,
                "harborai_slower_by_percentage": ((harborai_avg - agently_avg) / agently_avg * 100) if agently_avg > 0 else 0
            }
        
        return report
    
    def identify_bottlenecks(self, report: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # åˆ†æHarborAIæ­¥éª¤ç“¶é¢ˆ
        harborai_steps = report.get("harborai_performance", {}).get("step_analysis", {})
        for step_name, step_data in harborai_steps.items():
            if step_data["percentage"] > 20:  # å ç”¨è¶…è¿‡20%æ—¶é—´çš„æ­¥éª¤
                bottlenecks.append({
                    "type": "HarborAIæ­¥éª¤ç“¶é¢ˆ",
                    "step": step_name,
                    "percentage": step_data["percentage"],
                    "avg_duration": step_data["avg_duration"],
                    "severity": "é«˜" if step_data["percentage"] > 50 else "ä¸­"
                })
        
        # åˆ†æAgentlyæ­¥éª¤ç“¶é¢ˆ
        agently_steps = report.get("agently_performance", {}).get("step_analysis", {})
        for step_name, step_data in agently_steps.items():
            if step_data["percentage"] > 20:
                bottlenecks.append({
                    "type": "Agentlyæ­¥éª¤ç“¶é¢ˆ",
                    "step": step_name,
                    "percentage": step_data["percentage"],
                    "avg_duration": step_data["avg_duration"],
                    "severity": "é«˜" if step_data["percentage"] > 50 else "ä¸­"
                })
        
        return bottlenecks
    
    def generate_optimization_recommendations(self, report: Dict[str, Any], bottlenecks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºæ€§èƒ½å¯¹æ¯”çš„å»ºè®®
        comparison = report.get("performance_comparison", {})
        if comparison.get("harborai_slower_by_factor", 0) > 2:
            recommendations.append({
                "priority": "é«˜",
                "category": "æ¶æ„ä¼˜åŒ–",
                "title": "è€ƒè™‘é‡æ„HarborAIçš„Agentlyé›†æˆ",
                "description": f"HarborAIæ¯”ç›´æ¥Agentlyæ…¢{comparison.get('harborai_slower_by_factor', 0):.1f}å€ï¼Œå»ºè®®é‡æ„",
                "specific_actions": [
                    "å‡å°‘ä¸­é—´å±‚è°ƒç”¨å¼€é”€",
                    "ä¼˜åŒ–å‚æ•°ä¼ é€’æµç¨‹",
                    "ç¼“å­˜é‡å¤é…ç½®æ“ä½œ",
                    "è€ƒè™‘ç›´æ¥ä½¿ç”¨Agently API"
                ]
            })
        
        # åŸºäºç“¶é¢ˆçš„å»ºè®®
        for bottleneck in bottlenecks:
            if "HarborAI" in bottleneck["type"]:
                if "å®¢æˆ·ç«¯åˆ›å»º" in bottleneck["step"]:
                    recommendations.append({
                        "priority": "ä¸­",
                        "category": "å®¢æˆ·ç«¯ä¼˜åŒ–",
                        "title": "ä¼˜åŒ–HarborAIå®¢æˆ·ç«¯åˆ›å»º",
                        "description": f"å®¢æˆ·ç«¯åˆ›å»ºå ç”¨{bottleneck['percentage']:.1f}%æ—¶é—´",
                        "specific_actions": [
                            "å®ç°å®¢æˆ·ç«¯è¿æ¥æ± ",
                            "ç¼“å­˜å®¢æˆ·ç«¯å®ä¾‹",
                            "å»¶è¿Ÿåˆå§‹åŒ–éå¿…è¦ç»„ä»¶"
                        ]
                    })
                elif "ç»“æ„åŒ–è¾“å‡ºè°ƒç”¨" in bottleneck["step"]:
                    recommendations.append({
                        "priority": "é«˜",
                        "category": "APIè°ƒç”¨ä¼˜åŒ–",
                        "title": "ä¼˜åŒ–ç»“æ„åŒ–è¾“å‡ºè°ƒç”¨",
                        "description": f"APIè°ƒç”¨å ç”¨{bottleneck['percentage']:.1f}%æ—¶é—´",
                        "specific_actions": [
                            "å‡å°‘ä¸å¿…è¦çš„å‚æ•°å¤„ç†",
                            "ä¼˜åŒ–JSON Schemaè½¬æ¢",
                            "å®ç°è¯·æ±‚æ‰¹å¤„ç†"
                        ]
                    })
        
        # é€šç”¨ä¼˜åŒ–å»ºè®®
        recommendations.append({
            "priority": "ä¸­",
            "category": "ç›‘æ§ä¼˜åŒ–",
            "title": "æ·»åŠ ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ç›‘æ§",
            "description": "å»ºç«‹æŒç»­çš„æ€§èƒ½ç›‘æ§ä½“ç³»",
            "specific_actions": [
                "æ·»åŠ å…³é”®æ­¥éª¤çš„è€—æ—¶ç›‘æ§",
                "è®¾ç½®æ€§èƒ½å‘Šè­¦é˜ˆå€¼",
                "å®šæœŸè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•",
                "å»ºç«‹æ€§èƒ½å›å½’æ£€æµ‹"
            ]
        })
        
        return recommendations

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¯¦ç»†æ€§èƒ½åˆ†ææµ‹è¯•")
    print("=" * 80)
    
    # åˆå§‹åŒ–ç»„ä»¶
    config = TestConfiguration()
    monitor = PerformanceMonitor()
    analyzer = PerformanceAnalyzer()
    
    monitor.start_monitoring()
    
    try:
        # è¿è¡Œå¤šè½®æµ‹è¯•
        print(f"ğŸ”„ å¼€å§‹ {config.test_rounds} è½®æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("=" * 80)
        
        for round_num in range(1, config.test_rounds + 1):
            print(f"\nğŸ ç¬¬ {round_num}/{config.test_rounds} è½®æµ‹è¯•")
            print("-" * 40)
            
            # æµ‹è¯•HarborAI
            print("\nğŸŒŠ æµ‹è¯•HarborAI + Agently")
            harborai_monitor = PerformanceMonitor()
            harborai_monitor.start_monitoring()
            
            harborai_tester = HarborAITester(config, harborai_monitor)
            harborai_result, harborai_time = harborai_tester.run_single_test()
            
            analyzer.harborai_results.append((harborai_result, harborai_time))
            analyzer.harborai_metrics.extend(harborai_monitor.metrics)
            harborai_monitor.stop_monitoring()
            
            # æµ‹è¯•ç›´æ¥Agently
            print("\nâš¡ æµ‹è¯•ç›´æ¥Agently")
            agently_monitor = PerformanceMonitor()
            agently_monitor.start_monitoring()
            
            agently_tester = AgentlyTester(config, agently_monitor)
            agently_result, agently_time = agently_tester.run_single_test()
            
            analyzer.agently_results.append((agently_result, agently_time))
            analyzer.agently_metrics.extend(agently_monitor.metrics)
            agently_monitor.stop_monitoring()
            
            print(f"\nğŸ“Š ç¬¬{round_num}è½®ç»“æœ:")
            print(f"   HarborAI: {harborai_time:.4f}ç§’ {'âœ…' if harborai_result else 'âŒ'}")
            print(f"   Agently:  {agently_time:.4f}ç§’ {'âœ…' if agently_result else 'âŒ'}")
            
            if harborai_time > 0 and agently_time > 0:
                factor = harborai_time / agently_time
                print(f"   æ€§èƒ½å·®å¼‚: HarborAIæ¯”Agentlyæ…¢ {factor:.2f}å€")
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        print("\n" + "=" * 80)
        print("ğŸ“Š ç”Ÿæˆè¯¦ç»†æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("=" * 80)
        
        report = analyzer.generate_performance_report()
        bottlenecks = analyzer.identify_bottlenecks(report)
        recommendations = analyzer.generate_optimization_recommendations(report, bottlenecks)
        
        # è¾“å‡ºæŠ¥å‘Š
        print("\nğŸ“ˆ æµ‹è¯•æ€»ç»“:")
        summary = report["test_summary"]
        print(f"   æµ‹è¯•æ—¶é—´: {summary['test_time']}")
        print(f"   æ€»æµ‹è¯•è½®æ•°: {summary['total_rounds']}")
        print(f"   HarborAIæˆåŠŸç‡: {summary['harborai_success_rate']:.1f}%")
        print(f"   AgentlyæˆåŠŸç‡: {summary['agently_success_rate']:.1f}%")
        
        print("\nâ±ï¸ æ€§èƒ½å¯¹æ¯”:")
        if "performance_comparison" in report:
            comp = report["performance_comparison"]
            print(f"   HarborAIå¹³å‡è€—æ—¶: {comp['harborai_avg_time']:.4f}ç§’")
            print(f"   Agentlyå¹³å‡è€—æ—¶: {comp['agently_avg_time']:.4f}ç§’")
            print(f"   æ—¶é—´å·®å¼‚: {comp['time_difference']:.4f}ç§’")
            print(f"   HarborAIæ…¢å€æ•°: {comp['harborai_slower_by_factor']:.2f}å€")
            print(f"   HarborAIæ…¢ç™¾åˆ†æ¯”: {comp['harborai_slower_by_percentage']:.1f}%")
        
        print("\nğŸ” HarborAIæ­¥éª¤åˆ†æ:")
        harborai_steps = report["harborai_performance"]["step_analysis"]
        for step_name, step_data in harborai_steps.items():
            print(f"   {step_name}:")
            print(f"     å¹³å‡è€—æ—¶: {step_data['avg_duration']:.4f}ç§’")
            print(f"     æ—¶é—´å æ¯”: {step_data['percentage']:.1f}%")
            print(f"     å†…å­˜å˜åŒ–: {step_data['avg_memory_delta']:+.2f}MB")
        
        print("\nğŸ” Agentlyæ­¥éª¤åˆ†æ:")
        agently_steps = report["agently_performance"]["step_analysis"]
        for step_name, step_data in agently_steps.items():
            print(f"   {step_name}:")
            print(f"     å¹³å‡è€—æ—¶: {step_data['avg_duration']:.4f}ç§’")
            print(f"     æ—¶é—´å æ¯”: {step_data['percentage']:.1f}%")
            print(f"     å†…å­˜å˜åŒ–: {step_data['avg_memory_delta']:+.2f}MB")
        
        print("\nğŸš¨ æ€§èƒ½ç“¶é¢ˆè¯†åˆ«:")
        if bottlenecks:
            for bottleneck in bottlenecks:
                print(f"   [{bottleneck['severity']}] {bottleneck['type']}")
                print(f"     æ­¥éª¤: {bottleneck['step']}")
                print(f"     æ—¶é—´å æ¯”: {bottleneck['percentage']:.1f}%")
                print(f"     å¹³å‡è€—æ—¶: {bottleneck['avg_duration']:.4f}ç§’")
        else:
            print("   æœªå‘ç°æ˜æ˜¾æ€§èƒ½ç“¶é¢ˆ")
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. [{rec['priority']}] {rec['title']}")
            print(f"      ç±»åˆ«: {rec['category']}")
            print(f"      æè¿°: {rec['description']}")
            print(f"      å…·ä½“è¡ŒåŠ¨:")
            for action in rec['specific_actions']:
                print(f"        - {action}")
            print()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_data = {
            "report": report,
            "bottlenecks": bottlenecks,
            "recommendations": recommendations,
            "raw_metrics": {
                "harborai": [asdict(m) for m in analyzer.harborai_metrics],
                "agently": [asdict(m) for m in analyzer.agently_metrics]
            }
        }
        
        with open("detailed_performance_report.json", "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: detailed_performance_report.json")
        
        # é‡æ„å»ºè®®æ€»ç»“
        print("\n" + "=" * 80)
        print("ğŸ”§ é‡æ„å»ºè®®æ€»ç»“")
        print("=" * 80)
        
        if "performance_comparison" in report:
            factor = report["performance_comparison"]["harborai_slower_by_factor"]
            if factor > 3:
                print("â— å¼ºçƒˆå»ºè®®é‡æ„HarborAIçš„Agentlyé›†æˆ:")
                print("   1. æ€§èƒ½å·®å¼‚è¿‡å¤§ï¼Œéœ€è¦æ¶æ„çº§ä¼˜åŒ–")
                print("   2. è€ƒè™‘ç›´æ¥ä½¿ç”¨Agently APIè€ŒéåŒ…è£…å±‚")
                print("   3. å¦‚éœ€ä¿ç•™åŒ…è£…å±‚ï¼Œéœ€å¤§å¹…ä¼˜åŒ–è°ƒç”¨é“¾")
            elif factor > 2:
                print("âš ï¸ å»ºè®®ä¼˜åŒ–HarborAIçš„Agentlyé›†æˆ:")
                print("   1. ä¼˜åŒ–å…³é”®æ­¥éª¤çš„æ€§èƒ½ç“¶é¢ˆ")
                print("   2. å‡å°‘ä¸å¿…è¦çš„ä¸­é—´å¤„ç†")
                print("   3. è€ƒè™‘ç¼“å­˜å’Œè¿æ¥æ± ä¼˜åŒ–")
            else:
                print("âœ… HarborAIæ€§èƒ½å¯æ¥å—ï¼Œå»ºè®®è¿›è¡Œå¾®è°ƒä¼˜åŒ–")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        monitor.stop_monitoring()
        print("\nğŸ è¯¦ç»†æ€§èƒ½åˆ†ææµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()