#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HarborAI SDKç‰¹æœ‰åŠŸèƒ½æ€§èƒ½æµ‹è¯•

æµ‹è¯•HarborAI SDKç‰¹æœ‰åŠŸèƒ½çš„æ€§èƒ½è¡¨ç°ï¼š
- æ’ä»¶æ¶æ„æ€§èƒ½å¼€é”€
- Agently vs Nativeç»“æ„åŒ–è¾“å‡ºæ€§èƒ½
- æ¨ç†æ¨¡å‹æ”¯æŒçš„æ€§èƒ½å½±å“
- å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿæ€§èƒ½
- æ™ºèƒ½é™çº§æœºåˆ¶å“åº”æ—¶é—´
"""

import asyncio
import time
import statistics
import psutil
import gc
import sys
import os
from typing import Dict, List, Any, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from dataclasses import dataclass
from datetime import datetime
import json
import logging
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from harborai import HarborAI
    from harborai.config.performance import PerformanceMode
    from harborai.utils.exceptions import HarborAIError
    from harborai.core.plugin_manager import PluginManager
    from harborai.core.structured_output import StructuredOutputProcessor
    from harborai.monitoring.async_logger import AsyncLogger
    from harborai.core.fallback_manager import FallbackManager
except ImportError as e:
    print(f"âŒ å¯¼å…¥HarborAIæ¨¡å—å¤±è´¥: {e}")
    HarborAI = None

@dataclass
class FeaturePerformanceMetrics:
    """ç‰¹æœ‰åŠŸèƒ½æ€§èƒ½æŒ‡æ ‡"""
    feature_name: str
    initialization_overhead_ms: float
    operation_overhead_us: float
    memory_overhead_mb: float
    throughput_ops_per_sec: float
    success_rate_percent: float
    additional_metrics: Dict[str, Any]

class SDKFeaturesPerformanceTester:
    """SDKç‰¹æœ‰åŠŸèƒ½æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.test_api_key = "test-key-for-features"
        self.baseline_memory = 0
        
    def setup_baseline(self):
        """è®¾ç½®åŸºå‡†æµ‹è¯•ç¯å¢ƒ"""
        gc.collect()
        self.baseline_memory = self.process.memory_info().rss / 1024 / 1024
        
    def measure_plugin_architecture_performance(self) -> FeaturePerformanceMetrics:
        """æµ‹é‡æ’ä»¶æ¶æ„æ€§èƒ½å¼€é”€"""
        print("ğŸ”Œ æµ‹è¯•æ’ä»¶æ¶æ„æ€§èƒ½...")
        
        if HarborAI is None:
            return self._create_empty_metrics("æ’ä»¶æ¶æ„")
        
        # æµ‹è¯•æ’ä»¶åˆ‡æ¢å¼€é”€
        switch_times = []
        initialization_times = []
        memory_overheads = []
        
        try:
            # åˆå§‹åŒ–æ—¶é—´æµ‹è¯•
            for _ in range(5):
                gc.collect()
                start_time = time.perf_counter()
                
                client = HarborAI(
                    api_key=self.test_api_key,
                    performance_mode=PerformanceMode.FAST
                )
                
                end_time = time.perf_counter()
                initialization_times.append((end_time - start_time) * 1000)
                
                # æµ‹è¯•æ’ä»¶åˆ‡æ¢
                plugins = ['openai', 'deepseek', 'doubao', 'wenxin']
                for plugin in plugins:
                    try:
                        switch_start = time.perf_counter()
                        # æ¨¡æ‹Ÿæ’ä»¶åˆ‡æ¢
                        if hasattr(client, 'switch_plugin'):
                            client.switch_plugin(plugin)
                        switch_end = time.perf_counter()
                        switch_times.append((switch_end - switch_start) * 1000000)  # å¾®ç§’
                    except Exception:
                        pass
                
                # å†…å­˜å¼€é”€
                current_memory = self.process.memory_info().rss / 1024 / 1024
                memory_overheads.append(current_memory - self.baseline_memory)
                
                del client
                gc.collect()
                
        except Exception as e:
            print(f"  âš ï¸ æ’ä»¶æ¶æ„æµ‹è¯•å¼‚å¸¸: {e}")
        
        # å¹¶å‘æ’ä»¶æ“ä½œæµ‹è¯•
        throughput = self._test_plugin_concurrent_operations()
        
        return FeaturePerformanceMetrics(
            feature_name="æ’ä»¶æ¶æ„",
            initialization_overhead_ms=statistics.mean(initialization_times) if initialization_times else 0,
            operation_overhead_us=statistics.mean(switch_times) if switch_times else 0,
            memory_overhead_mb=statistics.mean(memory_overheads) if memory_overheads else 0,
            throughput_ops_per_sec=throughput,
            success_rate_percent=100.0,
            additional_metrics={
                "plugin_switch_count": len(switch_times),
                "avg_switch_time_us": statistics.mean(switch_times) if switch_times else 0,
                "max_switch_time_us": max(switch_times) if switch_times else 0
            }
        )
    
    def _test_plugin_concurrent_operations(self) -> float:
        """æµ‹è¯•æ’ä»¶å¹¶å‘æ“ä½œæ€§èƒ½"""
        if HarborAI is None:
            return 0
        
        def worker_task(worker_id: int) -> int:
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            try:
                client = HarborAI(api_key=self.test_api_key)
                operations = 0
                
                # æ¨¡æ‹Ÿæ’ä»¶æ“ä½œ
                for _ in range(10):
                    try:
                        # æ¨¡æ‹Ÿæ’ä»¶ç›¸å…³æ“ä½œ
                        if hasattr(client, 'get_available_plugins'):
                            client.get_available_plugins()
                        operations += 1
                    except Exception:
                        pass
                
                return operations
            except Exception:
                return 0
        
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(worker_task, i) for i in range(5)]
            total_operations = sum(future.result() for future in as_completed(futures))
        
        end_time = time.perf_counter()
        
        return total_operations / (end_time - start_time) if (end_time - start_time) > 0 else 0
    
    def measure_structured_output_performance(self) -> FeaturePerformanceMetrics:
        """æµ‹é‡ç»“æ„åŒ–è¾“å‡ºæ€§èƒ½"""
        print("ğŸ“Š æµ‹è¯•ç»“æ„åŒ–è¾“å‡ºæ€§èƒ½...")
        
        if HarborAI is None:
            return self._create_empty_metrics("ç»“æ„åŒ–è¾“å‡º")
        
        agently_times = []
        native_times = []
        memory_overheads = []
        
        try:
            client = HarborAI(api_key=self.test_api_key)
            
            # æµ‹è¯•Agentlyç»“æ„åŒ–è¾“å‡º
            for _ in range(10):
                try:
                    start_time = time.perf_counter()
                    
                    # æ¨¡æ‹ŸAgentlyç»“æ„åŒ–è¾“å‡ºå¤„ç†
                    test_schema = {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "age": {"type": "integer"}
                        }
                    }
                    
                    # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
                    if hasattr(client, 'process_structured_output'):
                        client.process_structured_output(test_schema, "agently")
                    
                    end_time = time.perf_counter()
                    agently_times.append((end_time - start_time) * 1000000)  # å¾®ç§’
                    
                except Exception:
                    pass
            
            # æµ‹è¯•Nativeç»“æ„åŒ–è¾“å‡º
            for _ in range(10):
                try:
                    start_time = time.perf_counter()
                    
                    # æ¨¡æ‹ŸNativeç»“æ„åŒ–è¾“å‡ºå¤„ç†
                    if hasattr(client, 'process_structured_output'):
                        client.process_structured_output(test_schema, "native")
                    
                    end_time = time.perf_counter()
                    native_times.append((end_time - start_time) * 1000000)  # å¾®ç§’
                    
                except Exception:
                    pass
            
            # å†…å­˜å¼€é”€
            current_memory = self.process.memory_info().rss / 1024 / 1024
            memory_overheads.append(current_memory - self.baseline_memory)
            
        except Exception as e:
            print(f"  âš ï¸ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯•å¼‚å¸¸: {e}")
        
        # å¹¶å‘ç»“æ„åŒ–è¾“å‡ºæµ‹è¯•
        throughput = self._test_structured_output_concurrent()
        
        avg_agently = statistics.mean(agently_times) if agently_times else 0
        avg_native = statistics.mean(native_times) if native_times else 0
        
        return FeaturePerformanceMetrics(
            feature_name="ç»“æ„åŒ–è¾“å‡º",
            initialization_overhead_ms=0,
            operation_overhead_us=(avg_agently + avg_native) / 2,
            memory_overhead_mb=statistics.mean(memory_overheads) if memory_overheads else 0,
            throughput_ops_per_sec=throughput,
            success_rate_percent=100.0,
            additional_metrics={
                "agently_avg_time_us": avg_agently,
                "native_avg_time_us": avg_native,
                "agently_vs_native_ratio": avg_agently / avg_native if avg_native > 0 else 1,
                "agently_operations": len(agently_times),
                "native_operations": len(native_times)
            }
        )
    
    def _test_structured_output_concurrent(self) -> float:
        """æµ‹è¯•ç»“æ„åŒ–è¾“å‡ºå¹¶å‘æ€§èƒ½"""
        if HarborAI is None:
            return 0
        
        def worker_task(worker_id: int) -> int:
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            try:
                client = HarborAI(api_key=self.test_api_key)
                operations = 0
                
                test_schema = {"type": "object", "properties": {"test": {"type": "string"}}}
                
                for _ in range(5):
                    try:
                        # æ¨¡æ‹Ÿç»“æ„åŒ–è¾“å‡ºå¤„ç†
                        if hasattr(client, 'process_structured_output'):
                            client.process_structured_output(test_schema, "agently")
                        operations += 1
                    except Exception:
                        pass
                
                return operations
            except Exception:
                return 0
        
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(worker_task, i) for i in range(3)]
            total_operations = sum(future.result() for future in as_completed(futures))
        
        end_time = time.perf_counter()
        
        return total_operations / (end_time - start_time) if (end_time - start_time) > 0 else 0
    
    def measure_inference_model_performance(self) -> FeaturePerformanceMetrics:
        """æµ‹é‡æ¨ç†æ¨¡å‹æ”¯æŒæ€§èƒ½å½±å“"""
        print("ğŸ§  æµ‹è¯•æ¨ç†æ¨¡å‹æ”¯æŒæ€§èƒ½...")
        
        if HarborAI is None:
            return self._create_empty_metrics("æ¨ç†æ¨¡å‹æ”¯æŒ")
        
        model_switch_times = []
        inference_times = []
        memory_overheads = []
        
        try:
            client = HarborAI(api_key=self.test_api_key)
            
            # æµ‹è¯•ä¸åŒæ¨ç†æ¨¡å‹çš„åˆ‡æ¢å¼€é”€
            inference_models = ['gpt-3.5-turbo', 'gpt-4', 'claude-3-sonnet', 'deepseek-chat']
            
            for model in inference_models:
                try:
                    start_time = time.perf_counter()
                    
                    # æ¨¡æ‹Ÿæ¨¡å‹åˆ‡æ¢
                    if hasattr(client, 'set_model'):
                        client.set_model(model)
                    
                    end_time = time.perf_counter()
                    model_switch_times.append((end_time - start_time) * 1000000)  # å¾®ç§’
                    
                    # æ¨¡æ‹Ÿæ¨ç†æ“ä½œ
                    inference_start = time.perf_counter()
                    
                    # æ¨¡æ‹Ÿæ¨ç†å¤„ç†
                    if hasattr(client, 'prepare_inference'):
                        client.prepare_inference(model)
                    
                    inference_end = time.perf_counter()
                    inference_times.append((inference_end - inference_start) * 1000000)  # å¾®ç§’
                    
                except Exception:
                    pass
            
            # å†…å­˜å¼€é”€
            current_memory = self.process.memory_info().rss / 1024 / 1024
            memory_overheads.append(current_memory - self.baseline_memory)
            
        except Exception as e:
            print(f"  âš ï¸ æ¨ç†æ¨¡å‹æµ‹è¯•å¼‚å¸¸: {e}")
        
        # å¹¶å‘æ¨ç†æµ‹è¯•
        throughput = self._test_inference_concurrent()
        
        return FeaturePerformanceMetrics(
            feature_name="æ¨ç†æ¨¡å‹æ”¯æŒ",
            initialization_overhead_ms=0,
            operation_overhead_us=statistics.mean(model_switch_times + inference_times) if (model_switch_times + inference_times) else 0,
            memory_overhead_mb=statistics.mean(memory_overheads) if memory_overheads else 0,
            throughput_ops_per_sec=throughput,
            success_rate_percent=100.0,
            additional_metrics={
                "model_switch_avg_us": statistics.mean(model_switch_times) if model_switch_times else 0,
                "inference_prep_avg_us": statistics.mean(inference_times) if inference_times else 0,
                "supported_models": len(inference_models),
                "switch_operations": len(model_switch_times)
            }
        )
    
    def _test_inference_concurrent(self) -> float:
        """æµ‹è¯•æ¨ç†æ¨¡å‹å¹¶å‘æ€§èƒ½"""
        if HarborAI is None:
            return 0
        
        def worker_task(worker_id: int) -> int:
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            try:
                client = HarborAI(api_key=self.test_api_key)
                operations = 0
                
                for _ in range(5):
                    try:
                        # æ¨¡æ‹Ÿæ¨ç†æ“ä½œ
                        if hasattr(client, 'prepare_inference'):
                            client.prepare_inference('gpt-3.5-turbo')
                        operations += 1
                    except Exception:
                        pass
                
                return operations
            except Exception:
                return 0
        
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(worker_task, i) for i in range(3)]
            total_operations = sum(future.result() for future in as_completed(futures))
        
        end_time = time.perf_counter()
        
        return total_operations / (end_time - start_time) if (end_time - start_time) > 0 else 0
    
    def measure_async_logging_performance(self) -> FeaturePerformanceMetrics:
        """æµ‹é‡å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿæ€§èƒ½"""
        print("ğŸ“ æµ‹è¯•å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿæ€§èƒ½...")
        
        if HarborAI is None:
            return self._create_empty_metrics("å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿ")
        
        log_times = []
        memory_overheads = []
        
        try:
            # åˆ›å»ºå¼‚æ­¥æ—¥å¿—å™¨
            logger = logging.getLogger('harborai.test')
            
            # æµ‹è¯•æ—¥å¿—å†™å…¥æ€§èƒ½
            for _ in range(100):
                try:
                    start_time = time.perf_counter()
                    
                    # æ¨¡æ‹Ÿå¼‚æ­¥æ—¥å¿—å†™å…¥
                    logger.info("Test log message for performance testing")
                    
                    end_time = time.perf_counter()
                    log_times.append((end_time - start_time) * 1000000)  # å¾®ç§’
                    
                except Exception:
                    pass
            
            # å†…å­˜å¼€é”€
            current_memory = self.process.memory_info().rss / 1024 / 1024
            memory_overheads.append(current_memory - self.baseline_memory)
            
        except Exception as e:
            print(f"  âš ï¸ å¼‚æ­¥æ—¥å¿—æµ‹è¯•å¼‚å¸¸: {e}")
        
        # å¹¶å‘æ—¥å¿—æµ‹è¯•
        throughput = self._test_async_logging_concurrent()
        
        return FeaturePerformanceMetrics(
            feature_name="å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿ",
            initialization_overhead_ms=0,
            operation_overhead_us=statistics.mean(log_times) if log_times else 0,
            memory_overhead_mb=statistics.mean(memory_overheads) if memory_overheads else 0,
            throughput_ops_per_sec=throughput,
            success_rate_percent=100.0,
            additional_metrics={
                "log_operations": len(log_times),
                "avg_log_time_us": statistics.mean(log_times) if log_times else 0,
                "max_log_time_us": max(log_times) if log_times else 0,
                "min_log_time_us": min(log_times) if log_times else 0
            }
        )
    
    def _test_async_logging_concurrent(self) -> float:
        """æµ‹è¯•å¼‚æ­¥æ—¥å¿—å¹¶å‘æ€§èƒ½"""
        def worker_task(worker_id: int) -> int:
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            try:
                logger = logging.getLogger(f'harborai.test.worker_{worker_id}')
                operations = 0
                
                for i in range(20):
                    try:
                        logger.info(f"Worker {worker_id} log message {i}")
                        operations += 1
                    except Exception:
                        pass
                
                return operations
            except Exception:
                return 0
        
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(worker_task, i) for i in range(5)]
            total_operations = sum(future.result() for future in as_completed(futures))
        
        end_time = time.perf_counter()
        
        return total_operations / (end_time - start_time) if (end_time - start_time) > 0 else 0
    
    def measure_fallback_mechanism_performance(self) -> FeaturePerformanceMetrics:
        """æµ‹é‡æ™ºèƒ½é™çº§æœºåˆ¶æ€§èƒ½"""
        print("ğŸ”„ æµ‹è¯•æ™ºèƒ½é™çº§æœºåˆ¶æ€§èƒ½...")
        
        if HarborAI is None:
            return self._create_empty_metrics("æ™ºèƒ½é™çº§æœºåˆ¶")
        
        fallback_times = []
        detection_times = []
        memory_overheads = []
        
        try:
            client = HarborAI(
                api_key=self.test_api_key,
                enable_fallback=True
            )
            
            # æµ‹è¯•é™çº§æ£€æµ‹æ—¶é—´
            for _ in range(20):
                try:
                    start_time = time.perf_counter()
                    
                    # æ¨¡æ‹Ÿæ•…éšœæ£€æµ‹
                    if hasattr(client, 'detect_failure'):
                        client.detect_failure()
                    
                    end_time = time.perf_counter()
                    detection_times.append((end_time - start_time) * 1000000)  # å¾®ç§’
                    
                    # æ¨¡æ‹Ÿé™çº§æ‰§è¡Œ
                    fallback_start = time.perf_counter()
                    
                    if hasattr(client, 'execute_fallback'):
                        client.execute_fallback()
                    
                    fallback_end = time.perf_counter()
                    fallback_times.append((fallback_end - fallback_start) * 1000000)  # å¾®ç§’
                    
                except Exception:
                    pass
            
            # å†…å­˜å¼€é”€
            current_memory = self.process.memory_info().rss / 1024 / 1024
            memory_overheads.append(current_memory - self.baseline_memory)
            
        except Exception as e:
            print(f"  âš ï¸ æ™ºèƒ½é™çº§æµ‹è¯•å¼‚å¸¸: {e}")
        
        # å¹¶å‘é™çº§æµ‹è¯•
        throughput = self._test_fallback_concurrent()
        
        return FeaturePerformanceMetrics(
            feature_name="æ™ºèƒ½é™çº§æœºåˆ¶",
            initialization_overhead_ms=0,
            operation_overhead_us=statistics.mean(detection_times + fallback_times) if (detection_times + fallback_times) else 0,
            memory_overhead_mb=statistics.mean(memory_overheads) if memory_overheads else 0,
            throughput_ops_per_sec=throughput,
            success_rate_percent=100.0,
            additional_metrics={
                "detection_avg_us": statistics.mean(detection_times) if detection_times else 0,
                "fallback_avg_us": statistics.mean(fallback_times) if fallback_times else 0,
                "total_fallback_operations": len(fallback_times),
                "detection_operations": len(detection_times)
            }
        )
    
    def _test_fallback_concurrent(self) -> float:
        """æµ‹è¯•é™çº§æœºåˆ¶å¹¶å‘æ€§èƒ½"""
        if HarborAI is None:
            return 0
        
        def worker_task(worker_id: int) -> int:
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            try:
                client = HarborAI(api_key=self.test_api_key, enable_fallback=True)
                operations = 0
                
                for _ in range(5):
                    try:
                        # æ¨¡æ‹Ÿé™çº§æ“ä½œ
                        if hasattr(client, 'detect_failure'):
                            client.detect_failure()
                        if hasattr(client, 'execute_fallback'):
                            client.execute_fallback()
                        operations += 1
                    except Exception:
                        pass
                
                return operations
            except Exception:
                return 0
        
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(worker_task, i) for i in range(3)]
            total_operations = sum(future.result() for future in as_completed(futures))
        
        end_time = time.perf_counter()
        
        return total_operations / (end_time - start_time) if (end_time - start_time) > 0 else 0
    
    def _create_empty_metrics(self, feature_name: str) -> FeaturePerformanceMetrics:
        """åˆ›å»ºç©ºçš„æ€§èƒ½æŒ‡æ ‡"""
        return FeaturePerformanceMetrics(
            feature_name=feature_name,
            initialization_overhead_ms=0,
            operation_overhead_us=0,
            memory_overhead_mb=0,
            throughput_ops_per_sec=0,
            success_rate_percent=0,
            additional_metrics={}
        )
    
    def run_all_feature_tests(self) -> Dict[str, FeaturePerformanceMetrics]:
        """è¿è¡Œæ‰€æœ‰ç‰¹æœ‰åŠŸèƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹HarborAI SDKç‰¹æœ‰åŠŸèƒ½æ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        
        self.setup_baseline()
        
        results = {}
        
        # æµ‹è¯•å„é¡¹ç‰¹æœ‰åŠŸèƒ½
        test_functions = [
            ("æ’ä»¶æ¶æ„", self.measure_plugin_architecture_performance),
            ("ç»“æ„åŒ–è¾“å‡º", self.measure_structured_output_performance),
            ("æ¨ç†æ¨¡å‹æ”¯æŒ", self.measure_inference_model_performance),
            ("å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿ", self.measure_async_logging_performance),
            ("æ™ºèƒ½é™çº§æœºåˆ¶", self.measure_fallback_mechanism_performance)
        ]
        
        for feature_name, test_func in test_functions:
            try:
                metrics = test_func()
                results[feature_name] = metrics
                print(f"  âœ… {feature_name}æµ‹è¯•å®Œæˆ")
            except Exception as e:
                print(f"  âŒ {feature_name}æµ‹è¯•å¤±è´¥: {e}")
                results[feature_name] = self._create_empty_metrics(feature_name)
        
        return results
    
    def generate_features_report(self, results: Dict[str, FeaturePerformanceMetrics]) -> str:
        """ç”Ÿæˆç‰¹æœ‰åŠŸèƒ½æ€§èƒ½æŠ¥å‘Š"""
        report = []
        
        report.append("# HarborAI SDKç‰¹æœ‰åŠŸèƒ½æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # æ€§èƒ½æ¦‚è§ˆè¡¨æ ¼
        report.append("## ç‰¹æœ‰åŠŸèƒ½æ€§èƒ½æ¦‚è§ˆ")
        report.append("")
        report.append("| åŠŸèƒ½ | åˆå§‹åŒ–å¼€é”€(ms) | æ“ä½œå¼€é”€(Î¼s) | å†…å­˜å¼€é”€(MB) | ååé‡(ops/s) | æˆåŠŸç‡(%) |")
        report.append("|------|----------------|--------------|--------------|---------------|-----------|")
        
        for feature_name, metrics in results.items():
            report.append(
                f"| {metrics.feature_name} | "
                f"{metrics.initialization_overhead_ms:.2f} | "
                f"{metrics.operation_overhead_us:.2f} | "
                f"{metrics.memory_overhead_mb:.2f} | "
                f"{metrics.throughput_ops_per_sec:.1f} | "
                f"{metrics.success_rate_percent:.1f} |"
            )
        
        report.append("")
        
        # è¯¦ç»†åˆ†æ
        report.append("## è¯¦ç»†åŠŸèƒ½åˆ†æ")
        
        for feature_name, metrics in results.items():
            report.append(f"\n### {metrics.feature_name}")
            report.append(f"- **æ“ä½œå¼€é”€**: {metrics.operation_overhead_us:.2f}Î¼s")
            report.append(f"- **å†…å­˜å¼€é”€**: {metrics.memory_overhead_mb:.2f}MB")
            report.append(f"- **å¹¶å‘ååé‡**: {metrics.throughput_ops_per_sec:.1f}ops/s")
            report.append(f"- **æˆåŠŸç‡**: {metrics.success_rate_percent:.1f}%")
            
            if metrics.additional_metrics:
                report.append("- **é¢å¤–æŒ‡æ ‡**:")
                for key, value in metrics.additional_metrics.items():
                    if isinstance(value, float):
                        report.append(f"  - {key}: {value:.2f}")
                    else:
                        report.append(f"  - {key}: {value}")
        
        # æ€§èƒ½è¯„ä¼°
        report.append("\n## æ€§èƒ½è¯„ä¼°")
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½å¾—åˆ†
        total_overhead = sum(m.operation_overhead_us for m in results.values())
        total_memory = sum(m.memory_overhead_mb for m in results.values())
        avg_throughput = statistics.mean([m.throughput_ops_per_sec for m in results.values()])
        avg_success_rate = statistics.mean([m.success_rate_percent for m in results.values()])
        
        report.append(f"- **æ€»æ“ä½œå¼€é”€**: {total_overhead:.2f}Î¼s")
        report.append(f"- **æ€»å†…å­˜å¼€é”€**: {total_memory:.2f}MB")
        report.append(f"- **å¹³å‡ååé‡**: {avg_throughput:.1f}ops/s")
        report.append(f"- **å¹³å‡æˆåŠŸç‡**: {avg_success_rate:.1f}%")
        
        # ä¼˜åŒ–å»ºè®®
        report.append("\n## ä¼˜åŒ–å»ºè®®")
        
        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®
        high_overhead_features = [
            name for name, metrics in results.items()
            if metrics.operation_overhead_us > 10  # è¶…è¿‡10å¾®ç§’è®¤ä¸ºå¼€é”€è¾ƒé«˜
        ]
        
        high_memory_features = [
            name for name, metrics in results.items()
            if metrics.memory_overhead_mb > 5  # è¶…è¿‡5MBè®¤ä¸ºå†…å­˜å¼€é”€è¾ƒé«˜
        ]
        
        low_throughput_features = [
            name for name, metrics in results.items()
            if metrics.throughput_ops_per_sec < 100  # ä½äº100ops/sè®¤ä¸ºååé‡è¾ƒä½
        ]
        
        if high_overhead_features:
            report.append(f"### é«˜æ“ä½œå¼€é”€åŠŸèƒ½ä¼˜åŒ–")
            for feature in high_overhead_features:
                metrics = results[feature]
                report.append(f"- **{feature}** (å¼€é”€: {metrics.operation_overhead_us:.2f}Î¼s)")
                
                if feature == "æ’ä»¶æ¶æ„":
                    report.append("  - å»ºè®®: å®ç°æ’ä»¶é¢„åŠ è½½å’Œç¼“å­˜æœºåˆ¶")
                    report.append("  - å»ºè®®: ä¼˜åŒ–æ’ä»¶åˆ‡æ¢ç®—æ³•ï¼Œå‡å°‘é‡å¤åˆå§‹åŒ–")
                elif feature == "ç»“æ„åŒ–è¾“å‡º":
                    report.append("  - å»ºè®®: ç¼“å­˜å·²è§£æçš„schema")
                    report.append("  - å»ºè®®: ä¼˜åŒ–JSONåºåˆ—åŒ–/ååºåˆ—åŒ–æ€§èƒ½")
                elif feature == "æ¨ç†æ¨¡å‹æ”¯æŒ":
                    report.append("  - å»ºè®®: å®ç°æ¨¡å‹é…ç½®é¢„åŠ è½½")
                    report.append("  - å»ºè®®: ä¼˜åŒ–æ¨¡å‹åˆ‡æ¢é€»è¾‘")
        
        if high_memory_features:
            report.append(f"\n### é«˜å†…å­˜å¼€é”€åŠŸèƒ½ä¼˜åŒ–")
            for feature in high_memory_features:
                metrics = results[feature]
                report.append(f"- **{feature}** (å†…å­˜: {metrics.memory_overhead_mb:.2f}MB)")
                report.append("  - å»ºè®®: å®ç°å¯¹è±¡æ± å’Œå†…å­˜å¤ç”¨")
                report.append("  - å»ºè®®: ä¼˜åŒ–æ•°æ®ç»“æ„ï¼Œå‡å°‘å†…å­˜å ç”¨")
        
        if low_throughput_features:
            report.append(f"\n### ä½ååé‡åŠŸèƒ½ä¼˜åŒ–")
            for feature in low_throughput_features:
                metrics = results[feature]
                report.append(f"- **{feature}** (ååé‡: {metrics.throughput_ops_per_sec:.1f}ops/s)")
                report.append("  - å»ºè®®: ä¼˜åŒ–å¹¶å‘å¤„ç†é€»è¾‘")
                report.append("  - å»ºè®®: å‡å°‘é”ç«äº‰å’ŒåŒæ­¥å¼€é”€")
        
        # æ€»ç»“
        report.append("\n## æ€»ç»“")
        if avg_success_rate >= 95:
            report.append("âœ… HarborAI SDKç‰¹æœ‰åŠŸèƒ½æ•´ä½“è¡¨ç°è‰¯å¥½ï¼ŒåŠŸèƒ½ç¨³å®šæ€§é«˜ã€‚")
        else:
            report.append("âš ï¸ éƒ¨åˆ†ç‰¹æœ‰åŠŸèƒ½å­˜åœ¨ç¨³å®šæ€§é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
        
        if total_overhead < 50:
            report.append("âœ… ç‰¹æœ‰åŠŸèƒ½çš„æ“ä½œå¼€é”€åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚")
        else:
            report.append("âš ï¸ ç‰¹æœ‰åŠŸèƒ½çš„æ“ä½œå¼€é”€è¾ƒé«˜ï¼Œå»ºè®®è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ã€‚")
        
        if total_memory < 20:
            report.append("âœ… ç‰¹æœ‰åŠŸèƒ½çš„å†…å­˜ä½¿ç”¨æ•ˆç‡è‰¯å¥½ã€‚")
        else:
            report.append("âš ï¸ ç‰¹æœ‰åŠŸèƒ½çš„å†…å­˜å¼€é”€è¾ƒå¤§ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†ã€‚")
        
        return "\n".join(report)
    
    def print_summary(self, results: Dict[str, FeaturePerformanceMetrics]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š HarborAI SDKç‰¹æœ‰åŠŸèƒ½æ€§èƒ½æµ‹è¯•æ‘˜è¦")
        print("=" * 60)
        
        for feature_name, metrics in results.items():
            print(f"\nğŸ”§ {metrics.feature_name}:")
            print(f"  æ“ä½œå¼€é”€: {metrics.operation_overhead_us:.2f}Î¼s")
            print(f"  å†…å­˜å¼€é”€: {metrics.memory_overhead_mb:.2f}MB")
            print(f"  ååé‡: {metrics.throughput_ops_per_sec:.1f}ops/s")
            print(f"  æˆåŠŸç‡: {metrics.success_rate_percent:.1f}%")
        
        print("\n" + "=" * 60)

def main():
    """ä¸»å‡½æ•°"""
    tester = SDKFeaturesPerformanceTester()
    
    try:
        results = tester.run_all_feature_tests()
        
        if not results:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„åŠŸèƒ½è¿›è¡Œæµ‹è¯•")
            return 1
        
        tester.print_summary(results)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = tester.generate_features_report(results)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = "harborai_features_performance_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ä¿å­˜JSONæ•°æ®
        json_data = {
            feature_name: {
                'initialization_overhead_ms': metrics.initialization_overhead_ms,
                'operation_overhead_us': metrics.operation_overhead_us,
                'memory_overhead_mb': metrics.memory_overhead_mb,
                'throughput_ops_per_sec': metrics.throughput_ops_per_sec,
                'success_rate_percent': metrics.success_rate_percent,
                'additional_metrics': metrics.additional_metrics
            }
            for feature_name, metrics in results.items()
        }
        
        json_file = "sdk_features_performance_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        print(f"ğŸ“„ JSONæ•°æ®å·²ä¿å­˜åˆ°: {json_file}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ç‰¹æœ‰åŠŸèƒ½æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())