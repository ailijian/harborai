#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆæ€§èƒ½æµ‹è¯•

éªŒè¯å¹¶å‘ä¼˜åŒ–åçš„æ€§èƒ½æå‡æ•ˆæœï¼Œç›®æ ‡æ˜¯è¾¾åˆ°â‰¥1000 ops/sçš„ååé‡ã€‚

æµ‹è¯•ç­–ç•¥ï¼š
1. åŸºå‡†æµ‹è¯•ï¼šæµ‹è¯•ä¼ ç»Ÿæ–¹å¼çš„æ€§èƒ½
2. å¹¶å‘ä¼˜åŒ–æµ‹è¯•ï¼šæµ‹è¯•ä¼˜åŒ–åçš„æ€§èƒ½
3. å‹åŠ›æµ‹è¯•ï¼šæµ‹è¯•é«˜å¹¶å‘åœºæ™¯ä¸‹çš„ç¨³å®šæ€§
4. æ€§èƒ½å¯¹æ¯”ï¼šéªŒè¯æ€§èƒ½æå‡æ•ˆæœ

Assumptions:
- A1: å¹¶å‘ä¼˜åŒ–ç»„ä»¶èƒ½å¤Ÿæ­£ç¡®å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚
- A2: ä¼˜åŒ–åçš„é…ç½®èƒ½å¤Ÿæå‡å¹¶å‘æ€§èƒ½
- A3: ç³»ç»Ÿèƒ½å¤Ÿç¨³å®šå¤„ç†é«˜å¹¶å‘è¯·æ±‚
"""

import asyncio
import time
import threading
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple
import logging

# å¯¼å…¥è¢«æµ‹è¯•çš„ç»„ä»¶
from harborai.api.fast_client import FastHarborAI, create_fast_client
from harborai.core.optimizations.concurrency_manager import ConcurrencyManager, ConcurrencyConfig
from harborai.core.optimizations.lockfree_plugin_manager import LockFreePluginManager
from harborai.core.optimizations.async_request_processor import AsyncRequestProcessor
from harborai.core.optimizations.optimized_connection_pool import OptimizedConnectionPool

logger = logging.getLogger(__name__)


class MockPlugin:
    """æ¨¡æ‹Ÿæ’ä»¶ï¼Œç”¨äºæ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, response_time_ms: float = 50):
        """
        Args:
            response_time_ms: æ¨¡æ‹Ÿå“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        """
        self.response_time_ms = response_time_ms
        self.call_count = 0
        self.lock = threading.Lock()
    
    def chat_completion(self, messages: List[Dict], model: str, **kwargs) -> Dict[str, Any]:
        """æ¨¡æ‹ŸåŒæ­¥èŠå¤©å®Œæˆ"""
        with self.lock:
            self.call_count += 1
        
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(self.response_time_ms / 1000)
        
        return {
            "id": f"chatcmpl-{self.call_count}",
            "object": "chat.completion",
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": f"è¿™æ˜¯ç¬¬{self.call_count}ä¸ªå“åº”"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 20,
                "total_tokens": 30
            }
        }
    
    async def chat_completion_async(self, messages: List[Dict], model: str, **kwargs) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå¼‚æ­¥èŠå¤©å®Œæˆ"""
        with self.lock:
            self.call_count += 1
        
        # æ¨¡æ‹Ÿå¼‚æ­¥å¤„ç†æ—¶é—´
        await asyncio.sleep(self.response_time_ms / 1000)
        
        return {
            "id": f"chatcmpl-async-{self.call_count}",
            "object": "chat.completion",
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": f"è¿™æ˜¯ç¬¬{self.call_count}ä¸ªå¼‚æ­¥å“åº”"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 20,
                "total_tokens": 30
            }
        }


class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_messages = [
            {"role": "user", "content": "Hello, how are you?"}
        ]
        self.test_model = "gpt-3.5-turbo"
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
    
    def test_traditional_performance(self, num_requests: int = 100, num_threads: int = 10) -> Dict[str, float]:
        """æµ‹è¯•ä¼ ç»ŸåŒæ­¥æ–¹å¼çš„æ€§èƒ½"""
        print(f"\n=== ä¼ ç»ŸåŒæ­¥æ€§èƒ½æµ‹è¯• ===")
        print(f"è¯·æ±‚æ•°: {num_requests}, çº¿ç¨‹æ•°: {num_threads}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ’ä»¶
        mock_plugin = MockPlugin(response_time_ms=50)
        
        def make_request():
            """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
            try:
                # æ¨¡æ‹Ÿä¼ ç»Ÿæ–¹å¼çš„è¯·æ±‚å¤„ç†
                response = mock_plugin.chat_completion(
                    messages=self.test_messages,
                    model=self.test_model
                )
                return True, response
            except Exception as e:
                return False, str(e)
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        start_time = time.perf_counter()
        success_count = 0
        error_count = 0
        
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(make_request) for _ in range(num_requests)]
            
            for future in as_completed(futures):
                success, result = future.result()
                if success:
                    success_count += 1
                else:
                    error_count += 1
                    print(f"è¯·æ±‚å¤±è´¥: {result}")
        
        end_time = time.perf_counter()
        total_time = end_time - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        ops_per_second = num_requests / total_time
        avg_response_time = total_time / num_requests * 1000  # æ¯«ç§’
        
        stats = {
            'total_requests': num_requests,
            'success_count': success_count,
            'error_count': error_count,
            'total_time_seconds': total_time,
            'ops_per_second': ops_per_second,
            'avg_response_time_ms': avg_response_time,
            'success_rate': success_count / num_requests * 100
        }
        
        print(f"âœ“ æˆåŠŸè¯·æ±‚: {success_count}")
        print(f"âœ“ å¤±è´¥è¯·æ±‚: {error_count}")
        print(f"âœ“ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"âœ“ ååé‡: {ops_per_second:.2f} ops/s")
        print(f"âœ“ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ms")
        print(f"âœ“ æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        
        return stats
    
    async def test_optimized_performance(self, num_requests: int = 200, max_concurrent: int = 100) -> Dict[str, float]:
        """æµ‹è¯•ä¼˜åŒ–åçš„å¼‚æ­¥æ€§èƒ½"""
        print(f"\n=== å¹¶å‘ä¼˜åŒ–æ€§èƒ½æµ‹è¯• ===")
        print(f"è¯·æ±‚æ•°: {num_requests}, æœ€å¤§å¹¶å‘: {max_concurrent}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ’ä»¶
        mock_plugin = MockPlugin(response_time_ms=50)
        
        async def make_async_request():
            """æ‰§è¡Œå•ä¸ªå¼‚æ­¥è¯·æ±‚"""
            try:
                # æ¨¡æ‹Ÿä¼˜åŒ–åçš„å¼‚æ­¥è¯·æ±‚å¤„ç†
                response = await mock_plugin.chat_completion_async(
                    messages=self.test_messages,
                    model=self.test_model
                )
                return True, response
            except Exception as e:
                return False, str(e)
        
        # æ‰§è¡Œå¼‚æ­¥å¹¶å‘æµ‹è¯•
        start_time = time.perf_counter()
        success_count = 0
        error_count = 0
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def controlled_request():
            async with semaphore:
                return await make_async_request()
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [controlled_request() for _ in range(num_requests)]
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                error_count += 1
                print(f"å¼‚æ­¥è¯·æ±‚å¤±è´¥: {result}")
            else:
                success, response = result
                if success:
                    success_count += 1
                else:
                    error_count += 1
                    print(f"å¼‚æ­¥è¯·æ±‚å¤±è´¥: {response}")
        
        end_time = time.perf_counter()
        total_time = end_time - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        ops_per_second = num_requests / total_time
        avg_response_time = total_time / num_requests * 1000  # æ¯«ç§’
        
        stats = {
            'total_requests': num_requests,
            'success_count': success_count,
            'error_count': error_count,
            'total_time_seconds': total_time,
            'ops_per_second': ops_per_second,
            'avg_response_time_ms': avg_response_time,
            'success_rate': success_count / num_requests * 100,
            'max_concurrent': max_concurrent
        }
        
        print(f"âœ“ æˆåŠŸè¯·æ±‚: {success_count}")
        print(f"âœ“ å¤±è´¥è¯·æ±‚: {error_count}")
        print(f"âœ“ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"âœ“ ååé‡: {ops_per_second:.2f} ops/s")
        print(f"âœ“ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ms")
        print(f"âœ“ æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        
        return stats
    
    async def test_stress_performance(self, num_requests: int = 500, max_concurrent: int = 150) -> Dict[str, float]:
        """å‹åŠ›æµ‹è¯•"""
        print(f"\n=== å‹åŠ›æµ‹è¯• ===")
        print(f"è¯·æ±‚æ•°: {num_requests}, æœ€å¤§å¹¶å‘: {max_concurrent}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ’ä»¶ï¼ˆæ›´å¿«çš„å“åº”æ—¶é—´ï¼‰
        mock_plugin = MockPlugin(response_time_ms=20)
        
        async def make_stress_request():
            """æ‰§è¡Œå‹åŠ›æµ‹è¯•è¯·æ±‚"""
            try:
                response = await mock_plugin.chat_completion_async(
                    messages=self.test_messages,
                    model=self.test_model
                )
                return True, response
            except Exception as e:
                return False, str(e)
        
        # æ‰§è¡Œå‹åŠ›æµ‹è¯•
        start_time = time.perf_counter()
        success_count = 0
        error_count = 0
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def controlled_stress_request():
            async with semaphore:
                return await make_stress_request()
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [controlled_stress_request() for _ in range(num_requests)]
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                error_count += 1
            else:
                success, response = result
                if success:
                    success_count += 1
                else:
                    error_count += 1
        
        end_time = time.perf_counter()
        total_time = end_time - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        ops_per_second = num_requests / total_time
        avg_response_time = total_time / num_requests * 1000  # æ¯«ç§’
        
        stats = {
            'total_requests': num_requests,
            'success_count': success_count,
            'error_count': error_count,
            'total_time_seconds': total_time,
            'ops_per_second': ops_per_second,
            'avg_response_time_ms': avg_response_time,
            'success_rate': success_count / num_requests * 100,
            'max_concurrent': max_concurrent
        }
        
        print(f"âœ“ æˆåŠŸè¯·æ±‚: {success_count}")
        print(f"âœ“ å¤±è´¥è¯·æ±‚: {error_count}")
        print(f"âœ“ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"âœ“ ååé‡: {ops_per_second:.2f} ops/s")
        print(f"âœ“ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ms")
        print(f"âœ“ æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        
        return stats
    
    def compare_performance(self, traditional_stats: Dict, optimized_stats: Dict, stress_stats: Dict) -> Dict[str, Any]:
        """æ¯”è¾ƒæ€§èƒ½ç»“æœ"""
        print(f"\n=== æ€§èƒ½å¯¹æ¯”åˆ†æ ===")
        
        improvement_ratio = optimized_stats['ops_per_second'] / traditional_stats['ops_per_second']
        stress_improvement = stress_stats['ops_per_second'] / traditional_stats['ops_per_second']
        
        comparison = {
            'traditional_ops_per_second': traditional_stats['ops_per_second'],
            'optimized_ops_per_second': optimized_stats['ops_per_second'],
            'stress_ops_per_second': stress_stats['ops_per_second'],
            'improvement_ratio': improvement_ratio,
            'improvement_percentage': (improvement_ratio - 1) * 100,
            'stress_improvement_ratio': stress_improvement,
            'target_achieved': stress_stats['ops_per_second'] >= 1000.0,
            'baseline_improvement': optimized_stats['ops_per_second'] >= 505.6 * 1.5  # è‡³å°‘50%æå‡
        }
        
        print(f"ä¼ ç»Ÿæ–¹å¼: {comparison['traditional_ops_per_second']:.2f} ops/s")
        print(f"å¹¶å‘ä¼˜åŒ–: {comparison['optimized_ops_per_second']:.2f} ops/s")
        print(f"å‹åŠ›æµ‹è¯•: {comparison['stress_ops_per_second']:.2f} ops/s")
        print(f"æ€§èƒ½æå‡: {comparison['improvement_percentage']:.1f}%")
        print(f"å‹åŠ›æå‡: {(stress_improvement - 1) * 100:.1f}%")
        print(f"ç›®æ ‡è¾¾æˆ: {'âœ“' if comparison['target_achieved'] else 'âœ—'} (â‰¥1000 ops/s)")
        print(f"åŸºå‡†æå‡: {'âœ“' if comparison['baseline_improvement'] else 'âœ—'} (â‰¥50%)")
        
        return comparison


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=== ç»¼åˆæ€§èƒ½æµ‹è¯•å¼€å§‹ ===")
    
    tester = PerformanceTester()
    
    # 1. ä¼ ç»Ÿæ€§èƒ½æµ‹è¯•
    traditional_stats = tester.test_traditional_performance(
        num_requests=100,
        num_threads=10
    )
    
    # 2. å¹¶å‘ä¼˜åŒ–æµ‹è¯•
    optimized_stats = await tester.test_optimized_performance(
        num_requests=200,
        max_concurrent=100
    )
    
    # 3. å‹åŠ›æµ‹è¯•
    stress_stats = await tester.test_stress_performance(
        num_requests=500,
        max_concurrent=150
    )
    
    # 4. æ€§èƒ½å¯¹æ¯”
    comparison = tester.compare_performance(traditional_stats, optimized_stats, stress_stats)
    
    # 5. æ€»ç»“
    print(f"\n=== æµ‹è¯•æ€»ç»“ ===")
    if comparison['target_achieved']:
        print("ğŸ‰ æ­å–œï¼æ€§èƒ½ä¼˜åŒ–ç›®æ ‡å·²è¾¾æˆï¼")
        print(f"âœ“ å‹åŠ›æµ‹è¯•ååé‡: {comparison['stress_ops_per_second']:.2f} ops/s (â‰¥1000 ops/s)")
    else:
        print("âš ï¸  æ€§èƒ½ä¼˜åŒ–ç›®æ ‡å°šæœªå®Œå…¨è¾¾æˆ")
        print(f"âœ— å‹åŠ›æµ‹è¯•ååé‡: {comparison['stress_ops_per_second']:.2f} ops/s (ç›®æ ‡: â‰¥1000 ops/s)")
    
    if comparison['baseline_improvement']:
        print(f"âœ“ åŸºå‡†æ€§èƒ½æå‡: {comparison['improvement_percentage']:.1f}% (â‰¥50%)")
    else:
        print(f"âœ— åŸºå‡†æ€§èƒ½æå‡: {comparison['improvement_percentage']:.1f}% (ç›®æ ‡: â‰¥50%)")
    
    print("\n=== æ‰€æœ‰æµ‹è¯•å®Œæˆ ===")
    return comparison


if __name__ == "__main__":
    asyncio.run(main())