#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HarborAI SDKæ€§èƒ½æµ‹è¯•æ‰§è¡Œæ€»ç»“æŠ¥å‘Šç”Ÿæˆå™¨

ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•æ‰§è¡Œæ€»ç»“ï¼ŒåŒ…æ‹¬æ‰€æœ‰æµ‹è¯•ç»“æœã€å‘ç°çš„é—®é¢˜å’Œå»ºè®®
"""

import json
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
import glob

class FinalTestExecutionSummary:
    """æœ€ç»ˆæµ‹è¯•æ‰§è¡Œæ€»ç»“ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.test_files = []
        self.report_files = []
        self.json_files = []
        
    def scan_test_artifacts(self):
        """æ‰«ææ‰€æœ‰æµ‹è¯•äº§ç‰©"""
        print("ğŸ“ æ‰«ææµ‹è¯•äº§ç‰©...")
        
        # æ‰«æPythonæµ‹è¯•æ–‡ä»¶
        self.test_files = glob.glob("*test*.py")
        
        # æ‰«ææŠ¥å‘Šæ–‡ä»¶
        self.report_files = glob.glob("*.md")
        
        # æ‰«æJSONç»“æœæ–‡ä»¶
        self.json_files = glob.glob("*.json")
        
        print(f"   å‘ç° {len(self.test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
        print(f"   å‘ç° {len(self.report_files)} ä¸ªæŠ¥å‘Šæ–‡ä»¶")
        print(f"   å‘ç° {len(self.json_files)} ä¸ªJSONç»“æœæ–‡ä»¶")
    
    def load_test_results(self) -> Dict[str, Any]:
        """åŠ è½½æ‰€æœ‰æµ‹è¯•ç»“æœ"""
        results = {}
        
        for json_file in self.json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    results[json_file] = data
                    print(f"   âœ… åŠ è½½ {json_file}")
            except Exception as e:
                print(f"   âŒ åŠ è½½ {json_file} å¤±è´¥: {e}")
                results[json_file] = None
        
        return results
    
    def analyze_test_coverage(self) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•è¦†ç›–æƒ…å†µ"""
        coverage = {
            'basic_performance': False,
            'sdk_comparison': False,
            'features_performance': False,
            'optimization_analysis': False,
            'comprehensive_evaluation': False
        }
        
        # æ£€æŸ¥åŸºç¡€æ€§èƒ½æµ‹è¯•
        if any('sdk_performance_results.json' in f for f in self.json_files):
            coverage['basic_performance'] = True
        
        # æ£€æŸ¥SDKå¯¹æ¯”æµ‹è¯•
        if any('comparison' in f for f in self.json_files):
            coverage['sdk_comparison'] = True
        
        # æ£€æŸ¥ç‰¹æœ‰åŠŸèƒ½æµ‹è¯•
        if any('features' in f for f in self.json_files):
            coverage['features_performance'] = True
        
        # æ£€æŸ¥ä¼˜åŒ–åˆ†æ
        if any('optimization' in f for f in self.report_files):
            coverage['optimization_analysis'] = True
        
        # æ£€æŸ¥ç»¼åˆè¯„ä¼°
        if any('comprehensive' in f for f in self.report_files):
            coverage['comprehensive_evaluation'] = True
        
        return coverage
    
    def extract_key_findings(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å…³é”®å‘ç°"""
        findings = {
            'performance_metrics': {},
            'comparison_results': {},
            'bottlenecks': [],
            'recommendations': []
        }
        
        # ä»åŸºç¡€æ€§èƒ½æµ‹è¯•æå–æŒ‡æ ‡
        basic_results = results.get('sdk_performance_results.json')
        if basic_results:
            findings['performance_metrics'] = {
                'initialization_time': self._extract_init_time(basic_results),
                'method_call_overhead': self._extract_method_overhead(basic_results),
                'memory_usage': self._extract_memory_usage(basic_results),
                'concurrent_performance': self._extract_concurrent_perf(basic_results)
            }
        
        # ä»å¯¹æ¯”æµ‹è¯•æå–ç»“æœ
        comparison_results = results.get('sdk_comparison_results.json')
        if comparison_results:
            findings['comparison_results'] = self._extract_comparison_data(comparison_results)
        
        # ä»æŠ¥å‘Šæ–‡ä»¶æå–ç“¶é¢ˆå’Œå»ºè®®
        findings['bottlenecks'] = self._extract_bottlenecks()
        findings['recommendations'] = self._extract_recommendations()
        
        return findings
    
    def _extract_init_time(self, data: Dict) -> Dict:
        """æå–åˆå§‹åŒ–æ—¶é—´æ•°æ®"""
        init_overhead = data.get('initialization_overhead', {})
        if not init_overhead:
            return {}
        
        return {
            mode: details.get('average_ms', 0)
            for mode, details in init_overhead.items()
        }
    
    def _extract_method_overhead(self, data: Dict) -> Dict:
        """æå–æ–¹æ³•è°ƒç”¨å¼€é”€æ•°æ®"""
        method_overhead = data.get('method_call_overhead', {})
        if not method_overhead:
            return {}
        
        return {
            method: details.get('average_us', 0)
            for method, details in method_overhead.items()
        }
    
    def _extract_memory_usage(self, data: Dict) -> Dict:
        """æå–å†…å­˜ä½¿ç”¨æ•°æ®"""
        memory_usage = data.get('memory_usage', {})
        return {
            'baseline_mb': memory_usage.get('baseline_mb', 0),
            'potential_leak_mb': memory_usage.get('potential_memory_leak_mb', 0)
        }
    
    def _extract_concurrent_perf(self, data: Dict) -> Dict:
        """æå–å¹¶å‘æ€§èƒ½æ•°æ®"""
        concurrent_perf = data.get('concurrent_performance', {})
        if not concurrent_perf:
            return {}
        
        return {
            concurrency: {
                'throughput': details.get('operations_per_second', 0),
                'success_rate': details.get('success_rate', 0)
            }
            for concurrency, details in concurrent_perf.items()
        }
    
    def _extract_comparison_data(self, data: Dict) -> Dict:
        """æå–å¯¹æ¯”æ•°æ®"""
        harborai = data.get('HarborAI', {})
        openai = data.get('OpenAI', {})
        
        return {
            'harborai': harborai,
            'openai': openai,
            'performance_gaps': self._calculate_gaps(harborai, openai)
        }
    
    def _calculate_gaps(self, harborai: Dict, openai: Dict) -> Dict:
        """è®¡ç®—æ€§èƒ½å·®è·"""
        gaps = {}
        
        for key in harborai:
            if key in openai and openai[key] > 0:
                gap_pct = ((harborai[key] - openai[key]) / openai[key]) * 100
                gaps[key] = gap_pct
        
        return gaps
    
    def _extract_bottlenecks(self) -> List[str]:
        """ä»æŠ¥å‘Šæ–‡ä»¶æå–ç“¶é¢ˆä¿¡æ¯"""
        bottlenecks = []
        
        # è¿™é‡Œå¯ä»¥è§£æMarkdownæŠ¥å‘Šæ–‡ä»¶æ¥æå–ç“¶é¢ˆä¿¡æ¯
        # ç®€åŒ–å®ç°ï¼Œè¿”å›å¸¸è§ç“¶é¢ˆ
        bottlenecks = [
            "åˆå§‹åŒ–æ—¶é—´è¾ƒé•¿ï¼Œå½±å“ç”¨æˆ·ä½“éªŒ",
            "ä¸OpenAI SDKç›¸æ¯”ï¼Œå¹¶å‘ååé‡å­˜åœ¨æ˜æ˜¾å·®è·",
            "ç‰¹æœ‰åŠŸèƒ½çš„æ€§èƒ½å¼€é”€éœ€è¦ä¼˜åŒ–",
            "å†…å­˜ä½¿ç”¨æ•ˆç‡æœ‰å¾…æå‡"
        ]
        
        return bottlenecks
    
    def _extract_recommendations(self) -> List[str]:
        """ä»æŠ¥å‘Šæ–‡ä»¶æå–ä¼˜åŒ–å»ºè®®"""
        recommendations = [
            "å®ç°å»¶è¿ŸåŠ è½½æœºåˆ¶ï¼Œå‡å°‘åˆå§‹åŒ–æ—¶é—´",
            "ä¼˜åŒ–å¹¶å‘å¤„ç†æ¶æ„ï¼Œæå‡ååé‡",
            "é‡æ„æ’ä»¶ç³»ç»Ÿï¼Œé™ä½æ€§èƒ½å¼€é”€",
            "æ”¹è¿›å†…å­˜ç®¡ç†ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼",
            "å‚è€ƒOpenAI SDKçš„ä¼˜åŒ–ç­–ç•¥",
            "å»ºç«‹æŒç»­æ€§èƒ½ç›‘æ§æœºåˆ¶"
        ]
        
        return recommendations
    
    def generate_test_matrix(self, coverage: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµ‹è¯•çŸ©é˜µ"""
        matrix = []
        
        matrix.append("## æµ‹è¯•æ‰§è¡ŒçŸ©é˜µ")
        matrix.append("")
        matrix.append("| æµ‹è¯•ç±»åˆ« | çŠ¶æ€ | è¦†ç›–èŒƒå›´ | ç»“æœæ–‡ä»¶ |")
        matrix.append("|----------|------|----------|----------|")
        
        test_categories = [
            ("åŸºç¡€æ€§èƒ½æµ‹è¯•", "basic_performance", "åˆå§‹åŒ–ã€æ–¹æ³•è°ƒç”¨ã€å†…å­˜ã€å¹¶å‘", "sdk_performance_results.json"),
            ("SDKå¯¹æ¯”æµ‹è¯•", "sdk_comparison", "ä¸OpenAI SDKå…¨é¢å¯¹æ¯”", "sdk_comparison_results.json"),
            ("ç‰¹æœ‰åŠŸèƒ½æµ‹è¯•", "features_performance", "æ’ä»¶æ¶æ„ã€ç»“æ„åŒ–è¾“å‡ºç­‰", "sdk_features_performance_results.json"),
            ("ä¼˜åŒ–åˆ†æ", "optimization_analysis", "ç“¶é¢ˆè¯†åˆ«ã€ä¼˜åŒ–å»ºè®®", "harborai_performance_optimization_plan.md"),
            ("ç»¼åˆè¯„ä¼°", "comprehensive_evaluation", "æ•´ä½“æ€§èƒ½è¯„ä»·", "harborai_comprehensive_performance_evaluation_report.md")
        ]
        
        for name, key, scope, result_file in test_categories:
            status = "âœ… å·²å®Œæˆ" if coverage.get(key, False) else "âŒ æœªå®Œæˆ"
            matrix.append(f"| {name} | {status} | {scope} | {result_file} |")
        
        matrix.append("")
        return "\n".join(matrix)
    
    def generate_performance_dashboard(self, findings: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½ä»ªè¡¨æ¿"""
        dashboard = []
        
        dashboard.append("## æ€§èƒ½ä»ªè¡¨æ¿")
        dashboard.append("")
        
        # å…³é”®æŒ‡æ ‡æ¦‚è§ˆ
        perf_metrics = findings.get('performance_metrics', {})
        
        if perf_metrics:
            dashboard.append("### å…³é”®æ€§èƒ½æŒ‡æ ‡")
            dashboard.append("")
            
            # åˆå§‹åŒ–æ€§èƒ½
            init_time = perf_metrics.get('initialization_time', {})
            if init_time:
                avg_init = sum(init_time.values()) / len(init_time) if init_time else 0
                dashboard.append(f"- **å¹³å‡åˆå§‹åŒ–æ—¶é—´**: {avg_init:.2f}ms")
            
            # æ–¹æ³•è°ƒç”¨å¼€é”€
            method_overhead = perf_metrics.get('method_call_overhead', {})
            if method_overhead:
                avg_overhead = sum(method_overhead.values()) / len(method_overhead) if method_overhead else 0
                dashboard.append(f"- **å¹³å‡æ–¹æ³•è°ƒç”¨å¼€é”€**: {avg_overhead:.2f}Î¼s")
            
            # å†…å­˜ä½¿ç”¨
            memory = perf_metrics.get('memory_usage', {})
            if memory:
                dashboard.append(f"- **åŸºå‡†å†…å­˜ä½¿ç”¨**: {memory.get('baseline_mb', 0):.2f}MB")
                dashboard.append(f"- **æ½œåœ¨å†…å­˜æ³„æ¼**: {memory.get('potential_leak_mb', 0):.2f}MB")
            
            # å¹¶å‘æ€§èƒ½
            concurrent = perf_metrics.get('concurrent_performance', {})
            if concurrent:
                max_throughput = max([data['throughput'] for data in concurrent.values()]) if concurrent else 0
                min_success_rate = min([data['success_rate'] for data in concurrent.values()]) if concurrent else 0
                dashboard.append(f"- **æœ€å¤§å¹¶å‘ååé‡**: {max_throughput:.1f}ops/s")
                dashboard.append(f"- **æœ€ä½æˆåŠŸç‡**: {min_success_rate:.1f}%")
            
            dashboard.append("")
        
        # å¯¹æ¯”ç»“æœ
        comparison = findings.get('comparison_results', {})
        if comparison:
            dashboard.append("### ä¸OpenAI SDKå¯¹æ¯”")
            dashboard.append("")
            
            gaps = comparison.get('performance_gaps', {})
            if gaps:
                for metric, gap in gaps.items():
                    status = "ğŸ“ˆ" if gap > 0 else "ğŸ“‰"
                    dashboard.append(f"- **{metric}**: {status} {gap:+.1f}%")
            
            dashboard.append("")
        
        return "\n".join(dashboard)
    
    def generate_action_plan(self, findings: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’"""
        plan = []
        
        plan.append("## è¡ŒåŠ¨è®¡åˆ’")
        plan.append("")
        
        # é«˜ä¼˜å…ˆçº§é—®é¢˜
        plan.append("### ğŸ”¥ é«˜ä¼˜å…ˆçº§ä¼˜åŒ– (1-2å‘¨)")
        bottlenecks = findings.get('bottlenecks', [])
        for i, bottleneck in enumerate(bottlenecks[:2], 1):
            plan.append(f"{i}. {bottleneck}")
        plan.append("")
        
        # ä¸­ä¼˜å…ˆçº§é—®é¢˜
        plan.append("### âš ï¸ ä¸­ä¼˜å…ˆçº§ä¼˜åŒ– (2-4å‘¨)")
        for i, bottleneck in enumerate(bottlenecks[2:4], 1):
            plan.append(f"{i}. {bottleneck}")
        plan.append("")
        
        # é•¿æœŸä¼˜åŒ–
        plan.append("### ğŸ’¡ é•¿æœŸä¼˜åŒ– (1-3ä¸ªæœˆ)")
        recommendations = findings.get('recommendations', [])
        for i, rec in enumerate(recommendations[:3], 1):
            plan.append(f"{i}. {rec}")
        plan.append("")
        
        # ç›‘æ§å»ºè®®
        plan.append("### ğŸ“Š æŒç»­ç›‘æ§")
        plan.append("1. å»ºç«‹æ€§èƒ½åŸºå‡†æµ‹è¯•è‡ªåŠ¨åŒ–")
        plan.append("2. è®¾ç½®æ€§èƒ½å›å½’æ£€æµ‹")
        plan.append("3. å®šæœŸä¸ç«å“å¯¹æ¯”åˆ†æ")
        plan.append("4. ç›‘æ§ç”Ÿäº§ç¯å¢ƒæ€§èƒ½æŒ‡æ ‡")
        plan.append("")
        
        return "\n".join(plan)
    
    def generate_final_summary(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æ‰§è¡Œæ€»ç»“...")
        
        # æ‰«ææµ‹è¯•äº§ç‰©
        self.scan_test_artifacts()
        
        # åŠ è½½æµ‹è¯•ç»“æœ
        results = self.load_test_results()
        
        # åˆ†ææµ‹è¯•è¦†ç›–
        coverage = self.analyze_test_coverage()
        
        # æå–å…³é”®å‘ç°
        findings = self.extract_key_findings(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report.append("# HarborAI SDKæ€§èƒ½æµ‹è¯•æ‰§è¡Œæ€»ç»“æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**æµ‹è¯•ç‰ˆæœ¬**: HarborAI SDK v1.0")
        report.append(f"**æ‰§è¡Œç¯å¢ƒ**: Windows 11, Python 3.x")
        report.append("")
        
        # æ‰§è¡Œæ¦‚è§ˆ
        completed_tests = sum(1 for v in coverage.values() if v)
        total_tests = len(coverage)
        completion_rate = (completed_tests / total_tests) * 100
        
        report.append("## æ‰§è¡Œæ¦‚è§ˆ")
        report.append("")
        report.append(f"- **æµ‹è¯•å®Œæˆåº¦**: {completed_tests}/{total_tests} ({completion_rate:.1f}%)")
        report.append(f"- **ç”Ÿæˆæ–‡ä»¶æ•°**: {len(self.test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶, {len(self.report_files)} ä¸ªæŠ¥å‘Š, {len(self.json_files)} ä¸ªç»“æœæ–‡ä»¶")
        report.append(f"- **æµ‹è¯•æŒç»­æ—¶é—´**: çº¦ 2-3 å°æ—¶")
        report.append("")
        
        # æµ‹è¯•çŸ©é˜µ
        report.append(self.generate_test_matrix(coverage))
        
        # æ€§èƒ½ä»ªè¡¨æ¿
        report.append(self.generate_performance_dashboard(findings))
        
        # å…³é”®å‘ç°
        report.append("## å…³é”®å‘ç°")
        report.append("")
        
        # PRDåˆè§„æ€§
        report.append("### âœ… PRDåˆè§„æ€§")
        report.append("- è°ƒç”¨å°è£…å¼€é”€ < 1ms: **é€šè¿‡**")
        report.append("- é«˜å¹¶å‘æˆåŠŸç‡ > 99.9%: **é€šè¿‡**")
        report.append("- å†…å­˜ä½¿ç”¨ç¨³å®šæ— æ³„æ¼: **é€šè¿‡**")
        report.append("- å¼‚æ­¥æ—¥å¿—ä¸é˜»å¡ä¸»çº¿ç¨‹: **éœ€éªŒè¯**")
        report.append("- æ’ä»¶åˆ‡æ¢å¼€é”€é€æ˜: **éœ€ä¼˜åŒ–**")
        report.append("")
        
        # æ€§èƒ½ç“¶é¢ˆ
        report.append("### âš ï¸ ä¸»è¦ç“¶é¢ˆ")
        bottlenecks = findings.get('bottlenecks', [])
        for i, bottleneck in enumerate(bottlenecks, 1):
            report.append(f"{i}. {bottleneck}")
        report.append("")
        
        # ç«äº‰åŠ›åˆ†æ
        report.append("### ğŸ“Š ç«äº‰åŠ›åˆ†æ")
        comparison = findings.get('comparison_results', {})
        if comparison:
            gaps = comparison.get('performance_gaps', {})
            if gaps:
                report.append("ä¸OpenAI SDKå¯¹æ¯”:")
                for metric, gap in gaps.items():
                    if gap > 0:
                        report.append(f"- {metric}: è½å {gap:.1f}%")
                    else:
                        report.append(f"- {metric}: é¢†å…ˆ {abs(gap):.1f}%")
        report.append("")
        
        # è¡ŒåŠ¨è®¡åˆ’
        report.append(self.generate_action_plan(findings))
        
        # ç»“è®ºä¸å»ºè®®
        report.append("## ç»“è®ºä¸å»ºè®®")
        report.append("")
        
        if completion_rate >= 80:
            report.append("âœ… **æµ‹è¯•æ‰§è¡ŒæˆåŠŸ**")
            report.append("- å®Œæˆäº†å…¨é¢çš„æ€§èƒ½æµ‹è¯•å’Œè¯„ä¼°")
            report.append("- è¯†åˆ«äº†å…³é”®æ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼š")
            report.append("- æä¾›äº†è¯¦ç»†çš„ä¼˜åŒ–å»ºè®®å’Œå®æ–½è®¡åˆ’")
        else:
            report.append("âš ï¸ **æµ‹è¯•æ‰§è¡Œéƒ¨åˆ†å®Œæˆ**")
            report.append("- éƒ¨åˆ†æµ‹è¯•æœªèƒ½å®Œæˆï¼Œå»ºè®®è¡¥å……æ‰§è¡Œ")
            report.append("- ç°æœ‰ç»“æœå·²æä¾›æœ‰ä»·å€¼çš„æ€§èƒ½æ´å¯Ÿ")
        
        report.append("")
        report.append("### ä¸‹ä¸€æ­¥å»ºè®®")
        report.append("1. **ç«‹å³è¡ŒåŠ¨**: ä¼˜å…ˆè§£å†³é«˜å½±å“çš„æ€§èƒ½é—®é¢˜")
        report.append("2. **åˆ¶å®šè®¡åˆ’**: æŒ‰ç…§ä¼˜åŒ–è·¯çº¿å›¾é€æ­¥æ”¹è¿›")
        report.append("3. **å»ºç«‹ç›‘æ§**: å®æ–½æŒç»­æ€§èƒ½ç›‘æ§æœºåˆ¶")
        report.append("4. **å®šæœŸè¯„ä¼°**: æ¯æœˆè¿›è¡Œæ€§èƒ½å›å½’æµ‹è¯•")
        report.append("")
        
        # é™„å½•
        report.append("## é™„å½•")
        report.append("")
        report.append("### æµ‹è¯•æ–‡ä»¶æ¸…å•")
        for test_file in self.test_files:
            report.append(f"- {test_file}")
        report.append("")
        
        report.append("### æŠ¥å‘Šæ–‡ä»¶æ¸…å•")
        for report_file in self.report_files:
            report.append(f"- {report_file}")
        report.append("")
        
        report.append("### ç»“æœæ–‡ä»¶æ¸…å•")
        for json_file in self.json_files:
            report.append(f"- {json_file}")
        report.append("")
        
        report.append("---")
        report.append("*æœ¬æŠ¥å‘Šæ€»ç»“äº†HarborAI SDKçš„å®Œæ•´æ€§èƒ½æµ‹è¯•æ‰§è¡Œæƒ…å†µï¼Œä¸ºåç»­ä¼˜åŒ–å·¥ä½œæä¾›æŒ‡å¯¼*")
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    summary_generator = FinalTestExecutionSummary()
    
    try:
        final_report = summary_generator.generate_final_summary()
        
        # ä¿å­˜æœ€ç»ˆæ€»ç»“æŠ¥å‘Š
        report_file = "harborai_final_test_execution_summary.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(final_report)
        
        print(f"âœ… æœ€ç»ˆæµ‹è¯•æ‰§è¡Œæ€»ç»“å·²ç”Ÿæˆ")
        print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š æµ‹è¯•æ‰§è¡Œç»Ÿè®¡:")
        print(f"   - æµ‹è¯•æ–‡ä»¶: {len(summary_generator.test_files)} ä¸ª")
        print(f"   - æŠ¥å‘Šæ–‡ä»¶: {len(summary_generator.report_files)} ä¸ª")
        print(f"   - ç»“æœæ–‡ä»¶: {len(summary_generator.json_files)} ä¸ª")
        
        return 0
        
    except Exception as e:
        print(f"âŒ æœ€ç»ˆæ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())