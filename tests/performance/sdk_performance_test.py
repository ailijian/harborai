#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HarborAI SDK ç›´æ¥æ€§èƒ½æµ‹è¯•

ç›´æ¥æµ‹è¯•SDKçš„æ€§èƒ½ï¼Œä¸éœ€è¦å¯åŠ¨WebæœåŠ¡
"""

import asyncio
import time
import statistics
import psutil
import gc
import sys
import os
from typing import Dict, List, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from dataclasses import dataclass
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from harborai import HarborAI
    from harborai.config.performance import PerformanceMode
    from harborai.utils.exceptions import HarborAIError
except ImportError as e:
    print(f"âŒ å¯¼å…¥HarborAIå¤±è´¥: {e}")
    print("è¯·ç¡®ä¿HarborAIå·²æ­£ç¡®å®‰è£…")
    sys.exit(1)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    response_times: List[float]
    memory_usage: List[float]
    cpu_usage: List[float]
    success_count: int
    error_count: int
    total_requests: int
    
    @property
    def avg_response_time(self) -> float:
        return statistics.mean(self.response_times) if self.response_times else 0
    
    @property
    def p95_response_time(self) -> float:
        return statistics.quantiles(self.response_times, n=20)[18] if len(self.response_times) >= 20 else 0
    
    @property
    def success_rate(self) -> float:
        return (self.success_count / self.total_requests * 100) if self.total_requests > 0 else 0

class SDKPerformanceTester:
    """SDKæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = {}
        self.process = psutil.Process()
        
    def setup_test_client(self, mode: PerformanceMode = PerformanceMode.BALANCED) -> HarborAI:
        """è®¾ç½®æµ‹è¯•å®¢æˆ·ç«¯"""
        try:
            # ä½¿ç”¨æ¨¡æ‹Ÿé…ç½®ï¼Œé¿å…çœŸå®APIè°ƒç”¨
            client = HarborAI(
                api_key="test-key-for-performance-testing",
                performance_mode=mode,
                enable_cache=True,
                enable_fallback=False,  # ç¦ç”¨fallbacké¿å…ç½‘ç»œè°ƒç”¨
                enable_cost_tracking=True
            )
            return client
        except Exception as e:
            print(f"âŒ åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None
    
    def measure_initialization_overhead(self) -> Dict[str, float]:
        """æµ‹é‡åˆå§‹åŒ–å¼€é”€"""
        print("ğŸ“Š æµ‹è¯•åˆå§‹åŒ–å¼€é”€...")
        
        results = {}
        
        for mode in [PerformanceMode.FAST, PerformanceMode.BALANCED, PerformanceMode.FULL]:
            times = []
            for _ in range(10):
                start_time = time.perf_counter()
                client = self.setup_test_client(mode)
                end_time = time.perf_counter()
                
                if client:
                    times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                
                # æ¸…ç†
                del client
                gc.collect()
            
            if times:
                results[mode.value] = {
                    'avg_ms': statistics.mean(times),
                    'min_ms': min(times),
                    'max_ms': max(times),
                    'p95_ms': statistics.quantiles(times, n=20)[18] if len(times) >= 20 else max(times)
                }
        
        return results
    
    def measure_method_call_overhead(self) -> Dict[str, Any]:
        """æµ‹é‡æ–¹æ³•è°ƒç”¨å¼€é”€"""
        print("ğŸ“Š æµ‹è¯•æ–¹æ³•è°ƒç”¨å¼€é”€...")
        
        client = self.setup_test_client(PerformanceMode.FAST)
        if not client:
            return {}
        
        # æµ‹è¯•ä¸åŒæ–¹æ³•çš„è°ƒç”¨å¼€é”€
        methods_to_test = [
            ('chat.completions.create', self._test_chat_completion_call),
            ('parameter_validation', self._test_parameter_validation),
            ('plugin_switching', self._test_plugin_switching)
        ]
        
        results = {}
        
        for method_name, test_func in methods_to_test:
            times = []
            errors = 0
            
            for _ in range(100):  # å¤šæ¬¡æµ‹è¯•è·å¾—å‡†ç¡®ç»“æœ
                try:
                    start_time = time.perf_counter()
                    test_func(client)
                    end_time = time.perf_counter()
                    times.append((end_time - start_time) * 1000000)  # è½¬æ¢ä¸ºå¾®ç§’
                except Exception:
                    errors += 1
            
            if times:
                results[method_name] = {
                    'avg_us': statistics.mean(times),
                    'min_us': min(times),
                    'max_us': max(times),
                    'p95_us': statistics.quantiles(times, n=20)[18] if len(times) >= 20 else max(times),
                    'error_count': errors
                }
        
        return results
    
    def _test_chat_completion_call(self, client: HarborAI):
        """æµ‹è¯•èŠå¤©å®Œæˆè°ƒç”¨ï¼ˆä¸å®é™…å‘é€è¯·æ±‚ï¼‰"""
        try:
            # åªæµ‹è¯•å‚æ•°å¤„ç†å’ŒéªŒè¯ï¼Œä¸å®é™…å‘é€è¯·æ±‚
            messages = [{"role": "user", "content": "Hello"}]
            
            # è¿™é‡Œæˆ‘ä»¬åªæµ‹è¯•å‚æ•°éªŒè¯å’Œé¢„å¤„ç†çš„å¼€é”€
            # å®é™…çš„ç½‘ç»œè¯·æ±‚ä¼šè¢«æ¨¡æ‹Ÿæˆ–è·³è¿‡
            params = {
                "model": "gpt-3.5-turbo",
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 100
            }
            
            # æ¨¡æ‹Ÿå‚æ•°éªŒè¯è¿‡ç¨‹
            if hasattr(client.chat.completions, '_validate_parameters'):
                client.chat.completions._validate_parameters(params)
            
        except Exception:
            # é¢„æœŸä¼šæœ‰é”™è¯¯ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰çœŸå®çš„APIå¯†é’¥
            pass
    
    def _test_parameter_validation(self, client: HarborAI):
        """æµ‹è¯•å‚æ•°éªŒè¯å¼€é”€"""
        messages = [{"role": "user", "content": "Test message"}]
        
        # æµ‹è¯•å„ç§å‚æ•°ç»„åˆçš„éªŒè¯å¼€é”€
        test_params = [
            {"model": "gpt-3.5-turbo", "messages": messages},
            {"model": "gpt-4", "messages": messages, "temperature": 0.5},
            {"model": "claude-3", "messages": messages, "max_tokens": 200, "stream": True}
        ]
        
        for params in test_params:
            try:
                # åªè¿›è¡Œå‚æ•°éªŒè¯ï¼Œä¸å®é™…è°ƒç”¨
                if hasattr(client.chat.completions, '_validate_parameters'):
                    client.chat.completions._validate_parameters(params)
            except Exception:
                pass
    
    def _test_plugin_switching(self, client: HarborAI):
        """æµ‹è¯•æ’ä»¶åˆ‡æ¢å¼€é”€"""
        models = ["gpt-3.5-turbo", "gpt-4", "claude-3-sonnet"]
        
        for model in models:
            try:
                # æµ‹è¯•æ’ä»¶ç®¡ç†å™¨çš„æ¨¡å‹åˆ‡æ¢å¼€é”€
                if hasattr(client, '_client_manager') and hasattr(client._client_manager, 'get_plugin'):
                    client._client_manager.get_plugin(model)
            except Exception:
                pass
    
    def measure_memory_usage(self) -> Dict[str, Any]:
        """æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("ğŸ“Š æµ‹è¯•å†…å­˜ä½¿ç”¨...")
        
        # è·å–åŸºçº¿å†…å­˜
        gc.collect()
        baseline_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
        results = {
            'baseline_mb': baseline_memory,
            'client_creation': {},
            'memory_leak_test': {}
        }
        
        # æµ‹è¯•å®¢æˆ·ç«¯åˆ›å»ºçš„å†…å­˜å¼€é”€
        for mode in [PerformanceMode.FAST, PerformanceMode.BALANCED, PerformanceMode.FULL]:
            gc.collect()
            before_memory = self.process.memory_info().rss / 1024 / 1024
            
            clients = []
            for _ in range(10):
                client = self.setup_test_client(mode)
                if client:
                    clients.append(client)
            
            after_memory = self.process.memory_info().rss / 1024 / 1024
            
            # æ¸…ç†
            for client in clients:
                del client
            clients.clear()
            gc.collect()
            
            cleanup_memory = self.process.memory_info().rss / 1024 / 1024
            
            results['client_creation'][mode.value] = {
                'before_mb': before_memory,
                'after_mb': after_memory,
                'cleanup_mb': cleanup_memory,
                'overhead_per_client_mb': (after_memory - before_memory) / 10 if clients else 0
            }
        
        # å†…å­˜æ³„æ¼æµ‹è¯•
        gc.collect()
        start_memory = self.process.memory_info().rss / 1024 / 1024
        
        for i in range(100):
            client = self.setup_test_client(PerformanceMode.FAST)
            if client:
                # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
                try:
                    self._test_chat_completion_call(client)
                except Exception:
                    pass
                del client
            
            if i % 20 == 0:
                gc.collect()
        
        gc.collect()
        end_memory = self.process.memory_info().rss / 1024 / 1024
        
        results['memory_leak_test'] = {
            'start_mb': start_memory,
            'end_mb': end_memory,
            'potential_leak_mb': end_memory - start_memory
        }
        
        return results
    
    def measure_concurrent_performance(self) -> Dict[str, Any]:
        """æµ‹é‡å¹¶å‘æ€§èƒ½"""
        print("ğŸ“Š æµ‹è¯•å¹¶å‘æ€§èƒ½...")
        
        def worker_task(worker_id: int, num_operations: int) -> Dict[str, Any]:
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            client = self.setup_test_client(PerformanceMode.FAST)
            if not client:
                return {'success': 0, 'errors': num_operations, 'times': []}
            
            times = []
            errors = 0
            
            for _ in range(num_operations):
                try:
                    start_time = time.perf_counter()
                    self._test_chat_completion_call(client)
                    end_time = time.perf_counter()
                    times.append((end_time - start_time) * 1000)
                except Exception:
                    errors += 1
            
            return {
                'success': num_operations - errors,
                'errors': errors,
                'times': times,
                'worker_id': worker_id
            }
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [1, 5, 10, 20]
        operations_per_worker = 50
        
        results = {}
        
        for concurrency in concurrency_levels:
            print(f"  æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
            
            start_time = time.perf_counter()
            
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = [
                    executor.submit(worker_task, i, operations_per_worker)
                    for i in range(concurrency)
                ]
                
                worker_results = []
                for future in as_completed(futures):
                    try:
                        result = future.result()
                        worker_results.append(result)
                    except Exception as e:
                        print(f"    å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")
            
            end_time = time.perf_counter()
            
            # æ±‡æ€»ç»“æœ
            total_success = sum(r['success'] for r in worker_results)
            total_errors = sum(r['errors'] for r in worker_results)
            all_times = []
            for r in worker_results:
                all_times.extend(r['times'])
            
            results[f'concurrency_{concurrency}'] = {
                'total_operations': concurrency * operations_per_worker,
                'successful_operations': total_success,
                'failed_operations': total_errors,
                'success_rate': (total_success / (concurrency * operations_per_worker) * 100) if concurrency * operations_per_worker > 0 else 0,
                'total_time_seconds': end_time - start_time,
                'operations_per_second': total_success / (end_time - start_time) if (end_time - start_time) > 0 else 0,
                'avg_response_time_ms': statistics.mean(all_times) if all_times else 0,
                'p95_response_time_ms': statistics.quantiles(all_times, n=20)[18] if len(all_times) >= 20 else 0
            }
        
        return results
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹HarborAI SDKç»¼åˆæ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        
        start_time = datetime.now()
        
        # æ”¶é›†ç³»ç»Ÿä¿¡æ¯
        system_info = {
            'python_version': sys.version,
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
            'platform': sys.platform
        }
        
        results = {
            'test_info': {
                'start_time': start_time.isoformat(),
                'system_info': system_info
            },
            'initialization_overhead': self.measure_initialization_overhead(),
            'method_call_overhead': self.measure_method_call_overhead(),
            'memory_usage': self.measure_memory_usage(),
            'concurrent_performance': self.measure_concurrent_performance()
        }
        
        end_time = datetime.now()
        results['test_info']['end_time'] = end_time.isoformat()
        results['test_info']['total_duration_seconds'] = (end_time - start_time).total_seconds()
        
        return results
    
    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š HarborAI SDK æ€§èƒ½æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 60)
        
        # åˆå§‹åŒ–å¼€é”€
        if 'initialization_overhead' in results:
            print("\nğŸš€ åˆå§‹åŒ–å¼€é”€:")
            for mode, metrics in results['initialization_overhead'].items():
                print(f"  {mode}æ¨¡å¼: {metrics['avg_ms']:.2f}ms (å¹³å‡)")
        
        # æ–¹æ³•è°ƒç”¨å¼€é”€
        if 'method_call_overhead' in results:
            print("\nâš¡ æ–¹æ³•è°ƒç”¨å¼€é”€:")
            for method, metrics in results['method_call_overhead'].items():
                print(f"  {method}: {metrics['avg_us']:.2f}Î¼s (å¹³å‡)")
        
        # å†…å­˜ä½¿ç”¨
        if 'memory_usage' in results:
            print("\nğŸ’¾ å†…å­˜ä½¿ç”¨:")
            baseline = results['memory_usage']['baseline_mb']
            print(f"  åŸºçº¿å†…å­˜: {baseline:.2f}MB")
            
            if 'memory_leak_test' in results['memory_usage']:
                leak = results['memory_usage']['memory_leak_test']['potential_leak_mb']
                print(f"  æ½œåœ¨å†…å­˜æ³„æ¼: {leak:.2f}MB")
        
        # å¹¶å‘æ€§èƒ½
        if 'concurrent_performance' in results:
            print("\nğŸ”„ å¹¶å‘æ€§èƒ½:")
            for level, metrics in results['concurrent_performance'].items():
                if level.startswith('concurrency_'):
                    concurrency = level.split('_')[1]
                    print(f"  {concurrency}å¹¶å‘: {metrics['success_rate']:.1f}%æˆåŠŸç‡, {metrics['operations_per_second']:.1f}ops/s")
        
        # PRDåˆè§„æ€§æ£€æŸ¥
        print("\nâœ… PRDåˆè§„æ€§æ£€æŸ¥:")
        self._check_prd_compliance(results)
        
        print("\n" + "=" * 60)
    
    def _check_prd_compliance(self, results: Dict[str, Any]):
        """æ£€æŸ¥PRDåˆè§„æ€§"""
        compliance_results = []
        
        # æ£€æŸ¥è°ƒç”¨å°è£…å¼€é”€ < 1ms
        if 'method_call_overhead' in results:
            for method, metrics in results['method_call_overhead'].items():
                avg_ms = metrics['avg_us'] / 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                if avg_ms < 1.0:
                    compliance_results.append(f"  âœ… {method}: {avg_ms:.3f}ms < 1ms")
                else:
                    compliance_results.append(f"  âŒ {method}: {avg_ms:.3f}ms >= 1ms")
        
        # æ£€æŸ¥é«˜å¹¶å‘æˆåŠŸç‡ > 99.9%
        if 'concurrent_performance' in results:
            for level, metrics in results['concurrent_performance'].items():
                if level.startswith('concurrency_'):
                    success_rate = metrics['success_rate']
                    if success_rate > 99.9:
                        compliance_results.append(f"  âœ… {level}: {success_rate:.1f}% > 99.9%")
                    else:
                        compliance_results.append(f"  âŒ {level}: {success_rate:.1f}% <= 99.9%")
        
        # æ£€æŸ¥å†…å­˜æ³„æ¼
        if 'memory_usage' in results and 'memory_leak_test' in results['memory_usage']:
            leak = results['memory_usage']['memory_leak_test']['potential_leak_mb']
            if leak < 10:  # å°äº10MBè®¤ä¸ºå¯æ¥å—
                compliance_results.append(f"  âœ… å†…å­˜æ³„æ¼: {leak:.2f}MB < 10MB")
            else:
                compliance_results.append(f"  âŒ å†…å­˜æ³„æ¼: {leak:.2f}MB >= 10MB")
        
        for result in compliance_results:
            print(result)

def main():
    """ä¸»å‡½æ•°"""
    tester = SDKPerformanceTester()
    
    try:
        results = tester.run_comprehensive_test()
        tester.print_summary(results)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        import json
        output_file = "sdk_performance_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())