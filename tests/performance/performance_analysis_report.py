#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HarborAI SDK æ€§èƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨

åŸºäºæ€§èƒ½æµ‹è¯•ç»“æœï¼Œç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œä¼˜åŒ–å»ºè®®
"""

import json
import sys
import os
from datetime import datetime
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass

@dataclass
class PerformanceThreshold:
    """æ€§èƒ½é˜ˆå€¼å®šä¹‰"""
    name: str
    value: float
    unit: str
    comparison: str  # 'less_than', 'greater_than', 'equal_to'
    description: str

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, results_file: str):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.results_file = results_file
        self.results = self._load_results()
        
        # PRD/TD æ€§èƒ½è¦æ±‚é˜ˆå€¼
        self.prd_thresholds = [
            PerformanceThreshold("è°ƒç”¨å°è£…å¼€é”€", 1.0, "ms", "less_than", "æ¯æ¬¡APIè°ƒç”¨çš„å°è£…å¼€é”€åº”å°äº1æ¯«ç§’"),
            PerformanceThreshold("é«˜å¹¶å‘æˆåŠŸç‡", 99.9, "%", "greater_than", "é«˜å¹¶å‘åœºæ™¯ä¸‹çš„æˆåŠŸç‡åº”å¤§äº99.9%"),
            PerformanceThreshold("å†…å­˜æ³„æ¼", 10.0, "MB", "less_than", "é•¿æœŸè¿è¡Œæ—¶çš„å†…å­˜æ³„æ¼åº”å°äº10MB"),
            PerformanceThreshold("åˆå§‹åŒ–æ—¶é—´", 500.0, "ms", "less_than", "SDKåˆå§‹åŒ–æ—¶é—´åº”å°äº500æ¯«ç§’"),
            PerformanceThreshold("å¹¶å‘ååé‡", 100.0, "ops/s", "greater_than", "å¹¶å‘å¤„ç†èƒ½åŠ›åº”å¤§äº100æ“ä½œ/ç§’")
        ]
    
    def _load_results(self) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•ç»“æœ"""
        try:
            with open(self.results_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def analyze_initialization_performance(self) -> Dict[str, Any]:
        """åˆ†æåˆå§‹åŒ–æ€§èƒ½"""
        init_data = self.results.get('initialization_overhead', {})
        
        analysis = {
            'summary': {},
            'bottlenecks': [],
            'recommendations': []
        }
        
        # åˆ†æå„æ¨¡å¼çš„åˆå§‹åŒ–æ—¶é—´
        for mode, metrics in init_data.items():
            avg_time = metrics.get('avg_ms', 0)
            max_time = metrics.get('max_ms', 0)
            
            analysis['summary'][mode] = {
                'avg_time_ms': avg_time,
                'max_time_ms': max_time,
                'meets_threshold': avg_time < 500.0,
                'performance_grade': self._grade_performance(avg_time, 200, 350, 500)
            }
            
            # è¯†åˆ«ç“¶é¢ˆ
            if avg_time > 400:
                analysis['bottlenecks'].append(f"{mode}æ¨¡å¼åˆå§‹åŒ–æ—¶é—´è¿‡é•¿: {avg_time:.1f}ms")
            
            if max_time > avg_time * 1.5:
                analysis['bottlenecks'].append(f"{mode}æ¨¡å¼åˆå§‹åŒ–æ—¶é—´ä¸ç¨³å®šï¼Œæœ€å¤§å€¼æ˜¯å¹³å‡å€¼çš„{max_time/avg_time:.1f}å€")
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if analysis['bottlenecks']:
            analysis['recommendations'].extend([
                "è€ƒè™‘å»¶è¿Ÿåˆå§‹åŒ–éå…³é”®ç»„ä»¶",
                "ä¼˜åŒ–æ’ä»¶ç®¡ç†å™¨çš„åˆå§‹åŒ–æµç¨‹",
                "å‡å°‘å¯åŠ¨æ—¶çš„é…ç½®éªŒè¯å¼€é”€",
                "ä½¿ç”¨è¿æ¥æ± é¢„çƒ­ç­–ç•¥"
            ])
        
        return analysis
    
    def analyze_method_call_performance(self) -> Dict[str, Any]:
        """åˆ†ææ–¹æ³•è°ƒç”¨æ€§èƒ½"""
        method_data = self.results.get('method_call_overhead', {})
        
        analysis = {
            'summary': {},
            'bottlenecks': [],
            'recommendations': []
        }
        
        # åˆ†æå„æ–¹æ³•çš„è°ƒç”¨å¼€é”€
        for method, metrics in method_data.items():
            avg_time_us = metrics.get('avg_us', 0)
            avg_time_ms = avg_time_us / 1000
            p95_time_us = metrics.get('p95_us', 0)
            
            analysis['summary'][method] = {
                'avg_time_us': avg_time_us,
                'avg_time_ms': avg_time_ms,
                'p95_time_us': p95_time_us,
                'meets_threshold': avg_time_ms < 1.0,
                'performance_grade': self._grade_performance(avg_time_ms, 0.1, 0.5, 1.0)
            }
            
            # è¯†åˆ«ç“¶é¢ˆ
            if avg_time_ms > 0.5:
                analysis['bottlenecks'].append(f"{method}è°ƒç”¨å¼€é”€è¾ƒé«˜: {avg_time_ms:.3f}ms")
            
            if p95_time_us > avg_time_us * 2:
                analysis['bottlenecks'].append(f"{method}è°ƒç”¨æ—¶é—´ä¸ç¨³å®šï¼ŒP95æ˜¯å¹³å‡å€¼çš„{p95_time_us/avg_time_us:.1f}å€")
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if any(m['avg_time_ms'] > 0.3 for m in analysis['summary'].values()):
            analysis['recommendations'].extend([
                "ä¼˜åŒ–å‚æ•°éªŒè¯é€»è¾‘ï¼Œå‡å°‘é‡å¤æ£€æŸ¥",
                "ä½¿ç”¨ç¼“å­˜æœºåˆ¶å­˜å‚¨å¸¸ç”¨é…ç½®",
                "å‡å°‘æ–¹æ³•è°ƒç”¨é“¾çš„æ·±åº¦",
                "è€ƒè™‘ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„"
            ])
        
        return analysis
    
    def analyze_memory_performance(self) -> Dict[str, Any]:
        """åˆ†æå†…å­˜æ€§èƒ½"""
        memory_data = self.results.get('memory_usage', {})
        
        analysis = {
            'summary': {},
            'bottlenecks': [],
            'recommendations': []
        }
        
        # åŸºçº¿å†…å­˜
        baseline_mb = memory_data.get('baseline_mb', 0)
        
        # å®¢æˆ·ç«¯åˆ›å»ºå†…å­˜å¼€é”€
        client_creation = memory_data.get('client_creation', {})
        for mode, metrics in client_creation.items():
            overhead = metrics.get('overhead_per_client_mb', 0)
            cleanup_efficiency = (metrics.get('after_mb', 0) - metrics.get('cleanup_mb', 0))
            
            analysis['summary'][f'{mode}_client'] = {
                'overhead_per_client_mb': overhead,
                'cleanup_efficiency_mb': cleanup_efficiency,
                'performance_grade': self._grade_performance(overhead, 1, 5, 10)
            }
        
        # å†…å­˜æ³„æ¼åˆ†æ
        leak_test = memory_data.get('memory_leak_test', {})
        potential_leak = leak_test.get('potential_leak_mb', 0)
        
        analysis['summary']['memory_leak'] = {
            'potential_leak_mb': potential_leak,
            'meets_threshold': potential_leak < 10.0,
            'performance_grade': self._grade_performance(potential_leak, 2, 5, 10, reverse=True)
        }
        
        # è¯†åˆ«ç“¶é¢ˆ
        if potential_leak > 5:
            analysis['bottlenecks'].append(f"å­˜åœ¨æ½œåœ¨å†…å­˜æ³„æ¼: {potential_leak:.2f}MB")
        
        for mode, metrics in client_creation.items():
            cleanup_efficiency = metrics.get('after_mb', 0) - metrics.get('cleanup_mb', 0)
            if cleanup_efficiency > 2:
                analysis['bottlenecks'].append(f"{mode}æ¨¡å¼å®¢æˆ·ç«¯æ¸…ç†ä¸å½»åº•: {cleanup_efficiency:.2f}MB")
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if analysis['bottlenecks']:
            analysis['recommendations'].extend([
                "æ£€æŸ¥å¯¹è±¡å¼•ç”¨ï¼Œç¡®ä¿åŠæ—¶é‡Šæ”¾",
                "ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼Œé¿å…æ— é™å¢é•¿",
                "ä½¿ç”¨å¼±å¼•ç”¨å‡å°‘å¾ªç¯å¼•ç”¨",
                "å®šæœŸæ‰§è¡Œåƒåœ¾å›æ”¶"
            ])
        
        return analysis
    
    def analyze_concurrent_performance(self) -> Dict[str, Any]:
        """åˆ†æå¹¶å‘æ€§èƒ½"""
        concurrent_data = self.results.get('concurrent_performance', {})
        
        analysis = {
            'summary': {},
            'bottlenecks': [],
            'recommendations': [],
            'scalability': {}
        }
        
        # åˆ†æå„å¹¶å‘çº§åˆ«çš„æ€§èƒ½
        concurrency_levels = []
        throughputs = []
        
        for level_key, metrics in concurrent_data.items():
            if level_key.startswith('concurrency_'):
                concurrency = int(level_key.split('_')[1])
                success_rate = metrics.get('success_rate', 0)
                ops_per_sec = metrics.get('operations_per_second', 0)
                avg_response_time = metrics.get('avg_response_time_ms', 0)
                
                concurrency_levels.append(concurrency)
                throughputs.append(ops_per_sec)
                
                analysis['summary'][f'concurrency_{concurrency}'] = {
                    'success_rate': success_rate,
                    'throughput_ops_per_sec': ops_per_sec,
                    'avg_response_time_ms': avg_response_time,
                    'meets_success_threshold': success_rate > 99.9,
                    'meets_throughput_threshold': ops_per_sec > 100,
                    'performance_grade': self._grade_concurrent_performance(success_rate, ops_per_sec)
                }
                
                # è¯†åˆ«ç“¶é¢ˆ
                if success_rate < 99.9:
                    analysis['bottlenecks'].append(f"{concurrency}å¹¶å‘æˆåŠŸç‡ä¸è¾¾æ ‡: {success_rate:.1f}%")
                
                if ops_per_sec < 100:
                    analysis['bottlenecks'].append(f"{concurrency}å¹¶å‘ååé‡ä¸è¾¾æ ‡: {ops_per_sec:.1f}ops/s")
        
        # å¯æ‰©å±•æ€§åˆ†æ
        if len(concurrency_levels) >= 2:
            # è®¡ç®—ååé‡å¢é•¿ç‡
            throughput_growth = []
            for i in range(1, len(throughputs)):
                growth = (throughputs[i] - throughputs[i-1]) / throughputs[i-1] * 100
                throughput_growth.append(growth)
            
            analysis['scalability'] = {
                'linear_scaling': all(growth > 0 for growth in throughput_growth),
                'avg_growth_rate': sum(throughput_growth) / len(throughput_growth) if throughput_growth else 0,
                'peak_throughput': max(throughputs),
                'optimal_concurrency': concurrency_levels[throughputs.index(max(throughputs))]
            }
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if analysis['bottlenecks']:
            analysis['recommendations'].extend([
                "ä¼˜åŒ–çº¿ç¨‹æ± é…ç½®",
                "å‡å°‘é”ç«äº‰å’ŒåŒæ­¥å¼€é”€",
                "ä½¿ç”¨å¼‚æ­¥I/Oæé«˜å¹¶å‘èƒ½åŠ›",
                "ä¼˜åŒ–èµ„æºæ± ç®¡ç†"
            ])
        
        return analysis
    
    def _grade_performance(self, value: float, excellent: float, good: float, acceptable: float, reverse: bool = False) -> str:
        """æ€§èƒ½è¯„çº§"""
        if not reverse:
            if value <= excellent:
                return "ä¼˜ç§€"
            elif value <= good:
                return "è‰¯å¥½"
            elif value <= acceptable:
                return "å¯æ¥å—"
            else:
                return "éœ€è¦ä¼˜åŒ–"
        else:
            if value >= excellent:
                return "ä¼˜ç§€"
            elif value >= good:
                return "è‰¯å¥½"
            elif value >= acceptable:
                return "å¯æ¥å—"
            else:
                return "éœ€è¦ä¼˜åŒ–"
    
    def _grade_concurrent_performance(self, success_rate: float, throughput: float) -> str:
        """å¹¶å‘æ€§èƒ½è¯„çº§"""
        if success_rate > 99.9 and throughput > 300:
            return "ä¼˜ç§€"
        elif success_rate > 99.5 and throughput > 200:
            return "è‰¯å¥½"
        elif success_rate > 99.0 and throughput > 100:
            return "å¯æ¥å—"
        else:
            return "éœ€è¦ä¼˜åŒ–"
    
    def check_prd_compliance(self) -> Dict[str, Any]:
        """æ£€æŸ¥PRDåˆè§„æ€§"""
        compliance = {
            'overall_score': 0,
            'passed_checks': 0,
            'total_checks': len(self.prd_thresholds),
            'details': []
        }
        
        for threshold in self.prd_thresholds:
            check_result = self._check_single_threshold(threshold)
            compliance['details'].append(check_result)
            if check_result['passed']:
                compliance['passed_checks'] += 1
        
        compliance['overall_score'] = (compliance['passed_checks'] / compliance['total_checks']) * 100
        compliance['compliance_level'] = self._get_compliance_level(compliance['overall_score'])
        
        return compliance
    
    def _check_single_threshold(self, threshold: PerformanceThreshold) -> Dict[str, Any]:
        """æ£€æŸ¥å•ä¸ªé˜ˆå€¼"""
        result = {
            'name': threshold.name,
            'threshold_value': threshold.value,
            'unit': threshold.unit,
            'description': threshold.description,
            'passed': False,
            'actual_value': None,
            'deviation': None
        }
        
        # æ ¹æ®é˜ˆå€¼ç±»å‹è·å–å®é™…å€¼
        if threshold.name == "è°ƒç”¨å°è£…å¼€é”€":
            # å–æ‰€æœ‰æ–¹æ³•è°ƒç”¨çš„å¹³å‡å¼€é”€
            method_data = self.results.get('method_call_overhead', {})
            if method_data:
                avg_times = [metrics.get('avg_us', 0) / 1000 for metrics in method_data.values()]
                result['actual_value'] = sum(avg_times) / len(avg_times) if avg_times else 0
        
        elif threshold.name == "é«˜å¹¶å‘æˆåŠŸç‡":
            # å–æœ€é«˜å¹¶å‘çº§åˆ«çš„æˆåŠŸç‡
            concurrent_data = self.results.get('concurrent_performance', {})
            max_concurrency_key = max([k for k in concurrent_data.keys() if k.startswith('concurrency_')], 
                                    key=lambda x: int(x.split('_')[1]), default=None)
            if max_concurrency_key:
                result['actual_value'] = concurrent_data[max_concurrency_key].get('success_rate', 0)
        
        elif threshold.name == "å†…å­˜æ³„æ¼":
            memory_data = self.results.get('memory_usage', {})
            leak_test = memory_data.get('memory_leak_test', {})
            result['actual_value'] = leak_test.get('potential_leak_mb', 0)
        
        elif threshold.name == "åˆå§‹åŒ–æ—¶é—´":
            # å–æ‰€æœ‰æ¨¡å¼çš„å¹³å‡åˆå§‹åŒ–æ—¶é—´
            init_data = self.results.get('initialization_overhead', {})
            if init_data:
                avg_times = [metrics.get('avg_ms', 0) for metrics in init_data.values()]
                result['actual_value'] = sum(avg_times) / len(avg_times) if avg_times else 0
        
        elif threshold.name == "å¹¶å‘ååé‡":
            # å–æœ€é«˜ååé‡
            concurrent_data = self.results.get('concurrent_performance', {})
            throughputs = [metrics.get('operations_per_second', 0) 
                          for metrics in concurrent_data.values()]
            result['actual_value'] = max(throughputs) if throughputs else 0
        
        # æ£€æŸ¥æ˜¯å¦é€šè¿‡é˜ˆå€¼
        if result['actual_value'] is not None:
            if threshold.comparison == "less_than":
                result['passed'] = result['actual_value'] < threshold.value
                result['deviation'] = result['actual_value'] - threshold.value
            elif threshold.comparison == "greater_than":
                result['passed'] = result['actual_value'] > threshold.value
                result['deviation'] = threshold.value - result['actual_value']
        
        return result
    
    def _get_compliance_level(self, score: float) -> str:
        """è·å–åˆè§„ç­‰çº§"""
        if score >= 90:
            return "ä¼˜ç§€"
        elif score >= 80:
            return "è‰¯å¥½"
        elif score >= 70:
            return "å¯æ¥å—"
        else:
            return "éœ€è¦æ”¹è¿›"
    
    def generate_optimization_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºå„é¡¹åˆ†æç»“æœç”Ÿæˆå»ºè®®
        init_analysis = self.analyze_initialization_performance()
        method_analysis = self.analyze_method_call_performance()
        memory_analysis = self.analyze_memory_performance()
        concurrent_analysis = self.analyze_concurrent_performance()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å»ºè®®
        if init_analysis['bottlenecks']:
            recommendations.append({
                'category': 'åˆå§‹åŒ–ä¼˜åŒ–',
                'priority': 'high',
                'impact': 'medium',
                'effort': 'medium',
                'recommendations': init_analysis['recommendations'],
                'expected_improvement': 'å‡å°‘20-30%çš„å¯åŠ¨æ—¶é—´'
            })
        
        # æ–¹æ³•è°ƒç”¨ä¼˜åŒ–å»ºè®®
        if method_analysis['bottlenecks']:
            recommendations.append({
                'category': 'æ–¹æ³•è°ƒç”¨ä¼˜åŒ–',
                'priority': 'high',
                'impact': 'high',
                'effort': 'low',
                'recommendations': method_analysis['recommendations'],
                'expected_improvement': 'å‡å°‘50-70%çš„è°ƒç”¨å¼€é”€'
            })
        
        # å†…å­˜ä¼˜åŒ–å»ºè®®
        if memory_analysis['bottlenecks']:
            recommendations.append({
                'category': 'å†…å­˜ç®¡ç†ä¼˜åŒ–',
                'priority': 'medium',
                'impact': 'medium',
                'effort': 'medium',
                'recommendations': memory_analysis['recommendations'],
                'expected_improvement': 'å‡å°‘å†…å­˜æ³„æ¼ï¼Œæé«˜é•¿æœŸç¨³å®šæ€§'
            })
        
        # å¹¶å‘ä¼˜åŒ–å»ºè®®
        if concurrent_analysis['bottlenecks']:
            recommendations.append({
                'category': 'å¹¶å‘æ€§èƒ½ä¼˜åŒ–',
                'priority': 'high',
                'impact': 'high',
                'effort': 'high',
                'recommendations': concurrent_analysis['recommendations'],
                'expected_improvement': 'æé«˜30-50%çš„å¹¶å‘å¤„ç†èƒ½åŠ›'
            })
        
        return recommendations
    
    def generate_comprehensive_report(self) -> str:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        report = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report.append("# HarborAI SDK æ€§èƒ½åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # æµ‹è¯•æ¦‚è§ˆ
        test_info = self.results.get('test_info', {})
        report.append("## æµ‹è¯•æ¦‚è§ˆ")
        report.append(f"- æµ‹è¯•å¼€å§‹æ—¶é—´: {test_info.get('start_time', 'N/A')}")
        report.append(f"- æµ‹è¯•ç»“æŸæ—¶é—´: {test_info.get('end_time', 'N/A')}")
        report.append(f"- æ€»æµ‹è¯•æ—¶é•¿: {test_info.get('total_duration_seconds', 0):.1f}ç§’")
        
        system_info = test_info.get('system_info', {})
        report.append(f"- CPUæ ¸å¿ƒæ•°: {system_info.get('cpu_count', 'N/A')}")
        report.append(f"- æ€»å†…å­˜: {system_info.get('memory_total_gb', 0):.1f}GB")
        report.append(f"- å¹³å°: {system_info.get('platform', 'N/A')}")
        report.append("")
        
        # PRDåˆè§„æ€§æ£€æŸ¥
        compliance = self.check_prd_compliance()
        report.append("## PRDåˆè§„æ€§æ£€æŸ¥")
        report.append(f"- æ€»ä½“å¾—åˆ†: {compliance['overall_score']:.1f}%")
        report.append(f"- åˆè§„ç­‰çº§: {compliance['compliance_level']}")
        report.append(f"- é€šè¿‡æ£€æŸ¥: {compliance['passed_checks']}/{compliance['total_checks']}")
        report.append("")
        
        for detail in compliance['details']:
            status = "âœ…" if detail['passed'] else "âŒ"
            actual_val = detail['actual_value'] if detail['actual_value'] is not None else 0
            report.append(f"{status} {detail['name']}: {actual_val:.3f}{detail['unit']} "
                         f"(é˜ˆå€¼: {detail['threshold_value']}{detail['unit']})")
        report.append("")
        
        # å„é¡¹æ€§èƒ½åˆ†æ
        analyses = [
            ("åˆå§‹åŒ–æ€§èƒ½åˆ†æ", self.analyze_initialization_performance()),
            ("æ–¹æ³•è°ƒç”¨æ€§èƒ½åˆ†æ", self.analyze_method_call_performance()),
            ("å†…å­˜æ€§èƒ½åˆ†æ", self.analyze_memory_performance()),
            ("å¹¶å‘æ€§èƒ½åˆ†æ", self.analyze_concurrent_performance())
        ]
        
        for title, analysis in analyses:
            report.append(f"## {title}")
            
            # æ€§èƒ½æ‘˜è¦
            if 'summary' in analysis:
                report.append("### æ€§èƒ½æ‘˜è¦")
                for item, metrics in analysis['summary'].items():
                    grade = metrics.get('performance_grade', 'N/A')
                    report.append(f"- {item}: {grade}")
                report.append("")
            
            # ç“¶é¢ˆè¯†åˆ«
            if analysis.get('bottlenecks'):
                report.append("### è¯†åˆ«çš„ç“¶é¢ˆ")
                for bottleneck in analysis['bottlenecks']:
                    report.append(f"- âš ï¸ {bottleneck}")
                report.append("")
            
            # ä¼˜åŒ–å»ºè®®
            if analysis.get('recommendations'):
                report.append("### ä¼˜åŒ–å»ºè®®")
                for rec in analysis['recommendations']:
                    report.append(f"- ğŸ’¡ {rec}")
                report.append("")
        
        # ç»¼åˆä¼˜åŒ–å»ºè®®
        recommendations = self.generate_optimization_recommendations()
        if recommendations:
            report.append("## ç»¼åˆä¼˜åŒ–å»ºè®®")
            for i, rec in enumerate(recommendations, 1):
                report.append(f"### {i}. {rec['category']}")
                report.append(f"- ä¼˜å…ˆçº§: {rec['priority']}")
                report.append(f"- å½±å“ç¨‹åº¦: {rec['impact']}")
                report.append(f"- å®æ–½éš¾åº¦: {rec['effort']}")
                report.append(f"- é¢„æœŸæ”¹è¿›: {rec['expected_improvement']}")
                report.append("- å…·ä½“å»ºè®®:")
                for suggestion in rec['recommendations']:
                    report.append(f"  - {suggestion}")
                report.append("")
        
        # ç»“è®ºå’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨
        report.append("## ç»“è®ºå’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨")
        report.append("åŸºäºæ€§èƒ½æµ‹è¯•ç»“æœï¼ŒHarborAI SDKåœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°è‰¯å¥½ï¼š")
        
        # æ ¹æ®åˆè§„æ€§å¾—åˆ†ç»™å‡ºç»“è®º
        if compliance['overall_score'] >= 80:
            report.append("- âœ… æ•´ä½“æ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œæ»¡è¶³å¤§éƒ¨åˆ†PRDè¦æ±‚")
        else:
            report.append("- âš ï¸ å­˜åœ¨ä¸€äº›æ€§èƒ½é—®é¢˜éœ€è¦ä¼˜åŒ–")
        
        report.append("")
        report.append("å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š")
        report.append("1. ä¼˜å…ˆè§£å†³é«˜ä¼˜å…ˆçº§çš„æ€§èƒ½ç“¶é¢ˆ")
        report.append("2. å®æ–½å…·ä½“çš„ä¼˜åŒ–å»ºè®®")
        report.append("3. å»ºç«‹æŒç»­çš„æ€§èƒ½ç›‘æ§æœºåˆ¶")
        report.append("4. å®šæœŸè¿›è¡Œæ€§èƒ½å›å½’æµ‹è¯•")
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    results_file = "sdk_performance_results.json"
    
    if not os.path.exists(results_file):
        print(f"âŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_file}")
        return 1
    
    analyzer = PerformanceAnalyzer(results_file)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    report = analyzer.generate_comprehensive_report()
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = "harborai_performance_analysis_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("ğŸ“Š HarborAI SDK æ€§èƒ½åˆ†æå®Œæˆ")
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # æ‰“å°å…³é”®ç»“æœ
    compliance = analyzer.check_prd_compliance()
    print(f"\nğŸ¯ PRDåˆè§„æ€§å¾—åˆ†: {compliance['overall_score']:.1f}% ({compliance['compliance_level']})")
    print(f"âœ… é€šè¿‡æ£€æŸ¥: {compliance['passed_checks']}/{compliance['total_checks']}")
    
    return 0

if __name__ == "__main__":
    exit(main())