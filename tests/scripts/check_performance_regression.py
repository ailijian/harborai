#!/usr/bin/env python3
"""
æ€§èƒ½å›å½’æ£€æŸ¥è„šæœ¬
ç”¨äºæ£€æµ‹å½“å‰ç‰ˆæœ¬ç›¸å¯¹äºåŸºçº¿ç‰ˆæœ¬çš„æ€§èƒ½å›å½’é—®é¢˜

åŠŸèƒ½ï¼š
1. åŠ è½½æ€§èƒ½åŸºçº¿æ•°æ®
2. è¿è¡Œå½“å‰ç‰ˆæœ¬çš„æ€§èƒ½æµ‹è¯•
3. å¯¹æ¯”åˆ†ææ€§èƒ½æŒ‡æ ‡
4. ç”Ÿæˆå›å½’æ£€æŸ¥æŠ¥å‘Š
5. æ ¹æ®é˜ˆå€¼åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ€§èƒ½å›å½’

ä½œè€…ï¼šailijian
åˆ›å»ºæ—¶é—´ï¼š2024
"""

import json
import sys
import os
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import subprocess
import statistics

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('performance_regression_check.log')
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    name: str
    value: float
    unit: str
    threshold_percent: float = 10.0  # é»˜è®¤é˜ˆå€¼ 10%
    
    
@dataclass
class RegressionResult:
    """å›å½’æ£€æŸ¥ç»“æœ"""
    metric_name: str
    baseline_value: float
    current_value: float
    change_percent: float
    is_regression: bool
    severity: str  # 'low', 'medium', 'high', 'critical'


class PerformanceRegressionChecker:
    """æ€§èƒ½å›å½’æ£€æŸ¥å™¨"""
    
    def __init__(self, baseline_dir: str = None, output_dir: str = None):
        """
        åˆå§‹åŒ–æ€§èƒ½å›å½’æ£€æŸ¥å™¨
        
        Args:
            baseline_dir: åŸºçº¿æ•°æ®ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºæŠ¥å‘Šç›®å½•è·¯å¾„
        """
        self.baseline_dir = Path(baseline_dir or "tests/data/performance_baselines")
        self.output_dir = Path(output_dir or "tests/reports")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ€§èƒ½é˜ˆå€¼é…ç½®
        self.thresholds = {
            'latency': 15.0,      # å»¶è¿Ÿå¢åŠ è¶…è¿‡ 15% ä¸ºå›å½’
            'throughput': -10.0,   # ååé‡ä¸‹é™è¶…è¿‡ 10% ä¸ºå›å½’
            'memory': 20.0,        # å†…å­˜ä½¿ç”¨å¢åŠ è¶…è¿‡ 20% ä¸ºå›å½’
            'cpu': 25.0,           # CPU ä½¿ç”¨å¢åŠ è¶…è¿‡ 25% ä¸ºå›å½’
            'concurrency': -15.0,  # å¹¶å‘æ€§èƒ½ä¸‹é™è¶…è¿‡ 15% ä¸ºå›å½’
        }
        
        # ä¸¥é‡ç¨‹åº¦åˆ†çº§
        self.severity_levels = {
            (0, 5): 'low',
            (5, 15): 'medium', 
            (15, 30): 'high',
            (30, float('inf')): 'critical'
        }
    
    def load_baseline_data(self, baseline_file: str) -> Dict[str, Any]:
        """
        åŠ è½½åŸºçº¿æ€§èƒ½æ•°æ®
        
        Args:
            baseline_file: åŸºçº¿æ–‡ä»¶å
            
        Returns:
            åŸºçº¿æ•°æ®å­—å…¸
        """
        baseline_path = self.baseline_dir / baseline_file
        
        if not baseline_path.exists():
            logger.warning(f"åŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨: {baseline_path}")
            return {}
            
        try:
            with open(baseline_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"âœ… æˆåŠŸåŠ è½½åŸºçº¿æ•°æ®: {baseline_file}")
            return data
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åŸºçº¿æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def run_performance_tests(self) -> Dict[str, Any]:
        """
        è¿è¡Œå½“å‰ç‰ˆæœ¬çš„æ€§èƒ½æµ‹è¯•
        
        Returns:
            å½“å‰æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ€§èƒ½æµ‹è¯•...")
        
        current_results = {
            'timestamp': datetime.now().isoformat(),
            'version': self._get_current_version(),
            'metrics': {}
        }
        
        try:
            # è¿è¡ŒåŸºç¡€æ€§èƒ½æµ‹è¯•
            latency_results = self._run_latency_tests()
            throughput_results = self._run_throughput_tests()
            memory_results = self._run_memory_tests()
            concurrency_results = self._run_concurrency_tests()
            
            current_results['metrics'] = {
                'latency': latency_results,
                'throughput': throughput_results,
                'memory': memory_results,
                'concurrency': concurrency_results
            }
            
            logger.info("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
            return current_results
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return current_results
    
    def _get_current_version(self) -> str:
        """è·å–å½“å‰ç‰ˆæœ¬å·"""
        try:
            # ä» pyproject.toml è¯»å–ç‰ˆæœ¬
            pyproject_path = project_root / "pyproject.toml"
            if pyproject_path.exists():
                with open(pyproject_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    import re
                    match = re.search(r'version = "([^"]+)"', content)
                    if match:
                        return match.group(1)
            return "unknown"
        except Exception:
            return "unknown"
    
    def _run_latency_tests(self) -> Dict[str, float]:
        """è¿è¡Œå»¶è¿Ÿæµ‹è¯•"""
        logger.info("ğŸ“Š è¿è¡Œå»¶è¿Ÿæµ‹è¯•...")
        
        # æ¨¡æ‹Ÿå»¶è¿Ÿæµ‹è¯•ç»“æœï¼ˆå®é™…åº”è¯¥è°ƒç”¨çœŸå®çš„æµ‹è¯•ï¼‰
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å»¶è¿Ÿæµ‹è¯•è„šæœ¬
            cmd = [sys.executable, "-m", "pytest", 
                   "tests/performance/test_basic_performance.py::test_response_time", 
                   "-v", "--tb=short"]
            
            result = subprocess.run(cmd, capture_output=True, text=True, 
                                  cwd=project_root, timeout=300)
            
            # è§£ææµ‹è¯•ç»“æœï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            return {
                'avg_response_time': 0.15,  # 150ms
                'p95_response_time': 0.25,  # 250ms
                'p99_response_time': 0.35,  # 350ms
            }
        except Exception as e:
            logger.warning(f"å»¶è¿Ÿæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {}
    
    def _run_throughput_tests(self) -> Dict[str, float]:
        """è¿è¡Œååé‡æµ‹è¯•"""
        logger.info("ğŸ“Š è¿è¡Œååé‡æµ‹è¯•...")
        
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„ååé‡æµ‹è¯•è„šæœ¬
            return {
                'requests_per_second': 850.0,
                'tokens_per_second': 1200.0,
                'concurrent_requests': 50.0,
            }
        except Exception as e:
            logger.warning(f"ååé‡æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {}
    
    def _run_memory_tests(self) -> Dict[str, float]:
        """è¿è¡Œå†…å­˜ä½¿ç”¨æµ‹è¯•"""
        logger.info("ğŸ“Š è¿è¡Œå†…å­˜æµ‹è¯•...")
        
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å†…å­˜æµ‹è¯•è„šæœ¬
            return {
                'peak_memory_mb': 128.5,
                'avg_memory_mb': 95.2,
                'memory_growth_rate': 0.02,  # 2% per hour
            }
        except Exception as e:
            logger.warning(f"å†…å­˜æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {}
    
    def _run_concurrency_tests(self) -> Dict[str, float]:
        """è¿è¡Œå¹¶å‘æ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸ“Š è¿è¡Œå¹¶å‘æµ‹è¯•...")
        
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å¹¶å‘æµ‹è¯•è„šæœ¬
            return {
                'max_concurrent_users': 100.0,
                'concurrent_throughput': 750.0,
                'error_rate_percent': 0.5,
            }
        except Exception as e:
            logger.warning(f"å¹¶å‘æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {}
    
    def compare_metrics(self, baseline: Dict[str, Any], 
                       current: Dict[str, Any]) -> List[RegressionResult]:
        """
        å¯¹æ¯”æ€§èƒ½æŒ‡æ ‡
        
        Args:
            baseline: åŸºçº¿æ•°æ®
            current: å½“å‰æµ‹è¯•æ•°æ®
            
        Returns:
            å›å½’æ£€æŸ¥ç»“æœåˆ—è¡¨
        """
        logger.info("ğŸ” å¼€å§‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”...")
        
        results = []
        
        for category, current_metrics in current.get('metrics', {}).items():
            baseline_metrics = baseline.get(category, {})
            
            for metric_name, current_value in current_metrics.items():
                if metric_name in baseline_metrics:
                    baseline_value = baseline_metrics[metric_name]
                    
                    # è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”
                    if baseline_value != 0:
                        change_percent = ((current_value - baseline_value) / baseline_value) * 100
                    else:
                        change_percent = 0.0
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºå›å½’
                    threshold = self.thresholds.get(category, 10.0)
                    is_regression = self._is_regression(change_percent, threshold, category)
                    
                    # ç¡®å®šä¸¥é‡ç¨‹åº¦
                    severity = self._get_severity(abs(change_percent))
                    
                    result = RegressionResult(
                        metric_name=f"{category}.{metric_name}",
                        baseline_value=baseline_value,
                        current_value=current_value,
                        change_percent=change_percent,
                        is_regression=is_regression,
                        severity=severity
                    )
                    
                    results.append(result)
        
        logger.info(f"âœ… å®Œæˆ {len(results)} ä¸ªæŒ‡æ ‡çš„å¯¹æ¯”")
        return results
    
    def _is_regression(self, change_percent: float, threshold: float, 
                      category: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ€§èƒ½å›å½’
        
        Args:
            change_percent: å˜åŒ–ç™¾åˆ†æ¯”
            threshold: é˜ˆå€¼
            category: æŒ‡æ ‡ç±»åˆ«
            
        Returns:
            æ˜¯å¦ä¸ºå›å½’
        """
        # å¯¹äºååé‡å’Œå¹¶å‘æ€§èƒ½ï¼Œä¸‹é™æ˜¯å›å½’
        if category in ['throughput', 'concurrency']:
            return change_percent < threshold
        # å¯¹äºå»¶è¿Ÿå’Œå†…å­˜ï¼Œå¢åŠ æ˜¯å›å½’
        else:
            return change_percent > threshold
    
    def _get_severity(self, change_percent: float) -> str:
        """è·å–ä¸¥é‡ç¨‹åº¦"""
        for (min_val, max_val), severity in self.severity_levels.items():
            if min_val <= change_percent < max_val:
                return severity
        return 'low'
    
    def generate_report(self, results: List[RegressionResult], 
                       output_file: str = None) -> str:
        """
        ç”Ÿæˆå›å½’æ£€æŸ¥æŠ¥å‘Š
        
        Args:
            results: å›å½’æ£€æŸ¥ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"performance_regression_report_{timestamp}.md"
        
        report_path = self.output_dir / output_file
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_metrics = len(results)
        regressions = [r for r in results if r.is_regression]
        regression_count = len(regressions)
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        severity_counts = {}
        for result in regressions:
            severity_counts[result.severity] = severity_counts.get(result.severity, 0) + 1
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = f"""# æ€§èƒ½å›å½’æ£€æŸ¥æŠ¥å‘Š

## ğŸ“Š æ€»ä½“æ¦‚å†µ

- **æ£€æŸ¥æ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **æ€»æŒ‡æ ‡æ•°**: {total_metrics}
- **å›å½’æŒ‡æ ‡æ•°**: {regression_count}
- **å›å½’ç‡**: {(regression_count/total_metrics*100):.1f}%

## ğŸš¨ ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ

"""
        
        for severity in ['critical', 'high', 'medium', 'low']:
            count = severity_counts.get(severity, 0)
            if count > 0:
                emoji = {'critical': 'ğŸ”´', 'high': 'ğŸŸ ', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}[severity]
                report_content += f"- {emoji} **{severity.upper()}**: {count} ä¸ª\n"
        
        report_content += "\n## ğŸ“ˆ è¯¦ç»†ç»“æœ\n\n"
        
        if regressions:
            report_content += "### ğŸš¨ å‘ç°çš„æ€§èƒ½å›å½’\n\n"
            report_content += "| æŒ‡æ ‡ | åŸºçº¿å€¼ | å½“å‰å€¼ | å˜åŒ– | ä¸¥é‡ç¨‹åº¦ |\n"
            report_content += "|------|--------|--------|------|----------|\n"
            
            for result in sorted(regressions, key=lambda x: abs(x.change_percent), reverse=True):
                emoji = {'critical': 'ğŸ”´', 'high': 'ğŸŸ ', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}[result.severity]
                report_content += f"| {result.metric_name} | {result.baseline_value:.3f} | {result.current_value:.3f} | {result.change_percent:+.1f}% | {emoji} {result.severity} |\n"
        
        # æ·»åŠ æ‰€æœ‰æŒ‡æ ‡çš„è¯¦ç»†ä¿¡æ¯
        report_content += "\n### ğŸ“Š æ‰€æœ‰æŒ‡æ ‡å¯¹æ¯”\n\n"
        report_content += "| æŒ‡æ ‡ | åŸºçº¿å€¼ | å½“å‰å€¼ | å˜åŒ– | çŠ¶æ€ |\n"
        report_content += "|------|--------|--------|------|------|\n"
        
        for result in results:
            status = "âŒ å›å½’" if result.is_regression else "âœ… æ­£å¸¸"
            report_content += f"| {result.metric_name} | {result.baseline_value:.3f} | {result.current_value:.3f} | {result.change_percent:+.1f}% | {status} |\n"
        
        # æ·»åŠ å»ºè®®
        report_content += "\n## ğŸ’¡ å»ºè®®\n\n"
        
        if regression_count == 0:
            report_content += "ğŸ‰ **æ­å–œï¼** æœªå‘ç°æ€§èƒ½å›å½’é—®é¢˜ã€‚\n"
        else:
            critical_count = severity_counts.get('critical', 0)
            high_count = severity_counts.get('high', 0)
            
            if critical_count > 0:
                report_content += "ğŸš¨ **ä¸¥é‡è­¦å‘Š**: å‘ç°ä¸¥é‡æ€§èƒ½å›å½’ï¼Œå»ºè®®ç«‹å³ä¿®å¤åå†å‘å¸ƒã€‚\n"
            elif high_count > 0:
                report_content += "âš ï¸ **è­¦å‘Š**: å‘ç°é«˜çº§åˆ«æ€§èƒ½å›å½’ï¼Œå»ºè®®ä¼˜åŒ–åå‘å¸ƒã€‚\n"
            else:
                report_content += "â„¹ï¸ **æç¤º**: å‘ç°è½»å¾®æ€§èƒ½å›å½’ï¼Œå¯è€ƒè™‘ä¼˜åŒ–ã€‚\n"
        
        # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            logger.info(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            return str(report_path)
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return ""
    
    def check_regression(self, baseline_file: str = "baseline_v1.0.json") -> Tuple[bool, str]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ€§èƒ½å›å½’æ£€æŸ¥
        
        Args:
            baseline_file: åŸºçº¿æ–‡ä»¶å
            
        Returns:
            (æ˜¯å¦æœ‰å›å½’, æŠ¥å‘Šæ–‡ä»¶è·¯å¾„)
        """
        logger.info("ğŸ” å¼€å§‹æ€§èƒ½å›å½’æ£€æŸ¥...")
        
        # 1. åŠ è½½åŸºçº¿æ•°æ®
        baseline_data = self.load_baseline_data(baseline_file)
        if not baseline_data:
            logger.error("âŒ æ— æ³•åŠ è½½åŸºçº¿æ•°æ®ï¼Œè·³è¿‡å›å½’æ£€æŸ¥")
            return False, ""
        
        # 2. è¿è¡Œå½“å‰æ€§èƒ½æµ‹è¯•
        current_data = self.run_performance_tests()
        if not current_data.get('metrics'):
            logger.error("âŒ æ— æ³•è·å–å½“å‰æ€§èƒ½æ•°æ®ï¼Œè·³è¿‡å›å½’æ£€æŸ¥")
            return False, ""
        
        # 3. å¯¹æ¯”åˆ†æ
        results = self.compare_metrics(baseline_data, current_data)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report_path = self.generate_report(results)
        
        # 5. åˆ¤æ–­æ˜¯å¦æœ‰å›å½’
        has_regression = any(r.is_regression for r in results)
        critical_regressions = [r for r in results if r.is_regression and r.severity == 'critical']
        
        if has_regression:
            logger.warning(f"âš ï¸ å‘ç° {len([r for r in results if r.is_regression])} ä¸ªæ€§èƒ½å›å½’")
            if critical_regressions:
                logger.error(f"ğŸš¨ å‘ç° {len(critical_regressions)} ä¸ªä¸¥é‡æ€§èƒ½å›å½’")
        else:
            logger.info("âœ… æœªå‘ç°æ€§èƒ½å›å½’")
        
        return has_regression, report_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ€§èƒ½å›å½’æ£€æŸ¥å·¥å…·")
    parser.add_argument("--baseline", default="baseline_v1.0.json",
                       help="åŸºçº¿æ–‡ä»¶å (é»˜è®¤: baseline_v1.0.json)")
    parser.add_argument("--baseline-dir", 
                       help="åŸºçº¿æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output-dir",
                       help="è¾“å‡ºæŠ¥å‘Šç›®å½•è·¯å¾„")
    parser.add_argument("--fail-on-regression", action="store_true",
                       help="å‘ç°å›å½’æ—¶ä»¥éé›¶çŠ¶æ€ç é€€å‡º")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºæ£€æŸ¥å™¨
    checker = PerformanceRegressionChecker(
        baseline_dir=args.baseline_dir,
        output_dir=args.output_dir
    )
    
    # æ‰§è¡Œæ£€æŸ¥
    has_regression, report_path = checker.check_regression(args.baseline)
    
    # è¾“å‡ºç»“æœ
    if report_path:
        print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_path}")
    
    if has_regression:
        print("âŒ å‘ç°æ€§èƒ½å›å½’")
        if args.fail_on_regression:
            sys.exit(1)
    else:
        print("âœ… æœªå‘ç°æ€§èƒ½å›å½’")
    
    sys.exit(0)


if __name__ == "__main__":
    main()