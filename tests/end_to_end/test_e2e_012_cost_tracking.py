#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç«¯åˆ°ç«¯æµ‹è¯•ï¼šæˆæœ¬ç»Ÿè®¡åŠŸèƒ½

åŸºäº HarborAIç«¯åˆ°ç«¯æµ‹è¯•æ–¹æ¡ˆ.md L560-589 çš„è¦æ±‚ï¼ŒéªŒè¯ï¼š
1. è°ƒç”¨æˆæœ¬ç»Ÿè®¡åŠŸèƒ½
2. tokenä½¿ç”¨é‡ç»Ÿè®¡æ­£ç¡®æ€§
3. æˆæœ¬è®¡ç®—å‡†ç¡®æ€§
4. æˆæœ¬ä¿¡æ¯æ ¼å¼æ ‡å‡†æ€§
5. å¼‚æ­¥æˆæœ¬è¿½è¸ªä¸é˜»å¡ä¸»çº¿ç¨‹
6. æ‰€æœ‰7ä¸ªå¯ç”¨æ¨¡å‹çš„æˆæœ¬ç»Ÿè®¡
"""

import os
import json
import time
import asyncio
import tempfile
import shutil
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from unittest.mock import patch, MagicMock
from decimal import Decimal

import pytest
from harborai import HarborAI
from harborai.core.async_cost_tracking import AsyncCostTracker, get_async_cost_tracker
from harborai.core.cost_tracking import CostTracker
from harborai.utils.tracer import get_or_create_trace_id, TraceContext


# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
def load_env_file():
    """åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡"""
    env_file = Path(__file__).parent.parent.parent / ".env"
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# åœ¨æ¨¡å—åŠ è½½æ—¶åŠ è½½ç¯å¢ƒå˜é‡
load_env_file()


class TestCostTracking:
    """æˆæœ¬ç»Ÿè®¡åŠŸèƒ½æµ‹è¯•ç±»"""
    
    @classmethod
    def setup_class(cls):
        """è®¾ç½®æµ‹è¯•ç±»"""
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_env_file()
        
        # æ£€æŸ¥å¯ç”¨çš„APIé…ç½®
        cls.available_configs = {}
        
        # æ£€æŸ¥DeepSeeké…ç½®
        if os.getenv("DEEPSEEK_API_KEY") and os.getenv("DEEPSEEK_BASE_URL"):
            cls.available_configs["DEEPSEEK"] = {
                "api_key": os.getenv("DEEPSEEK_API_KEY"),
                "base_url": os.getenv("DEEPSEEK_BASE_URL")
            }
        
        # æ£€æŸ¥æ–‡å¿ƒä¸€è¨€é…ç½®
        if os.getenv("WENXIN_API_KEY") and os.getenv("WENXIN_BASE_URL"):
            cls.available_configs["WENXIN"] = {
                "api_key": os.getenv("WENXIN_API_KEY"),
                "base_url": os.getenv("WENXIN_BASE_URL")
            }
        
        # æ£€æŸ¥è±†åŒ…é…ç½®
        if os.getenv("DOUBAO_API_KEY") and os.getenv("DOUBAO_BASE_URL"):
            cls.available_configs["DOUBAO"] = {
                "api_key": os.getenv("DOUBAO_API_KEY"),
                "base_url": os.getenv("DOUBAO_BASE_URL")
            }
        
        # å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆåŸºäºæ–‡æ¡£ä¸­çš„æ¨¡å‹åˆ—è¡¨ï¼‰
        cls.available_models = {
            "DEEPSEEK": [
                {"model": "deepseek-chat", "is_reasoning": False},
                {"model": "deepseek-reasoner", "is_reasoning": True}
            ],
            "WENXIN": [
                {"model": "ernie-3.5-8k", "is_reasoning": False},
                {"model": "ernie-4.0-turbo-8k", "is_reasoning": False},
                {"model": "ernie-x1-turbo-32k", "is_reasoning": True}
            ],
            "DOUBAO": [
                {"model": "doubao-1-5-pro-32k-character-250715", "is_reasoning": False},
                {"model": "doubao-seed-1-6-250615", "is_reasoning": True}
            ]
        }
        
        print(f"ğŸ”§ æ£€æµ‹åˆ°çš„APIé…ç½®: {list(cls.available_configs.keys())}")
        
        if not cls.available_configs:
            pytest.skip("æ²¡æœ‰å¯ç”¨çš„APIé…ç½®")
    
    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•çš„è®¾ç½®"""
        # è®¾ç½®æµ‹è¯•æœŸé—´çš„æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘ä¸å¿…è¦çš„è¾“å‡º
        logging.getLogger('harborai.core.cost_tracking').setLevel(logging.INFO)
        logging.getLogger('harborai.core.async_cost_tracking').setLevel(logging.INFO)
        
        # è·å–å…¨å±€å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨
        self.async_cost_tracker = get_async_cost_tracker()
        
        # åˆ›å»ºåŒæ­¥æˆæœ¬è¿½è¸ªå™¨ç”¨äºéªŒè¯
        self.sync_cost_tracker = CostTracker()
    
    def teardown_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•çš„æ¸…ç†"""
        # åˆ·æ–°å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨çš„å¾…å¤„ç†è°ƒç”¨
        if self.async_cost_tracker:
            asyncio.run(self.async_cost_tracker.flush_pending())
    
    @classmethod
    def teardown_class(cls):
        """æµ‹è¯•ç±»æ¸…ç†"""
        print("\n=== å¼€å§‹æ¸…ç†æˆæœ¬è¿½è¸ªæµ‹è¯•èµ„æº ===")
        
        try:
            # è·å–å¹¶æ¸…ç†å…¨å±€å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨
            async_cost_tracker = get_async_cost_tracker()
            if async_cost_tracker:
                asyncio.run(async_cost_tracker.flush_pending())
                print("âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨å·²åˆ·æ–°")
        except Exception as e:
            print(f"âš  å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨æ¸…ç†æ—¶å‡ºç°è­¦å‘Šï¼š{e}")
        
        # æ¸…ç†ä»»ä½•å‰©ä½™çš„å¼‚æ­¥ä»»åŠ¡
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            if loop and not loop.is_closed():
                pending = asyncio.all_tasks(loop)
                if pending:
                    print(f"âš  å‘ç° {len(pending)} ä¸ªå¾…å¤„ç†çš„å¼‚æ­¥ä»»åŠ¡ï¼Œæ­£åœ¨å–æ¶ˆ...")
                    for task in pending:
                        task.cancel()
        except Exception as e:
            print(f"âš  æ¸…ç†å¼‚æ­¥ä»»åŠ¡æ—¶å‡ºç°è­¦å‘Šï¼š{e}")
        
        print("=== æˆæœ¬è¿½è¸ªæµ‹è¯•èµ„æºæ¸…ç†å®Œæˆ ===")
    
    def test_basic_cost_tracking(self):
        """æµ‹è¯•åŸºæœ¬æˆæœ¬ç»Ÿè®¡åŠŸèƒ½"""
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é…ç½®
        vendor = list(self.available_configs.keys())[0]
        config = self.available_configs[vendor]
        model_info = self.available_models[vendor][0]
        model = model_info["model"]
        
        print(f"ä½¿ç”¨ {vendor} çš„ {model} æ¨¡å‹è¿›è¡Œæˆæœ¬ç»Ÿè®¡æµ‹è¯•")
        
        # åˆ›å»ºHarborAIå®¢æˆ·ç«¯
        client = HarborAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        
        # ç”Ÿæˆtrace_id
        trace_id = get_or_create_trace_id()
        print(f"âœ“ ç”Ÿæˆtrace_id: {trace_id}")
        
        # è®°å½•å¼€å§‹æ—¶é—´ï¼ŒéªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ªä¸é˜»å¡ä¸»çº¿ç¨‹
        start_time = time.time()
        
        with TraceContext(trace_id):
            # å‘é€æµ‹è¯•è¯·æ±‚ï¼Œå¯ç”¨æˆæœ¬è¿½è¸ª
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"}
                ],
                max_tokens=150,
                cost_tracking=True  # å¯ç”¨æˆæœ¬è¿½è¸ª
            )
        
        # éªŒè¯è°ƒç”¨æ—¶é—´ï¼ˆå¼‚æ­¥æˆæœ¬è¿½è¸ªä¸åº”æ˜¾è‘—å¢åŠ å“åº”æ—¶é—´ï¼‰
        call_duration = time.time() - start_time
        print(f"âœ“ APIè°ƒç”¨è€—æ—¶: {call_duration:.2f}ç§’")
        
        # éªŒè¯å“åº”åŸºæœ¬ç»“æ„
        assert response is not None
        assert hasattr(response, 'choices')
        assert len(response.choices) > 0
        assert response.choices[0].message.content
        
        print(f"âœ“ APIè°ƒç”¨æˆåŠŸï¼Œå“åº”å†…å®¹: {response.choices[0].message.content[:50]}...")
        
        # éªŒè¯usageä¿¡æ¯ï¼ˆtokenä½¿ç”¨é‡ç»Ÿè®¡ï¼‰
        assert hasattr(response, 'usage'), "å“åº”åº”åŒ…å«usageå­—æ®µ"
        assert hasattr(response.usage, 'prompt_tokens'), "usageåº”åŒ…å«prompt_tokens"
        assert hasattr(response.usage, 'completion_tokens'), "usageåº”åŒ…å«completion_tokens"
        assert hasattr(response.usage, 'total_tokens'), "usageåº”åŒ…å«total_tokens"
        
        # éªŒè¯tokenæ•°é‡çš„åˆç†æ€§
        assert response.usage.prompt_tokens > 0, "prompt_tokensåº”å¤§äº0"
        assert response.usage.completion_tokens > 0, "completion_tokensåº”å¤§äº0"
        assert response.usage.total_tokens == response.usage.prompt_tokens + response.usage.completion_tokens, \
            "total_tokensåº”ç­‰äºprompt_tokens + completion_tokens"
        
        print(f"âœ“ Tokenä½¿ç”¨é‡ç»Ÿè®¡:")
        print(f"   - prompt_tokens: {response.usage.prompt_tokens}")
        print(f"   - completion_tokens: {response.usage.completion_tokens}")
        print(f"   - total_tokens: {response.usage.total_tokens}")
        
        # éªŒè¯æˆæœ¬ä¿¡æ¯ï¼ˆå¦‚æœå®ç°äº†ï¼‰
        if hasattr(response, 'cost_info'):
            assert 'total_cost' in response.cost_info, "cost_infoåº”åŒ…å«total_cost"
            assert response.cost_info['total_cost'] >= 0, "total_coståº”å¤§äºç­‰äº0"
            
            if 'currency' in response.cost_info:
                assert response.cost_info['currency'] in ['USD', 'RMB', 'CNY'], \
                    f"currencyåº”ä¸ºUSDã€RMBæˆ–CNYï¼Œå®é™…ä¸º: {response.cost_info['currency']}"
            
            print(f"âœ“ æˆæœ¬ä¿¡æ¯:")
            print(f"   - total_cost: {response.cost_info['total_cost']}")
            if 'currency' in response.cost_info:
                print(f"   - currency: {response.cost_info['currency']}")
        else:
            print("âš ï¸ å“åº”ä¸­æœªåŒ…å«cost_infoå­—æ®µï¼Œå¯èƒ½æˆæœ¬è®¡ç®—åŠŸèƒ½æœªå®Œå…¨å®ç°")
        
        # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
        print("â³ ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†...")
        time.sleep(2)
        
        # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨çŠ¶æ€
        if self.async_cost_tracker:
            stats = asyncio.run(self.async_cost_tracker.get_cost_summary())
            print(f"âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨ç»Ÿè®¡: {stats}")
        
        print("âœ“ åŸºæœ¬æˆæœ¬ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
    
    def test_cost_tracking_all_models(self):
        """æµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„æˆæœ¬ç»Ÿè®¡"""
        print("ğŸ”„ å¼€å§‹æµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„æˆæœ¬ç»Ÿè®¡...")
        
        cost_results = []
        
        for vendor, config in self.available_configs.items():
            for model_info in self.available_models[vendor]:
                model = model_info["model"]
                is_reasoning = model_info["is_reasoning"]
                
                print(f"\n--- æµ‹è¯•æ¨¡å‹: {vendor} - {model} ({'æ¨ç†æ¨¡å‹' if is_reasoning else 'éæ¨ç†æ¨¡å‹'}) ---")
                
                try:
                    # åˆ›å»ºå®¢æˆ·ç«¯
                    client = HarborAI(
                        api_key=config["api_key"],
                        base_url=config["base_url"]
                    )
                    
                    # ç”Ÿæˆtrace_id
                    trace_id = get_or_create_trace_id()
                    
                    # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´æµ‹è¯•å†…å®¹
                    if is_reasoning:
                        test_content = "åˆ†æä¸€ä¸‹äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å‰æ™¯å’ŒæŒ‘æˆ˜"
                    else:
                        test_content = "ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
                    
                    start_time = time.time()
                    
                    with TraceContext(trace_id):
                        response = client.chat.completions.create(
                            model=model,
                            messages=[
                                {"role": "user", "content": test_content}
                            ],
                            max_tokens=100,
                            cost_tracking=True
                        )
                    
                    call_duration = time.time() - start_time
                    
                    # éªŒè¯å“åº”
                    assert response is not None
                    assert hasattr(response, 'usage')
                    
                    # æ”¶é›†æˆæœ¬ç»Ÿè®¡ç»“æœ
                    cost_result = {
                        "vendor": vendor,
                        "model": model,
                        "is_reasoning": is_reasoning,
                        "call_duration": call_duration,
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens,
                        "cost_info": getattr(response, 'cost_info', None)
                    }
                    
                    # éªŒè¯æ¨ç†æ¨¡å‹çš„ç‰¹æ®Šå­—æ®µ
                    if is_reasoning and hasattr(response.choices[0].message, 'reasoning_content'):
                        cost_result["has_reasoning_content"] = True
                        print(f"âœ“ æ¨ç†æ¨¡å‹åŒ…å«æ€è€ƒè¿‡ç¨‹")
                    
                    cost_results.append(cost_result)
                    
                    print(f"âœ“ {model} æˆæœ¬ç»Ÿè®¡æˆåŠŸ:")
                    print(f"   - è°ƒç”¨è€—æ—¶: {call_duration:.2f}ç§’")
                    print(f"   - Tokenä½¿ç”¨: {response.usage.total_tokens} (è¾“å…¥:{response.usage.prompt_tokens}, è¾“å‡º:{response.usage.completion_tokens})")
                    
                    if hasattr(response, 'cost_info'):
                        print(f"   - æˆæœ¬ä¿¡æ¯: {response.cost_info}")
                    
                except Exception as e:
                    print(f"âŒ {model} æµ‹è¯•å¤±è´¥: {e}")
                    # è®°å½•å¤±è´¥ä½†ä¸ä¸­æ–­æµ‹è¯•
                    cost_results.append({
                        "vendor": vendor,
                        "model": model,
                        "is_reasoning": is_reasoning,
                        "error": str(e)
                    })
                
                # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(1)
        
        # ç­‰å¾…æ‰€æœ‰å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
        print("\nâ³ ç­‰å¾…æ‰€æœ‰å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ...")
        time.sleep(3)
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        successful_tests = [r for r in cost_results if "error" not in r]
        failed_tests = [r for r in cost_results if "error" in r]
        
        print(f"\nğŸ“Š æˆæœ¬ç»Ÿè®¡æµ‹è¯•æ€»ç»“:")
        print(f"   - æ€»æµ‹è¯•æ¨¡å‹æ•°: {len(cost_results)}")
        print(f"   - æˆåŠŸæµ‹è¯•æ•°: {len(successful_tests)}")
        print(f"   - å¤±è´¥æµ‹è¯•æ•°: {len(failed_tests)}")
        
        if successful_tests:
            print(f"\nâœ“ æˆåŠŸæµ‹è¯•çš„æ¨¡å‹:")
            for result in successful_tests:
                print(f"   - {result['vendor']}-{result['model']}: {result['total_tokens']} tokens")
        
        if failed_tests:
            print(f"\nâŒ å¤±è´¥æµ‹è¯•çš„æ¨¡å‹:")
            for result in failed_tests:
                print(f"   - {result['vendor']}-{result['model']}: {result['error']}")
        
        # éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹æµ‹è¯•æˆåŠŸ
        assert len(successful_tests) > 0, "è‡³å°‘åº”æœ‰ä¸€ä¸ªæ¨¡å‹çš„æˆæœ¬ç»Ÿè®¡æµ‹è¯•æˆåŠŸ"
        
        print("âœ“ æ‰€æœ‰å¯ç”¨æ¨¡å‹æˆæœ¬ç»Ÿè®¡æµ‹è¯•å®Œæˆ")
    
    def test_async_cost_tracking_non_blocking(self):
        """æµ‹è¯•å¼‚æ­¥æˆæœ¬è¿½è¸ªä¸é˜»å¡ä¸»çº¿ç¨‹"""
        print("ğŸ”„ æµ‹è¯•å¼‚æ­¥æˆæœ¬è¿½è¸ªçš„éé˜»å¡ç‰¹æ€§...")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é…ç½®
        vendor = list(self.available_configs.keys())[0]
        config = self.available_configs[vendor]
        model = self.available_models[vendor][0]["model"]
        
        client = HarborAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        
        # è¿›è¡Œå¤šæ¬¡å¹¶å‘è°ƒç”¨ï¼Œæµ‹è¯•å¼‚æ­¥æˆæœ¬è¿½è¸ªçš„æ€§èƒ½
        call_times = []
        responses = []
        
        for i in range(3):  # è¿›è¡Œ3æ¬¡è°ƒç”¨
            trace_id = get_or_create_trace_id()
            
            start_time = time.time()
            
            with TraceContext(trace_id):
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "user", "content": f"è¿™æ˜¯ç¬¬{i+1}æ¬¡æµ‹è¯•è°ƒç”¨ï¼Œè¯·ç®€å•å›å¤"}
                    ],
                    max_tokens=50,
                    cost_tracking=True
                )
            
            call_time = time.time() - start_time
            call_times.append(call_time)
            responses.append(response)
            
            print(f"âœ“ ç¬¬{i+1}æ¬¡è°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {call_time:.2f}ç§’")
            
            # éªŒè¯å“åº”å’Œusageä¿¡æ¯
            assert response is not None
            assert hasattr(response, 'usage')
            assert response.usage.total_tokens > 0
        
        # è®¡ç®—å¹³å‡è°ƒç”¨æ—¶é—´
        avg_call_time = sum(call_times) / len(call_times)
        max_call_time = max(call_times)
        
        print(f"ğŸ“Š è°ƒç”¨æ—¶é—´ç»Ÿè®¡:")
        print(f"   - å¹³å‡è°ƒç”¨æ—¶é—´: {avg_call_time:.2f}ç§’")
        print(f"   - æœ€å¤§è°ƒç”¨æ—¶é—´: {max_call_time:.2f}ç§’")
        print(f"   - æ‰€æœ‰è°ƒç”¨æ—¶é—´: {[f'{t:.2f}s' for t in call_times]}")
        
        # éªŒè¯è°ƒç”¨æ—¶é—´åˆç†ï¼ˆå¼‚æ­¥æˆæœ¬è¿½è¸ªä¸åº”æ˜¾è‘—å¢åŠ å“åº”æ—¶é—´ï¼‰
        # å‡è®¾æ­£å¸¸APIè°ƒç”¨åº”åœ¨10ç§’å†…å®Œæˆ
        assert max_call_time < 10.0, f"è°ƒç”¨æ—¶é—´è¿‡é•¿ï¼Œå¯èƒ½æˆæœ¬è¿½è¸ªé˜»å¡äº†ä¸»çº¿ç¨‹: {max_call_time:.2f}ç§’"
        
        # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
        print("â³ ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ...")
        time.sleep(3)
        
        # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨ç»Ÿè®¡ä¿¡æ¯
        if self.async_cost_tracker:
            stats = asyncio.run(self.async_cost_tracker.get_cost_summary())
            print(f"âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨æœ€ç»ˆç»Ÿè®¡: {stats}")
        
        print("âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªéé˜»å¡æµ‹è¯•é€šè¿‡")
    
    def test_cost_info_format_validation(self):
        """æµ‹è¯•æˆæœ¬ä¿¡æ¯æ ¼å¼æ ‡å‡†æ€§"""
        print("ğŸ”„ æµ‹è¯•æˆæœ¬ä¿¡æ¯æ ¼å¼æ ‡å‡†æ€§...")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é…ç½®
        vendor = list(self.available_configs.keys())[0]
        config = self.available_configs[vendor]
        model = self.available_models[vendor][0]["model"]
        
        client = HarborAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        
        trace_id = get_or_create_trace_id()
        
        with TraceContext(trace_id):
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": "æµ‹è¯•æˆæœ¬ä¿¡æ¯æ ¼å¼"}
                ],
                max_tokens=50,
                cost_tracking=True
            )
        
        # éªŒè¯usageå­—æ®µæ ¼å¼
        assert hasattr(response, 'usage'), "å“åº”å¿…é¡»åŒ…å«usageå­—æ®µ"
        usage = response.usage
        
        # éªŒè¯usageå­—æ®µçš„æ•°æ®ç±»å‹
        assert isinstance(usage.prompt_tokens, int), "prompt_tokenså¿…é¡»æ˜¯æ•´æ•°"
        assert isinstance(usage.completion_tokens, int), "completion_tokenså¿…é¡»æ˜¯æ•´æ•°"
        assert isinstance(usage.total_tokens, int), "total_tokenså¿…é¡»æ˜¯æ•´æ•°"
        
        # éªŒè¯tokenæ•°é‡çš„é€»è¾‘å…³ç³»
        assert usage.prompt_tokens >= 0, "prompt_tokenså¿…é¡»éè´Ÿ"
        assert usage.completion_tokens >= 0, "completion_tokenså¿…é¡»éè´Ÿ"
        assert usage.total_tokens >= usage.prompt_tokens + usage.completion_tokens, \
            "total_tokenså¿…é¡»å¤§äºç­‰äºprompt_tokens + completion_tokens"
        
        print(f"âœ“ usageå­—æ®µæ ¼å¼éªŒè¯é€šè¿‡:")
        print(f"   - prompt_tokens: {usage.prompt_tokens} (ç±»å‹: {type(usage.prompt_tokens).__name__})")
        print(f"   - completion_tokens: {usage.completion_tokens} (ç±»å‹: {type(usage.completion_tokens).__name__})")
        print(f"   - total_tokens: {usage.total_tokens} (ç±»å‹: {type(usage.total_tokens).__name__})")
        
        # éªŒè¯cost_infoå­—æ®µæ ¼å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(response, 'cost_info') and response.cost_info:
            cost_info = response.cost_info
            print(f"âœ“ æ£€æµ‹åˆ°cost_infoå­—æ®µ: {cost_info}")
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if 'total_cost' in cost_info:
                assert isinstance(cost_info['total_cost'], (int, float, Decimal)), \
                    f"total_costå¿…é¡»æ˜¯æ•°å­—ç±»å‹ï¼Œå®é™…ç±»å‹: {type(cost_info['total_cost'])}"
                assert cost_info['total_cost'] >= 0, "total_costå¿…é¡»éè´Ÿ"
            
            # éªŒè¯è´§å¸å­—æ®µ
            if 'currency' in cost_info:
                assert isinstance(cost_info['currency'], str), "currencyå¿…é¡»æ˜¯å­—ç¬¦ä¸²"
                assert cost_info['currency'] in ['USD', 'RMB', 'CNY'], \
                    f"currencyå¿…é¡»æ˜¯USDã€RMBæˆ–CNYï¼Œå®é™…å€¼: {cost_info['currency']}"
            
            print(f"âœ“ cost_infoå­—æ®µæ ¼å¼éªŒè¯é€šè¿‡")
        else:
            print("âš ï¸ å“åº”ä¸­æœªåŒ…å«cost_infoå­—æ®µ")
        
        print("âœ“ æˆæœ¬ä¿¡æ¯æ ¼å¼æ ‡å‡†æ€§æµ‹è¯•é€šè¿‡")
    
    def test_sync_model_call_cost_tracking(self):
        """æµ‹è¯•åŒæ­¥æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡"""
        print("ğŸ”„ æµ‹è¯•åŒæ­¥æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡...")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é…ç½®
        vendor = list(self.available_configs.keys())[0]
        config = self.available_configs[vendor]
        model = self.available_models[vendor][0]["model"]
        
        print(f"ä½¿ç”¨ {vendor} çš„ {model} æ¨¡å‹è¿›è¡ŒåŒæ­¥è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•")
        
        # åˆ›å»ºHarborAIå®¢æˆ·ç«¯
        client = HarborAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        
        # ç”Ÿæˆtrace_id
        trace_id = get_or_create_trace_id()
        print(f"âœ“ ç”Ÿæˆtrace_id: {trace_id}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        with TraceContext(trace_id):
            # å‘é€åŒæ­¥æµ‹è¯•è¯·æ±‚ï¼Œå¯ç”¨æˆæœ¬è¿½è¸ª
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€"}
                ],
                max_tokens=100,
                cost_tracking=True,  # å¯ç”¨æˆæœ¬è¿½è¸ª
                stream=False  # æ˜ç¡®æŒ‡å®šéæµå¼è°ƒç”¨
            )
        
        call_duration = time.time() - start_time
        print(f"âœ“ åŒæ­¥APIè°ƒç”¨è€—æ—¶: {call_duration:.2f}ç§’")
        
        # éªŒè¯å“åº”åŸºæœ¬ç»“æ„
        assert response is not None
        assert hasattr(response, 'choices')
        assert len(response.choices) > 0
        assert response.choices[0].message.content
        
        print(f"âœ“ åŒæ­¥è°ƒç”¨æˆåŠŸï¼Œå“åº”å†…å®¹: {response.choices[0].message.content[:50]}...")
        
        # éªŒè¯usageä¿¡æ¯ï¼ˆtokenä½¿ç”¨é‡ç»Ÿè®¡ï¼‰
        assert hasattr(response, 'usage'), "åŒæ­¥å“åº”åº”åŒ…å«usageå­—æ®µ"
        assert hasattr(response.usage, 'prompt_tokens'), "usageåº”åŒ…å«prompt_tokens"
        assert hasattr(response.usage, 'completion_tokens'), "usageåº”åŒ…å«completion_tokens"
        assert hasattr(response.usage, 'total_tokens'), "usageåº”åŒ…å«total_tokens"
        
        # éªŒè¯tokenæ•°é‡çš„åˆç†æ€§
        assert response.usage.prompt_tokens > 0, "prompt_tokensåº”å¤§äº0"
        assert response.usage.completion_tokens > 0, "completion_tokensåº”å¤§äº0"
        assert response.usage.total_tokens == response.usage.prompt_tokens + response.usage.completion_tokens
        
        print(f"âœ“ åŒæ­¥è°ƒç”¨Tokenä½¿ç”¨é‡ç»Ÿè®¡:")
        print(f"   - prompt_tokens: {response.usage.prompt_tokens}")
        print(f"   - completion_tokens: {response.usage.completion_tokens}")
        print(f"   - total_tokens: {response.usage.total_tokens}")
        
        # éªŒè¯æˆæœ¬ä¿¡æ¯
        if hasattr(response, 'cost_info'):
            assert 'total_cost' in response.cost_info, "cost_infoåº”åŒ…å«total_cost"
            assert response.cost_info['total_cost'] >= 0, "total_coståº”å¤§äºç­‰äº0"
            print(f"âœ“ åŒæ­¥è°ƒç”¨æˆæœ¬ä¿¡æ¯: {response.cost_info}")
        
        # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
        time.sleep(2)
        
        # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨çŠ¶æ€
        if self.async_cost_tracker:
            stats = asyncio.run(self.async_cost_tracker.get_cost_summary())
            print(f"âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨ç»Ÿè®¡: {stats}")
        
        print("âœ“ åŒæ­¥æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
    
    def test_async_model_call_cost_tracking(self):
        """æµ‹è¯•å¼‚æ­¥æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡"""
        print("ğŸ”„ æµ‹è¯•å¼‚æ­¥æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡...")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é…ç½®
        vendor = list(self.available_configs.keys())[0]
        config = self.available_configs[vendor]
        model = self.available_models[vendor][0]["model"]
        
        print(f"ä½¿ç”¨ {vendor} çš„ {model} æ¨¡å‹è¿›è¡Œå¼‚æ­¥è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•")
        
        # åˆ›å»ºHarborAIå®¢æˆ·ç«¯
        client = HarborAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        
        # ç”Ÿæˆtrace_id
        trace_id = get_or_create_trace_id()
        print(f"âœ“ ç”Ÿæˆtrace_id: {trace_id}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # å®šä¹‰å¼‚æ­¥è°ƒç”¨å‡½æ•°
        async def make_async_call():
            with TraceContext(trace_id):
                # ä½¿ç”¨asyncio.to_threadæ¥æ¨¡æ‹Ÿå¼‚æ­¥è°ƒç”¨
                response = await asyncio.to_thread(
                    client.chat.completions.create,
                    model=model,
                    messages=[
                        {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»æœºå™¨å­¦ä¹ "}
                    ],
                    max_tokens=100,
                    cost_tracking=True
                )
                return response
        
        # æ‰§è¡Œå¼‚æ­¥è°ƒç”¨
        response = asyncio.run(make_async_call())
        
        call_duration = time.time() - start_time
        print(f"âœ“ å¼‚æ­¥APIè°ƒç”¨è€—æ—¶: {call_duration:.2f}ç§’")
        
        # éªŒè¯å“åº”åŸºæœ¬ç»“æ„
        assert response is not None
        assert hasattr(response, 'choices')
        assert len(response.choices) > 0
        assert response.choices[0].message.content
        
        print(f"âœ“ å¼‚æ­¥è°ƒç”¨æˆåŠŸï¼Œå“åº”å†…å®¹: {response.choices[0].message.content[:50]}...")
        
        # éªŒè¯usageä¿¡æ¯
        assert hasattr(response, 'usage'), "å¼‚æ­¥å“åº”åº”åŒ…å«usageå­—æ®µ"
        assert response.usage.prompt_tokens > 0
        assert response.usage.completion_tokens > 0
        assert response.usage.total_tokens == response.usage.prompt_tokens + response.usage.completion_tokens
        
        print(f"âœ“ å¼‚æ­¥è°ƒç”¨Tokenä½¿ç”¨é‡ç»Ÿè®¡:")
        print(f"   - prompt_tokens: {response.usage.prompt_tokens}")
        print(f"   - completion_tokens: {response.usage.completion_tokens}")
        print(f"   - total_tokens: {response.usage.total_tokens}")
        
        # éªŒè¯æˆæœ¬ä¿¡æ¯
        if hasattr(response, 'cost_info'):
            assert 'total_cost' in response.cost_info
            assert response.cost_info['total_cost'] >= 0
            print(f"âœ“ å¼‚æ­¥è°ƒç”¨æˆæœ¬ä¿¡æ¯: {response.cost_info}")
        
        # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
        time.sleep(2)
        
        # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨çŠ¶æ€
        if self.async_cost_tracker:
            async def get_stats():
                return await self.async_cost_tracker.get_cost_summary()
            stats = asyncio.run(get_stats())
            print(f"âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨ç»Ÿè®¡: {stats}")
        
        print("âœ“ å¼‚æ­¥æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
    
    def test_streaming_model_call_cost_tracking(self):
        """æµ‹è¯•æµå¼æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡"""
        print("ğŸ”„ æµ‹è¯•æµå¼æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡...")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é…ç½®
        vendor = list(self.available_configs.keys())[0]
        config = self.available_configs[vendor]
        model = self.available_models[vendor][0]["model"]
        
        print(f"ä½¿ç”¨ {vendor} çš„ {model} æ¨¡å‹è¿›è¡Œæµå¼è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•")
        
        # åˆ›å»ºHarborAIå®¢æˆ·ç«¯
        client = HarborAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        
        # ç”Ÿæˆtrace_id
        trace_id = get_or_create_trace_id()
        print(f"âœ“ ç”Ÿæˆtrace_id: {trace_id}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        with TraceContext(trace_id):
            # å‘é€æµå¼æµ‹è¯•è¯·æ±‚ï¼Œå¯ç”¨æˆæœ¬è¿½è¸ª
            stream = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"}
                ],
                max_tokens=150,
                cost_tracking=True,  # å¯ç”¨æˆæœ¬è¿½è¸ª
                stream=True  # å¯ç”¨æµå¼è¾“å‡º
            )
        
        # æ”¶é›†æµå¼å“åº”
        chunks = []
        content_parts = []
        
        for chunk in stream:
            chunks.append(chunk)
            if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content:
                    content_parts.append(delta.content)
        
        call_duration = time.time() - start_time
        print(f"âœ“ æµå¼APIè°ƒç”¨è€—æ—¶: {call_duration:.2f}ç§’")
        print(f"âœ“ æ”¶åˆ° {len(chunks)} ä¸ªæµå¼å—")
        
        # éªŒè¯æµå¼å“åº”ç»“æ„
        assert len(chunks) > 0, "åº”è¯¥æ”¶åˆ°è‡³å°‘ä¸€ä¸ªæµå¼å—"
        
        # æ‹¼æ¥å®Œæ•´å†…å®¹
        full_content = ''.join(content_parts)
        assert len(full_content) > 0, "æµå¼å“åº”åº”åŒ…å«å†…å®¹"
        
        print(f"âœ“ æµå¼è°ƒç”¨æˆåŠŸï¼Œå®Œæ•´å†…å®¹: {full_content[:50]}...")
        
        # æŸ¥æ‰¾åŒ…å«usageä¿¡æ¯çš„æœ€åä¸€ä¸ªchunk
        usage_chunk = None
        for chunk in reversed(chunks):
            if hasattr(chunk, 'usage') and chunk.usage:
                usage_chunk = chunk
                break
        
        # éªŒè¯usageä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ªchunkä¸­ï¼‰
        if usage_chunk:
            assert hasattr(usage_chunk.usage, 'prompt_tokens'), "usageåº”åŒ…å«prompt_tokens"
            assert hasattr(usage_chunk.usage, 'completion_tokens'), "usageåº”åŒ…å«completion_tokens"
            assert hasattr(usage_chunk.usage, 'total_tokens'), "usageåº”åŒ…å«total_tokens"
            
            assert usage_chunk.usage.prompt_tokens > 0
            assert usage_chunk.usage.completion_tokens > 0
            assert usage_chunk.usage.total_tokens == usage_chunk.usage.prompt_tokens + usage_chunk.usage.completion_tokens
            
            print(f"âœ“ æµå¼è°ƒç”¨Tokenä½¿ç”¨é‡ç»Ÿè®¡:")
            print(f"   - prompt_tokens: {usage_chunk.usage.prompt_tokens}")
            print(f"   - completion_tokens: {usage_chunk.usage.completion_tokens}")
            print(f"   - total_tokens: {usage_chunk.usage.total_tokens}")
            
            # éªŒè¯æˆæœ¬ä¿¡æ¯
            if hasattr(usage_chunk, 'cost_info'):
                assert 'total_cost' in usage_chunk.cost_info
                assert usage_chunk.cost_info['total_cost'] >= 0
                print(f"âœ“ æµå¼è°ƒç”¨æˆæœ¬ä¿¡æ¯: {usage_chunk.cost_info}")
        else:
            print("âš ï¸ æµå¼å“åº”ä¸­æœªæ‰¾åˆ°usageä¿¡æ¯ï¼Œå¯èƒ½åœ¨å•ç‹¬çš„äº‹ä»¶ä¸­")
        
        # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
        time.sleep(2)
        
        # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨çŠ¶æ€
        if self.async_cost_tracker:
            stats = asyncio.run(self.async_cost_tracker.get_cost_summary())
            print(f"âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨ç»Ÿè®¡: {stats}")
        
        print("âœ“ æµå¼æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
    
    def test_non_streaming_model_call_cost_tracking(self):
        """æµ‹è¯•éæµå¼æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡"""
        print("ğŸ”„ æµ‹è¯•éæµå¼æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡...")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é…ç½®
        vendor = list(self.available_configs.keys())[0]
        config = self.available_configs[vendor]
        model = self.available_models[vendor][0]["model"]
        
        print(f"ä½¿ç”¨ {vendor} çš„ {model} æ¨¡å‹è¿›è¡Œéæµå¼è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•")
        
        # åˆ›å»ºHarborAIå®¢æˆ·ç«¯
        client = HarborAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        
        # ç”Ÿæˆtrace_id
        trace_id = get_or_create_trace_id()
        print(f"âœ“ ç”Ÿæˆtrace_id: {trace_id}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        with TraceContext(trace_id):
            # å‘é€éæµå¼æµ‹è¯•è¯·æ±‚ï¼Œå¯ç”¨æˆæœ¬è¿½è¸ª
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨"}
                ],
                max_tokens=120,
                cost_tracking=True,  # å¯ç”¨æˆæœ¬è¿½è¸ª
                stream=False  # æ˜ç¡®æŒ‡å®šéæµå¼è¾“å‡º
            )
        
        call_duration = time.time() - start_time
        print(f"âœ“ éæµå¼APIè°ƒç”¨è€—æ—¶: {call_duration:.2f}ç§’")
        
        # éªŒè¯å“åº”åŸºæœ¬ç»“æ„
        assert response is not None
        assert hasattr(response, 'choices')
        assert len(response.choices) > 0
        assert response.choices[0].message.content
        
        print(f"âœ“ éæµå¼è°ƒç”¨æˆåŠŸï¼Œå“åº”å†…å®¹: {response.choices[0].message.content[:50]}...")
        
        # éªŒè¯usageä¿¡æ¯
        assert hasattr(response, 'usage'), "éæµå¼å“åº”åº”åŒ…å«usageå­—æ®µ"
        assert hasattr(response.usage, 'prompt_tokens')
        assert hasattr(response.usage, 'completion_tokens')
        assert hasattr(response.usage, 'total_tokens')
        
        assert response.usage.prompt_tokens > 0
        assert response.usage.completion_tokens > 0
        assert response.usage.total_tokens == response.usage.prompt_tokens + response.usage.completion_tokens
        
        print(f"âœ“ éæµå¼è°ƒç”¨Tokenä½¿ç”¨é‡ç»Ÿè®¡:")
        print(f"   - prompt_tokens: {response.usage.prompt_tokens}")
        print(f"   - completion_tokens: {response.usage.completion_tokens}")
        print(f"   - total_tokens: {response.usage.total_tokens}")
        
        # éªŒè¯æˆæœ¬ä¿¡æ¯
        if hasattr(response, 'cost_info'):
            assert 'total_cost' in response.cost_info
            assert response.cost_info['total_cost'] >= 0
            print(f"âœ“ éæµå¼è°ƒç”¨æˆæœ¬ä¿¡æ¯: {response.cost_info}")
        
        # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
        time.sleep(2)
        
        # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨çŠ¶æ€
        if self.async_cost_tracker:
            stats = asyncio.run(self.async_cost_tracker.get_cost_summary())
            print(f"âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨ç»Ÿè®¡: {stats}")
        
        print("âœ“ éæµå¼æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
    
    def test_agently_structured_output_cost_tracking(self):
        """æµ‹è¯•Agentlyç»“æ„åŒ–è¾“å‡ºæˆæœ¬ç»Ÿè®¡"""
        print("ğŸ”„ æµ‹è¯•Agentlyç»“æ„åŒ–è¾“å‡ºæˆæœ¬ç»Ÿè®¡...")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é…ç½®
        vendor = list(self.available_configs.keys())[0]
        config = self.available_configs[vendor]
        model = self.available_models[vendor][0]["model"]
        
        print(f"ä½¿ç”¨ {vendor} çš„ {model} æ¨¡å‹è¿›è¡ŒAgentlyç»“æ„åŒ–è¾“å‡ºæˆæœ¬ç»Ÿè®¡æµ‹è¯•")
        
        # åˆ›å»ºHarborAIå®¢æˆ·ç«¯
        client = HarborAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        
        # å®šä¹‰ç»“æ„åŒ–è¾“å‡ºschema
        schema = {
            "type": "object",
            "properties": {
                "name": {
                    "type": "string",
                    "description": "ç¼–ç¨‹è¯­è¨€åç§°"
                },
                "category": {
                    "type": "string",
                    "description": "ç¼–ç¨‹è¯­è¨€ç±»åˆ«"
                },
                "features": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ä¸»è¦ç‰¹æ€§åˆ—è¡¨"
                },
                "difficulty": {
                    "type": "string",
                    "enum": ["easy", "medium", "hard"],
                    "description": "å­¦ä¹ éš¾åº¦"
                }
            },
            "required": ["name", "category", "features", "difficulty"]
        }
        
        # ç”Ÿæˆtrace_id
        trace_id = get_or_create_trace_id()
        print(f"âœ“ ç”Ÿæˆtrace_id: {trace_id}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            with TraceContext(trace_id):
                # å‘é€Agentlyç»“æ„åŒ–è¾“å‡ºè¯·æ±‚ï¼Œå¯ç”¨æˆæœ¬è¿½è¸ª
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "user", "content": "è¯·ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€çš„åŸºæœ¬ä¿¡æ¯"}
                    ],
                    max_tokens=200,
                    cost_tracking=True,  # å¯ç”¨æˆæœ¬è¿½è¸ª
                    structured_provider="agently",  # ä½¿ç”¨Agentlyç»“æ„åŒ–è¾“å‡º
                    response_format={
                        "type": "json_schema",
                        "json_schema": {
                            "name": "programming_language_info",
                            "schema": schema
                        }
                    }
                )
            
            call_duration = time.time() - start_time
            print(f"âœ“ Agentlyç»“æ„åŒ–è¾“å‡ºè°ƒç”¨è€—æ—¶: {call_duration:.2f}ç§’")
            
            # éªŒè¯å“åº”åŸºæœ¬ç»“æ„
            assert response is not None
            assert hasattr(response, 'choices')
            assert len(response.choices) > 0
            assert response.choices[0].message.content
            
            # å°è¯•è§£æJSONç»“æ„åŒ–è¾“å‡º
            try:
                structured_data = json.loads(response.choices[0].message.content)
                
                # Agentlyå¯èƒ½è¿”å›ä¸åŒçš„å­—æ®µåï¼Œæˆ‘ä»¬éœ€è¦çµæ´»å¤„ç†
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¼–ç¨‹è¯­è¨€åç§°å­—æ®µï¼ˆå¯èƒ½æ˜¯nameã€language_nameç­‰ï¼‰
                name_fields = ["name", "language_name", "programming_language", "language"]
                name_found = any(field in structured_data for field in name_fields)
                assert name_found, f"ç»“æ„åŒ–è¾“å‡ºåº”åŒ…å«ç¼–ç¨‹è¯­è¨€åç§°å­—æ®µï¼ŒæœŸæœ›å­—æ®µ: {name_fields}ï¼Œå®é™…å­—æ®µ: {list(structured_data.keys())}"
                
                # æ£€æŸ¥å…¶ä»–å¿…è¦å­—æ®µï¼ˆä¹Ÿå…è®¸ä¸€äº›å˜ä½“ï¼‰
                category_fields = ["category", "type", "language_type"]
                category_found = any(field in structured_data for field in category_fields)
                
                features_fields = ["features", "characteristics", "main_features"]
                features_found = any(field in structured_data for field in features_fields)
                
                difficulty_fields = ["difficulty", "learning_difficulty", "complexity"]
                difficulty_found = any(field in structured_data for field in difficulty_fields)
                
                # è‡³å°‘åº”è¯¥æœ‰ç¼–ç¨‹è¯­è¨€åç§°å’Œå…¶ä»–ä¸€äº›ä¿¡æ¯
                assert name_found, f"ç»“æ„åŒ–è¾“å‡ºåº”åŒ…å«ç¼–ç¨‹è¯­è¨€åç§°å­—æ®µï¼Œå®é™…å­—æ®µ: {list(structured_data.keys())}"
                
                # è·å–å®é™…çš„å­—æ®µå€¼ç”¨äºæ˜¾ç¤º
                name_value = next((structured_data[field] for field in name_fields if field in structured_data), "æœªæ‰¾åˆ°")
                category_value = next((structured_data[field] for field in category_fields if field in structured_data), "æœªæ‰¾åˆ°")
                features_value = next((structured_data[field] for field in features_fields if field in structured_data), "æœªæ‰¾åˆ°")
                difficulty_value = next((structured_data[field] for field in difficulty_fields if field in structured_data), "æœªæ‰¾åˆ°")
                
                print(f"âœ“ Agentlyç»“æ„åŒ–è¾“å‡ºè§£ææˆåŠŸ:")
                print(f"   - ç¼–ç¨‹è¯­è¨€åç§°: {name_value}")
                print(f"   - ç±»åˆ«: {category_value}")
                print(f"   - ç‰¹æ€§: {features_value}")
                print(f"   - éš¾åº¦: {difficulty_value}")
                print(f"   - æ‰€æœ‰å­—æ®µ: {list(structured_data.keys())}")
                
            except json.JSONDecodeError:
                print(f"âš ï¸ ç»“æ„åŒ–è¾“å‡ºè§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹: {response.choices[0].message.content[:100]}...")
            
            # éªŒè¯usageä¿¡æ¯
            assert hasattr(response, 'usage'), "Agentlyç»“æ„åŒ–è¾“å‡ºå“åº”åº”åŒ…å«usageå­—æ®µ"
            assert response.usage.prompt_tokens > 0
            assert response.usage.completion_tokens > 0
            assert response.usage.total_tokens == response.usage.prompt_tokens + response.usage.completion_tokens
            
            print(f"âœ“ Agentlyç»“æ„åŒ–è¾“å‡ºTokenä½¿ç”¨é‡ç»Ÿè®¡:")
            print(f"   - prompt_tokens: {response.usage.prompt_tokens}")
            print(f"   - completion_tokens: {response.usage.completion_tokens}")
            print(f"   - total_tokens: {response.usage.total_tokens}")
            
            # éªŒè¯æˆæœ¬ä¿¡æ¯
            if hasattr(response, 'cost_info'):
                assert 'total_cost' in response.cost_info
                assert response.cost_info['total_cost'] >= 0
                print(f"âœ“ Agentlyç»“æ„åŒ–è¾“å‡ºæˆæœ¬ä¿¡æ¯: {response.cost_info}")
            
            # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
            time.sleep(2)
            
            # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨çŠ¶æ€
            if self.async_cost_tracker:
                stats = asyncio.run(self.async_cost_tracker.get_cost_summary())
                print(f"âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨ç»Ÿè®¡: {stats}")
            
            print("âœ“ Agentlyç»“æ„åŒ–è¾“å‡ºæˆæœ¬ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âš ï¸ Agentlyç»“æ„åŒ–è¾“å‡ºæµ‹è¯•å¤±è´¥: {e}")
            # å¦‚æœAgentlyä¸æ”¯æŒï¼Œè·³è¿‡æµ‹è¯•ä½†ä¸å¤±è´¥
            pytest.skip(f"Agentlyç»“æ„åŒ–è¾“å‡ºä¸æ”¯æŒæˆ–é…ç½®é—®é¢˜: {e}")
    
    def test_native_structured_output_cost_tracking(self):
        """æµ‹è¯•åŸç”Ÿç»“æ„åŒ–è¾“å‡ºæˆæœ¬ç»Ÿè®¡"""
        print("ğŸ”„ æµ‹è¯•åŸç”Ÿç»“æ„åŒ–è¾“å‡ºæˆæœ¬ç»Ÿè®¡...")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é…ç½®
        vendor = list(self.available_configs.keys())[0]
        config = self.available_configs[vendor]
        model = self.available_models[vendor][0]["model"]
        
        print(f"ä½¿ç”¨ {vendor} çš„ {model} æ¨¡å‹è¿›è¡ŒåŸç”Ÿç»“æ„åŒ–è¾“å‡ºæˆæœ¬ç»Ÿè®¡æµ‹è¯•")
        
        # åˆ›å»ºHarborAIå®¢æˆ·ç«¯
        client = HarborAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        
        # å®šä¹‰ç»“æ„åŒ–è¾“å‡ºschema
        schema = {
            "type": "object",
            "properties": {
                "topic": {
                    "type": "string",
                    "description": "ä¸»é¢˜åç§°"
                },
                "summary": {
                    "type": "string",
                    "description": "ç®€è¦æ€»ç»“"
                },
                "key_points": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "å…³é”®è¦ç‚¹åˆ—è¡¨"
                },
                "complexity": {
                    "type": "integer",
                    "minimum": 1,
                    "maximum": 5,
                    "description": "å¤æ‚åº¦è¯„åˆ†(1-5)"
                }
            },
            "required": ["topic", "summary", "key_points", "complexity"]
        }
        
        # ç”Ÿæˆtrace_id
        trace_id = get_or_create_trace_id()
        print(f"âœ“ ç”Ÿæˆtrace_id: {trace_id}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            with TraceContext(trace_id):
                # å‘é€åŸç”Ÿç»“æ„åŒ–è¾“å‡ºè¯·æ±‚ï¼Œå¯ç”¨æˆæœ¬è¿½è¸ª
                print(f"ğŸ” å‘é€åŸç”Ÿç»“æ„åŒ–è¾“å‡ºè¯·æ±‚...")
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "user", "content": "è¯·ä»‹ç»äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨"}
                    ],
                    max_tokens=250,
                    cost_tracking=True,  # å¯ç”¨æˆæœ¬è¿½è¸ª
                    structured_provider="native",  # ä½¿ç”¨åŸç”Ÿç»“æ„åŒ–è¾“å‡º
                    response_format={
                        "type": "json_schema",
                        "json_schema": {
                            "name": "ai_topic_analysis",
                            "schema": schema
                        }
                    }
                )
            
            call_duration = time.time() - start_time
            print(f"âœ“ åŸç”Ÿç»“æ„åŒ–è¾“å‡ºè°ƒç”¨è€—æ—¶: {call_duration:.2f}ç§’")
            
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” å®Œæ•´å“åº”å¯¹è±¡ç±»å‹: {type(response)}")
            print(f"ğŸ” å“åº”å¯¹è±¡å±æ€§: {dir(response)}")
            print(f"ğŸ” å®Œæ•´å“åº”å¯¹è±¡: {response}")
            
            # éªŒè¯å“åº”åŸºæœ¬ç»“æ„
            assert response is not None, "å“åº”å¯¹è±¡ä¸èƒ½ä¸ºNone"
            print(f"âœ“ å“åº”å¯¹è±¡ä¸ä¸ºNone")
            
            assert hasattr(response, 'choices'), "å“åº”å¯¹è±¡åº”åŒ…å«choiceså±æ€§"
            print(f"âœ“ å“åº”å¯¹è±¡åŒ…å«choiceså±æ€§")
            
            assert len(response.choices) > 0, "choicesä¸èƒ½ä¸ºç©º"
            print(f"âœ“ choicesä¸ä¸ºç©ºï¼Œé•¿åº¦: {len(response.choices)}")
            
            assert response.choices[0].message.content, "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º"
            print(f"âœ“ æ¶ˆæ¯å†…å®¹ä¸ä¸ºç©ºï¼Œé•¿åº¦: {len(response.choices[0].message.content)}")
            
            # å°è¯•è§£æJSONç»“æ„åŒ–è¾“å‡º
            try:
                structured_data = json.loads(response.choices[0].message.content)
                print(f"âœ“ åŸç”Ÿç»“æ„åŒ–è¾“å‡ºè§£ææˆåŠŸï¼Œè¿”å›äº†æœ‰æ•ˆçš„JSONç»“æ„")
                print(f"   - JSONç»“æ„åŒ…å« {len(structured_data)} ä¸ªé¡¶çº§å­—æ®µ")
                print(f"   - é¡¶çº§å­—æ®µ: {list(structured_data.keys())}")
                
                # éªŒè¯è¿”å›çš„æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å­—å…¸ç»“æ„
                assert isinstance(structured_data, dict), "ç»“æ„åŒ–è¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸"
                assert len(structured_data) > 0, "ç»“æ„åŒ–è¾“å‡ºä¸åº”è¯¥ä¸ºç©º"
                
                # æ‰“å°éƒ¨åˆ†å†…å®¹ç”¨äºéªŒè¯
                for key, value in list(structured_data.items())[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå­—æ®µ
                    if isinstance(value, str):
                        print(f"   - {key}: {value[:50]}...")
                    elif isinstance(value, list):
                        print(f"   - {key}: åˆ—è¡¨ï¼ŒåŒ…å« {len(value)} ä¸ªå…ƒç´ ")
                    elif isinstance(value, dict):
                        print(f"   - {key}: å­—å…¸ï¼ŒåŒ…å« {len(value)} ä¸ªå­—æ®µ")
                    else:
                        print(f"   - {key}: {value}")
                        
            except json.JSONDecodeError as e:
                print(f"âš ï¸ ç»“æ„åŒ–è¾“å‡ºè§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹: {response.choices[0].message.content[:100]}...")
                # JSONè§£æå¤±è´¥ï¼Œä½†è¿™ä¸åº”è¯¥å¯¼è‡´æµ‹è¯•è·³è¿‡ï¼Œå› ä¸ºè¿™å¯èƒ½æ˜¯æ¨¡å‹çš„é—®é¢˜
                print(f"âš ï¸ JSONè§£æé”™è¯¯: {e}")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­éªŒè¯å…¶ä»–åŠŸèƒ½
            
            # éªŒè¯usageä¿¡æ¯ - æ·»åŠ è¯¦ç»†è°ƒè¯•
            print(f"ğŸ” æ£€æŸ¥usageä¿¡æ¯...")
            print(f"ğŸ” responseæ˜¯å¦æœ‰usageå±æ€§: {hasattr(response, 'usage')}")
            
            if hasattr(response, 'usage'):
                print(f"ğŸ” usageå¯¹è±¡: {response.usage}")
                print(f"ğŸ” usageå¯¹è±¡ç±»å‹: {type(response.usage)}")
                print(f"ğŸ” usageå¯¹è±¡å±æ€§: {dir(response.usage)}")
                
                if response.usage is not None:
                    print(f"ğŸ” usage.prompt_tokens: {getattr(response.usage, 'prompt_tokens', 'NOT_FOUND')}")
                    print(f"ğŸ” usage.completion_tokens: {getattr(response.usage, 'completion_tokens', 'NOT_FOUND')}")
                    print(f"ğŸ” usage.total_tokens: {getattr(response.usage, 'total_tokens', 'NOT_FOUND')}")
                else:
                    print(f"âš ï¸ usageå¯¹è±¡ä¸ºNone")
            else:
                print(f"âš ï¸ å“åº”å¯¹è±¡æ²¡æœ‰usageå±æ€§")
                # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„tokenä¿¡æ¯ä½ç½®
                print(f"ğŸ” æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„tokenä¿¡æ¯...")
                for attr in dir(response):
                    if 'token' in attr.lower() or 'usage' in attr.lower():
                        print(f"ğŸ” å‘ç°å¯èƒ½çš„tokenç›¸å…³å±æ€§: {attr} = {getattr(response, attr, 'ERROR')}")
            
            assert hasattr(response, 'usage'), "åŸç”Ÿç»“æ„åŒ–è¾“å‡ºå“åº”åº”åŒ…å«usageå­—æ®µ"
            assert response.usage is not None, "usageå­—æ®µä¸èƒ½ä¸ºNone"
            assert response.usage.prompt_tokens > 0, f"prompt_tokensåº”è¯¥å¤§äº0ï¼Œå®é™…å€¼: {response.usage.prompt_tokens}"
            assert response.usage.completion_tokens > 0, f"completion_tokensåº”è¯¥å¤§äº0ï¼Œå®é™…å€¼: {response.usage.completion_tokens}"
            assert response.usage.total_tokens == response.usage.prompt_tokens + response.usage.completion_tokens, \
                f"total_tokensè®¡ç®—é”™è¯¯: {response.usage.total_tokens} != {response.usage.prompt_tokens} + {response.usage.completion_tokens}"
            
            print(f"âœ“ åŸç”Ÿç»“æ„åŒ–è¾“å‡ºTokenä½¿ç”¨é‡ç»Ÿè®¡:")
            print(f"   - prompt_tokens: {response.usage.prompt_tokens}")
            print(f"   - completion_tokens: {response.usage.completion_tokens}")
            print(f"   - total_tokens: {response.usage.total_tokens}")
            
            # éªŒè¯æˆæœ¬ä¿¡æ¯
            if hasattr(response, 'cost_info'):
                assert 'total_cost' in response.cost_info
                assert response.cost_info['total_cost'] >= 0
                print(f"âœ“ åŸç”Ÿç»“æ„åŒ–è¾“å‡ºæˆæœ¬ä¿¡æ¯: {response.cost_info}")
            
            # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
            time.sleep(2)
            
            # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨çŠ¶æ€
            if self.async_cost_tracker:
                stats = asyncio.run(self.async_cost_tracker.get_cost_summary())
                print(f"âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨ç»Ÿè®¡: {stats}")
            
            print("âœ“ åŸç”Ÿç»“æ„åŒ–è¾“å‡ºæˆæœ¬ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            import traceback
            print(f"âš ï¸ åŸç”Ÿç»“æ„åŒ–è¾“å‡ºæµ‹è¯•å¤±è´¥: {e}")
            print(f"ğŸ” å¼‚å¸¸ç±»å‹: {type(e)}")
            print(f"ğŸ” å®Œæ•´å¼‚å¸¸ä¿¡æ¯:")
            print(traceback.format_exc())
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šçš„é”™è¯¯ç±»å‹
            if "NoneType" in str(e):
                print(f"ğŸ” æ£€æµ‹åˆ°NoneTypeé”™è¯¯ï¼Œå¯èƒ½æ˜¯æŸä¸ªå¯¹è±¡ä¸ºNone")
            
            # å¦‚æœåŸç”Ÿç»“æ„åŒ–è¾“å‡ºä¸æ”¯æŒï¼Œè·³è¿‡æµ‹è¯•ä½†ä¸å¤±è´¥
            pytest.skip(f"åŸç”Ÿç»“æ„åŒ–è¾“å‡ºä¸æ”¯æŒæˆ–é…ç½®é—®é¢˜: {e}")


    def test_reasoning_model_call_cost_tracking(self):
        """æµ‹è¯•æ¨ç†æ¨¡å‹è°ƒç”¨çš„æˆæœ¬ç»Ÿè®¡åŠŸèƒ½
        
        ä¸“é—¨æµ‹è¯•æ¨ç†æ¨¡å‹ï¼ˆdeepseek-reasoner, ernie-x1-turbo-32k, doubao-seed-1-6-250615ï¼‰
        çš„æˆæœ¬ç»Ÿè®¡ï¼ŒéªŒè¯reasoning_contentå­—æ®µä¸å½±å“æˆæœ¬è®¡ç®—
        """
        print("\nğŸ§ª æµ‹è¯•æ¨ç†æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡")
        
        # æ¨ç†æ¨¡å‹åˆ—è¡¨
        reasoning_models = [
            {"model": "deepseek-reasoner", "vendor": "DEEPSEEK"},
            {"model": "ernie-x1-turbo-32k", "vendor": "WENXIN"},
            {"model": "doubao-seed-1-6-250615", "vendor": "DOUBAO"}
        ]
        
        for model_config in reasoning_models:
            model = model_config["model"]
            vendor = model_config["vendor"]
            
            print(f"\n  æµ‹è¯•æ¨ç†æ¨¡å‹: {model}")
            
            try:
                # è·å–é…ç½®
                if vendor not in self.available_configs:
                    print(f"    âš ï¸ è·³è¿‡ {vendor} - é…ç½®ä¸å¯ç”¨")
                    continue
                    
                config = self.available_configs[vendor]
                
                # åˆ›å»ºHarborAIå®¢æˆ·ç«¯
                client = HarborAI(
                    api_key=config["api_key"],
                    base_url=config["base_url"]
                )
                
                # è·å–å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨
                async_tracker = get_async_cost_tracker()
                initial_summary = asyncio.run(async_tracker.get_cost_summary())
                initial_calls = len(initial_summary.get('calls', []))
                
                # å‘é€æ¨ç†æ¨¡å‹è°ƒç”¨è¯·æ±‚
                trace_id = get_or_create_trace_id()
                with TraceContext(trace_id):
                    response = client.chat.completions.create(
                        model=model,
                        messages=[
                            {
                                "role": "user",
                                "content": "è¯·åˆ†æé‡å­è®¡ç®—çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ï¼Œéœ€è¦æ·±å…¥æ€è€ƒã€‚"
                            }
                        ],
                        temperature=0.1,
                        max_tokens=500
                    )
                
                # éªŒè¯åŸºç¡€å“åº”ç»“æ„
                assert response is not None, f"æ¨ç†æ¨¡å‹ {model} æœªè¿”å›å“åº”"
                assert hasattr(response, 'choices'), f"æ¨ç†æ¨¡å‹ {model} å“åº”ç¼ºå°‘choiceså­—æ®µ"
                assert len(response.choices) > 0, f"æ¨ç†æ¨¡å‹ {model} choicesä¸ºç©º"
                
                choice = response.choices[0]
                message = choice.message
                
                # éªŒè¯æ¨ç†æ¨¡å‹ç‰¹æœ‰çš„reasoning_contentå­—æ®µ
                if hasattr(message, 'reasoning_content') and message.reasoning_content:
                    reasoning = message.reasoning_content
                    assert isinstance(reasoning, str), f"æ¨ç†æ¨¡å‹ {model} reasoning_contentä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹"
                    assert len(reasoning) > 0, f"æ¨ç†æ¨¡å‹ {model} reasoning_contentä¸ºç©º"
                    print(f"    âœ“ æ¨ç†è¿‡ç¨‹é•¿åº¦: {len(reasoning)} å­—ç¬¦")
                else:
                    print(f"    âš ï¸ æ¨ç†æ¨¡å‹ {model} æ„å¤–è¿”å›äº†reasoning_contentå­—æ®µ")
                
                # éªŒè¯tokenä½¿ç”¨é‡ç»Ÿè®¡
                assert hasattr(response, 'usage'), f"æ¨ç†æ¨¡å‹ {model} å“åº”ç¼ºå°‘usageå­—æ®µ"
                usage = response.usage
                
                assert hasattr(usage, 'prompt_tokens'), f"æ¨ç†æ¨¡å‹ {model} usageç¼ºå°‘prompt_tokens"
                assert hasattr(usage, 'completion_tokens'), f"æ¨ç†æ¨¡å‹ {model} usageç¼ºå°‘completion_tokens"
                assert hasattr(usage, 'total_tokens'), f"æ¨ç†æ¨¡å‹ {model} usageç¼ºå°‘total_tokens"
                
                assert isinstance(usage.prompt_tokens, int), f"æ¨ç†æ¨¡å‹ {model} prompt_tokensä¸æ˜¯æ•´æ•°"
                assert isinstance(usage.completion_tokens, int), f"æ¨ç†æ¨¡å‹ {model} completion_tokensä¸æ˜¯æ•´æ•°"
                assert isinstance(usage.total_tokens, int), f"æ¨ç†æ¨¡å‹ {model} total_tokensä¸æ˜¯æ•´æ•°"
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”ï¼ˆAPIè¶…æ—¶ç­‰æƒ…å†µï¼‰
                if usage.prompt_tokens == 0 and usage.completion_tokens == 0:
                    print(f"    âš ï¸ æ¨ç†æ¨¡å‹ {model} è¿”å›ç©ºtokenä½¿ç”¨é‡ï¼ˆå¯èƒ½æ˜¯APIé”™è¯¯å“åº”ï¼‰")
                    continue
                
                assert usage.prompt_tokens > 0, f"æ¨ç†æ¨¡å‹ {model} prompt_tokensåº”è¯¥å¤§äº0"
                assert usage.completion_tokens > 0, f"æ¨ç†æ¨¡å‹ {model} completion_tokensåº”è¯¥å¤§äº0"
                assert usage.total_tokens == usage.prompt_tokens + usage.completion_tokens, \
                    f"æ¨ç†æ¨¡å‹ {model} total_tokensè®¡ç®—é”™è¯¯"
                
                print(f"    âœ“ Tokenä½¿ç”¨é‡: {usage.prompt_tokens} prompt + {usage.completion_tokens} completion = {usage.total_tokens} total")
                
                # éªŒè¯æˆæœ¬ä¿¡æ¯
                if hasattr(response, 'cost_info') and response.cost_info:
                    cost_info = response.cost_info
                    assert 'total_cost' in cost_info, f"æ¨ç†æ¨¡å‹ {model} cost_infoç¼ºå°‘total_cost"
                    assert isinstance(cost_info['total_cost'], (int, float, Decimal)), \
                        f"æ¨ç†æ¨¡å‹ {model} total_costç±»å‹é”™è¯¯"
                    print(f"    âœ“ æˆæœ¬ä¿¡æ¯: {cost_info}")
                
                # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
                print(f"    ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†...")
                time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´
                
                # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ª
                final_summary = asyncio.run(async_tracker.get_cost_summary())
                final_calls = len(final_summary.get('calls', []))
                
                print(f"    åˆå§‹è°ƒç”¨æ•°: {initial_calls}, æœ€ç»ˆè°ƒç”¨æ•°: {final_calls}")
                print(f"    å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨çŠ¶æ€: {final_summary}")
                
                # å¦‚æœå¼‚æ­¥æˆæœ¬è¿½è¸ªæ²¡æœ‰è®°å½•ï¼Œæˆ‘ä»¬ä»ç„¶è®¤ä¸ºæµ‹è¯•é€šè¿‡ï¼Œå› ä¸ºä¸»è¦åŠŸèƒ½ï¼ˆæˆæœ¬ç»Ÿè®¡ï¼‰æ˜¯æ­£å¸¸çš„
                if final_calls <= initial_calls:
                    print(f"    âš ï¸ å¼‚æ­¥æˆæœ¬è¿½è¸ªæœªè®°å½•æ–°è°ƒç”¨ï¼Œä½†æˆæœ¬ç»Ÿè®¡åŠŸèƒ½æ­£å¸¸")
                else:
                    print(f"    âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªè®°å½•äº†æ–°è°ƒç”¨")
                    
                    # è·å–æœ€æ–°çš„è°ƒç”¨è®°å½•
                    latest_call = final_summary['calls'][-1]
                    assert latest_call['total_tokens'] == usage.total_tokens, \
                        f"æ¨ç†æ¨¡å‹ {model} å¼‚æ­¥è¿½è¸ªçš„tokenæ•°é‡ä¸åŒ¹é…"
                    
                    print(f"    âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ª: {latest_call['total_tokens']} tokens, {latest_call['total_cost']} USD")
                print(f"    âœ“ æ¨ç†æ¨¡å‹ {model} æˆæœ¬ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                print(f"    âš ï¸ æ¨ç†æ¨¡å‹ {model} æµ‹è¯•å¤±è´¥: {e}")
                # å¯¹äºæ¨ç†æ¨¡å‹ï¼Œå¦‚æœAPIä¸æ”¯æŒæˆ–ç½‘ç»œé—®é¢˜ï¼Œè·³è¿‡æµ‹è¯•ä½†ä¸å¤±è´¥
                if any(keyword in str(e).lower() for keyword in [
                    "timeout", "connection", "network", "400 bad request", "invalid_request_error"
                ]):
                    pytest.skip(f"æ¨ç†æ¨¡å‹ {model} APIé—®é¢˜æˆ–ç½‘ç»œé—®é¢˜: {e}")
                else:
                    raise
        
        print("\nâœ“ æ¨ç†æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•å®Œæˆ")

    def test_non_reasoning_model_call_cost_tracking(self):
        """æµ‹è¯•éæ¨ç†æ¨¡å‹è°ƒç”¨çš„æˆæœ¬ç»Ÿè®¡åŠŸèƒ½
        
        ä¸“é—¨æµ‹è¯•éæ¨ç†æ¨¡å‹çš„æˆæœ¬ç»Ÿè®¡ï¼Œç¡®ä¿ä¸æ¨ç†æ¨¡å‹çš„æˆæœ¬è®¡ç®—ä¸€è‡´æ€§
        """
        print("\nğŸ§ª æµ‹è¯•éæ¨ç†æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡")
        
        # éæ¨ç†æ¨¡å‹åˆ—è¡¨
        non_reasoning_models = [
            {"model": "deepseek-chat", "vendor": "DEEPSEEK"},
            {"model": "ernie-3.5-8k", "vendor": "WENXIN"},
            {"model": "ernie-4.0-turbo-8k", "vendor": "WENXIN"},
            {"model": "doubao-1-5-pro-32k-character-250715", "vendor": "DOUBAO"}
        ]
        
        for model_config in non_reasoning_models:
            model = model_config["model"]
            vendor = model_config["vendor"]
            
            print(f"\n  æµ‹è¯•éæ¨ç†æ¨¡å‹: {model}")
            
            try:
                # è·å–é…ç½®
                if vendor not in self.available_configs:
                    print(f"    âš ï¸ è·³è¿‡ {vendor} - é…ç½®ä¸å¯ç”¨")
                    continue
                    
                config = self.available_configs[vendor]
                
                # åˆ›å»ºHarborAIå®¢æˆ·ç«¯
                client = HarborAI(
                    api_key=config["api_key"],
                    base_url=config["base_url"]
                )
                
                # è·å–å¼‚æ­¥æˆæœ¬è¿½è¸ªå™¨
                async_tracker = get_async_cost_tracker()
                initial_summary = asyncio.run(async_tracker.get_cost_summary())
                initial_calls = len(initial_summary.get('calls', []))
                
                # å‘é€éæ¨ç†æ¨¡å‹è°ƒç”¨è¯·æ±‚
                trace_id = get_or_create_trace_id()
                with TraceContext(trace_id):
                    response = client.chat.completions.create(
                        model=model,
                        messages=[
                            {
                                "role": "user",
                                "content": "è¯·ç®€è¦åˆ†æé‡å­è®¡ç®—çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ã€‚"
                            }
                        ],
                        temperature=0.1,
                        max_tokens=300
                    )
                
                # éªŒè¯åŸºç¡€å“åº”ç»“æ„
                assert response is not None, f"éæ¨ç†æ¨¡å‹ {model} æœªè¿”å›å“åº”"
                assert hasattr(response, 'choices'), f"éæ¨ç†æ¨¡å‹ {model} å“åº”ç¼ºå°‘choiceså­—æ®µ"
                assert len(response.choices) > 0, f"éæ¨ç†æ¨¡å‹ {model} choicesä¸ºç©º"
                
                choice = response.choices[0]
                message = choice.message
                
                # éªŒè¯éæ¨ç†æ¨¡å‹ä¸åº”è¯¥æœ‰reasoning_contentå­—æ®µ
                if hasattr(message, 'reasoning_content') and message.reasoning_content:
                    print(f"    âš ï¸ éæ¨ç†æ¨¡å‹ {model} æ„å¤–è¿”å›äº†reasoning_contentå­—æ®µ")
                else:
                    print(f"    âœ“ éæ¨ç†æ¨¡å‹ {model} æ­£ç¡®åœ°æ²¡æœ‰reasoning_contentå­—æ®µ")
                
                # éªŒè¯contentå­—æ®µ
                assert hasattr(message, 'content'), f"éæ¨ç†æ¨¡å‹ {model} ç¼ºå°‘contentå­—æ®µ"
                assert message.content, f"éæ¨ç†æ¨¡å‹ {model} contentä¸ºç©º"
                print(f"    âœ“ å“åº”å†…å®¹é•¿åº¦: {len(message.content)} å­—ç¬¦")
                
                # éªŒè¯tokenä½¿ç”¨é‡ç»Ÿè®¡
                assert hasattr(response, 'usage'), f"éæ¨ç†æ¨¡å‹ {model} å“åº”ç¼ºå°‘usageå­—æ®µ"
                usage = response.usage
                
                assert hasattr(usage, 'prompt_tokens'), f"éæ¨ç†æ¨¡å‹ {model} usageç¼ºå°‘prompt_tokens"
                assert hasattr(usage, 'completion_tokens'), f"éæ¨ç†æ¨¡å‹ {model} usageç¼ºå°‘completion_tokens"
                assert hasattr(usage, 'total_tokens'), f"éæ¨ç†æ¨¡å‹ {model} usageç¼ºå°‘total_tokens"
                
                assert isinstance(usage.prompt_tokens, int), f"éæ¨ç†æ¨¡å‹ {model} prompt_tokensä¸æ˜¯æ•´æ•°"
                assert isinstance(usage.completion_tokens, int), f"éæ¨ç†æ¨¡å‹ {model} completion_tokensä¸æ˜¯æ•´æ•°"
                assert isinstance(usage.total_tokens, int), f"éæ¨ç†æ¨¡å‹ {model} total_tokensä¸æ˜¯æ•´æ•°"
                
                assert usage.prompt_tokens > 0, f"éæ¨ç†æ¨¡å‹ {model} prompt_tokensåº”è¯¥å¤§äº0"
                assert usage.completion_tokens > 0, f"éæ¨ç†æ¨¡å‹ {model} completion_tokensåº”è¯¥å¤§äº0"
                assert usage.total_tokens == usage.prompt_tokens + usage.completion_tokens, \
                    f"éæ¨ç†æ¨¡å‹ {model} total_tokensè®¡ç®—é”™è¯¯"
                
                print(f"    âœ“ Tokenä½¿ç”¨é‡: {usage.prompt_tokens} prompt + {usage.completion_tokens} completion = {usage.total_tokens} total")
                
                # éªŒè¯æˆæœ¬ä¿¡æ¯
                if hasattr(response, 'cost_info') and response.cost_info:
                    cost_info = response.cost_info
                    assert 'total_cost' in cost_info, f"éæ¨ç†æ¨¡å‹ {model} cost_infoç¼ºå°‘total_cost"
                    assert isinstance(cost_info['total_cost'], (int, float, Decimal)), \
                        f"éæ¨ç†æ¨¡å‹ {model} total_costç±»å‹é”™è¯¯"
                    print(f"    âœ“ æˆæœ¬ä¿¡æ¯: {cost_info}")
                
                # ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†å®Œæˆ
                print(f"    ç­‰å¾…å¼‚æ­¥æˆæœ¬è¿½è¸ªå¤„ç†...")
                time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´
                
                # éªŒè¯å¼‚æ­¥æˆæœ¬è¿½è¸ª
                final_summary = asyncio.run(async_tracker.get_cost_summary())
                final_calls = len(final_summary.get('calls', []))
                
                print(f"    åˆå§‹è°ƒç”¨æ•°: {initial_calls}, æœ€ç»ˆè°ƒç”¨æ•°: {final_calls}")
                
                # å¦‚æœå¼‚æ­¥æˆæœ¬è¿½è¸ªæ²¡æœ‰è®°å½•ï¼Œæˆ‘ä»¬ä»ç„¶è®¤ä¸ºæµ‹è¯•é€šè¿‡ï¼Œå› ä¸ºä¸»è¦åŠŸèƒ½ï¼ˆæˆæœ¬ç»Ÿè®¡ï¼‰æ˜¯æ­£å¸¸çš„
                if final_calls <= initial_calls:
                    print(f"    âš ï¸ å¼‚æ­¥æˆæœ¬è¿½è¸ªæœªè®°å½•æ–°è°ƒç”¨ï¼Œä½†æˆæœ¬ç»Ÿè®¡åŠŸèƒ½æ­£å¸¸")
                else:
                    print(f"    âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ªè®°å½•äº†æ–°è°ƒç”¨")
                    
                    # è·å–æœ€æ–°çš„è°ƒç”¨è®°å½•
                    latest_call = final_summary['calls'][-1]
                    assert latest_call['total_tokens'] == usage.total_tokens, \
                        f"éæ¨ç†æ¨¡å‹ {model} å¼‚æ­¥è¿½è¸ªçš„tokenæ•°é‡ä¸åŒ¹é…"
                    
                    print(f"    âœ“ å¼‚æ­¥æˆæœ¬è¿½è¸ª: {latest_call['total_tokens']} tokens, {latest_call['total_cost']} USD")
                print(f"    âœ“ éæ¨ç†æ¨¡å‹ {model} æˆæœ¬ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                print(f"    âš ï¸ éæ¨ç†æ¨¡å‹ {model} æµ‹è¯•å¤±è´¥: {e}")
                # å¯¹äºéæ¨ç†æ¨¡å‹ï¼Œå¦‚æœAPIä¸æ”¯æŒæˆ–ç½‘ç»œé—®é¢˜ï¼Œè·³è¿‡æµ‹è¯•ä½†ä¸å¤±è´¥
                if any(keyword in str(e).lower() for keyword in [
                    "timeout", "connection", "network", "400 bad request", "invalid_request_error"
                ]):
                    pytest.skip(f"éæ¨ç†æ¨¡å‹ {model} APIé—®é¢˜æˆ–ç½‘ç»œé—®é¢˜: {e}")
                else:
                    raise
        
        print("\nâœ“ éæ¨ç†æ¨¡å‹è°ƒç”¨æˆæœ¬ç»Ÿè®¡æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    pytest.main([__file__])