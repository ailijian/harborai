# HarborAI æ—¥å¿—ç³»ç»Ÿé‡æ„è®¾è®¡æ–¹æ¡ˆ

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 é‡æ„ç›®æ ‡

åŸºäºç°æœ‰HarborAIæŠ€æœ¯æ¶æ„ï¼Œä¼˜åŒ–æ—¥å¿—ç³»ç»Ÿçš„tokenè§£ææœºåˆ¶ã€æˆæœ¬ç®¡ç†å’Œåˆ†å¸ƒå¼è¿½è¸ªèƒ½åŠ›ï¼Œæå‡æ•°æ®å‡†ç¡®æ€§å’Œç³»ç»Ÿå¯è§‚æµ‹æ€§ã€‚

**æ ¸å¿ƒæ”¹è¿›ç‚¹ï¼š**
- **Tokenå­—æ®µå¯¹é½**ï¼šä¿æŒ `prompt_tokens` å’Œ `completion_tokens` å­—æ®µåä¸å‚å•†å“åº”ä¸€è‡´
- **æˆæœ¬ç®¡ç†ä¼˜åŒ–**ï¼šæ”¯æŒç¯å¢ƒå˜é‡å’ŒåŠ¨æ€é…ç½®æ¨¡å‹ä»·æ ¼ï¼Œç»†åŒ–è¾“å…¥è¾“å‡ºæˆæœ¬
- **åˆ†å¸ƒå¼è¿½è¸ªé›†æˆ**ï¼šé›†æˆOpenTelemetryå®ç°å…¨é“¾è·¯è¿½è¸ªå’Œæ€§èƒ½åˆ†æ
- **å­˜å‚¨æ¶æ„ç®€åŒ–**ï¼šé‡‡ç”¨ PostgreSQL + æ–‡ä»¶æ—¥å¿—çš„åŒå­˜å‚¨æ¶æ„ï¼Œç§»é™¤Redisä¾èµ–
- **è‡ªåŠ¨é™çº§æœºåˆ¶**ï¼šPostgreSQLä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°æ–‡ä»¶æ—¥å¿—
- **åŸºäºç°æœ‰æ¶æ„**ï¼šå……åˆ†åˆ©ç”¨å·²æœ‰çš„ `observability`ã€`FallbackLogger`ã€`PostgreSQLLogger`ã€`FileSystemLogger` ç­‰ç»„ä»¶

### 1.2 æŠ€æœ¯èƒŒæ™¯

**ç°æœ‰æ¶æ„ä¼˜åŠ¿ï¼š**
- âœ… å·²æœ‰å®Œå–„çš„ `observability` æ¨¡å—æä¾›ç»Ÿä¸€å¯è§‚æµ‹æ€§æ¥å£
- âœ… å·²æœ‰ `FallbackLogger` å®ç°è‡ªåŠ¨é™çº§æœºåˆ¶
- âœ… å·²æœ‰ `PostgreSQLLogger` æ”¯æŒå¼‚æ­¥æ‰¹é‡å†™å…¥
- âœ… å·²æœ‰ `FileSystemLogger` æ”¯æŒæ—¥å¿—è½®è½¬å’Œå‹ç¼©
- âœ… å·²æœ‰å®Œæ•´çš„æ•æ„Ÿä¿¡æ¯æ£€æµ‹å’Œè„±æ•æœºåˆ¶
- âœ… å·²æœ‰ `PrometheusMetrics` æ”¯æŒæŒ‡æ ‡æ”¶é›†
- âœ… å·²æœ‰ `structlog` ç»“æ„åŒ–æ—¥å¿—è®°å½•
- âœ… å·²æœ‰ `PricingCalculator` æ”¯æŒæˆæœ¬è®¡ç®—å’Œæ¨¡å‹ä»·æ ¼ç®¡ç†
- âœ… å·²æœ‰ OpenTelemetry åˆ†å¸ƒå¼è¿½è¸ªé›†æˆ

**éœ€è¦ä¼˜åŒ–çš„é—®é¢˜ï¼š**
- ğŸ”„ Tokenå­—æ®µåéœ€è¦ä¸å‚å•†å“åº”ä¿æŒä¸€è‡´
- ğŸ”„ æˆæœ¬å­—æ®µéœ€è¦ç»†åŒ–ä¸ºè¾“å…¥æˆæœ¬å’Œè¾“å‡ºæˆæœ¬
- ğŸ”„ å¢å¼ºåˆ†å¸ƒå¼è¿½è¸ªä¿¡æ¯çš„è®°å½•å’ŒæŸ¥è¯¢
- ğŸ”„ ç®€åŒ–å­˜å‚¨æ¶æ„ï¼Œä¸“æ³¨äºPostgreSQL+æ–‡ä»¶æ—¥å¿—
- ğŸ”„ ä¼˜åŒ–tokenè§£æé€»è¾‘ï¼Œç›´æ¥ä»å‚å•†å“åº”ä¸­æå–

## 2. æ ¸å¿ƒåŠŸèƒ½

### 2.1 ç”¨æˆ·è§’è‰²

æœ¬é¡¹ç›®ä¸ºPython SDKï¼Œä¸»è¦æœåŠ¡äºå¼€å‘è€…ç”¨æˆ·ï¼š

| è§’è‰² | ä½¿ç”¨æ–¹å¼ | æ ¸å¿ƒæƒé™ |
|------|----------|----------|
| SDKå¼€å‘è€… | Pythonä»£ç é›†æˆ | å¯è®°å½•å’ŒæŸ¥è¯¢APIè°ƒç”¨æ—¥å¿—ï¼ŒæŸ¥çœ‹æˆæœ¬ç»Ÿè®¡ |
| è¿ç»´äººå‘˜ | å‘½ä»¤è¡Œå·¥å…· | å¯æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€ï¼Œå¯¼å‡ºæ—¥å¿—æ•°æ®ï¼Œç›‘æ§ç³»ç»Ÿå¥åº· |

### 2.2 åŠŸèƒ½æ¨¡å—

æˆ‘ä»¬çš„æ—¥å¿—ç³»ç»Ÿé‡æ„åŒ…å«ä»¥ä¸‹æ ¸å¿ƒæ¨¡å—ï¼š

1. **Tokenè§£æä¼˜åŒ–æ¨¡å—**ï¼šç›´æ¥ä»å‚å•†å“åº”è§£ætokenæ•°æ®ï¼Œä¿æŒå­—æ®µåä¸€è‡´æ€§
2. **æˆæœ¬ç®¡ç†ä¼˜åŒ–æ¨¡å—**ï¼šæ”¯æŒç¯å¢ƒå˜é‡é…ç½®å’ŒåŠ¨æ€ä»·æ ¼è®¾ç½®ï¼Œç»†åŒ–è¾“å…¥è¾“å‡ºæˆæœ¬
3. **åˆ†å¸ƒå¼è¿½è¸ªå¢å¼ºæ¨¡å—**ï¼šé›†æˆOpenTelemetryå®ç°å…¨é“¾è·¯è¿½è¸ªå’Œæ€§èƒ½åˆ†æ
4. **å­˜å‚¨æ¶æ„ä¼˜åŒ–æ¨¡å—**ï¼šç®€åŒ–ä¸ºPostgreSQLä¸»å­˜å‚¨+æ–‡ä»¶æ—¥å¿—å¤‡ä»½çš„åŒå­˜å‚¨æ¶æ„
5. **è‡ªåŠ¨é™çº§å¢å¼ºæ¨¡å—**ï¼šä¼˜åŒ–ç°æœ‰FallbackLoggerçš„é™çº§ç­–ç•¥å’Œæ¢å¤æœºåˆ¶
6. **æ•°æ®ä¸€è‡´æ€§ä¿éšœæ¨¡å—**ï¼šç¡®ä¿tokenæ•°æ®çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
7. **ç›‘æ§å’Œå‘Šè­¦æ¨¡å—**ï¼šåŸºäºç°æœ‰PrometheusMetricså¢å¼ºç³»ç»Ÿç›‘æ§

### 2.3 é¡µé¢è¯¦æƒ…

| æ¨¡å—åç§° | ç»„ä»¶åç§° | åŠŸèƒ½æè¿° |
|----------|----------|----------|
| Tokenè§£æä¼˜åŒ–æ¨¡å— | TokenParsingService | ä»å‚å•†å“åº”ä¸­ç›´æ¥è§£æprompt_tokenså’Œcompletion_tokensï¼Œä¿æŒå­—æ®µåä¸å˜ |
| Tokenè§£æä¼˜åŒ–æ¨¡å— | ProviderTokenParser | é’ˆå¯¹ä¸åŒå‚å•†å®ç°ä¸“ç”¨çš„tokenè§£æå™¨ï¼ˆDeepSeekã€OpenAIã€Doubaoç­‰ï¼‰ |
| Tokenè§£æä¼˜åŒ–æ¨¡å— | TokenValidationService | éªŒè¯tokenæ•°æ®ä¸€è‡´æ€§ï¼Œç¡®ä¿total_tokens = prompt_tokens + completion_tokens |
| æˆæœ¬ç®¡ç†ä¼˜åŒ–æ¨¡å— | EnhancedPricingCalculator | åŸºäºç°æœ‰PricingCalculatorå¢å¼ºï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®å’ŒåŠ¨æ€ä»·æ ¼è®¾ç½® |
| æˆæœ¬ç®¡ç†ä¼˜åŒ–æ¨¡å— | CostBreakdownService | è®¡ç®—å’Œè®°å½•è¾“å…¥æˆæœ¬ã€è¾“å‡ºæˆæœ¬çš„ç»†åˆ†ä¿¡æ¯ |
| æˆæœ¬ç®¡ç†ä¼˜åŒ–æ¨¡å— | EnvironmentPricingLoader | ä»ç¯å¢ƒå˜é‡åŠ è½½æ¨¡å‹ä»·æ ¼é…ç½®ï¼ˆå¦‚OPENAI_GPT4_INPUT_PRICEï¼‰ |
| åˆ†å¸ƒå¼è¿½è¸ªå¢å¼ºæ¨¡å— | OpenTelemetryTracer | é›†æˆOpenTelemetryå®ç°åˆ†å¸ƒå¼è¿½è¸ªï¼Œè®°å½•trace_idå’Œspan_id |
| åˆ†å¸ƒå¼è¿½è¸ªå¢å¼ºæ¨¡å— | TracingDataCollector | æ”¶é›†å’Œè®°å½•è¿½è¸ªç›¸å…³çš„æ€§èƒ½æŒ‡æ ‡å’Œæ ‡ç­¾ä¿¡æ¯ |
| åˆ†å¸ƒå¼è¿½è¸ªå¢å¼ºæ¨¡å— | TraceContextManager | ç®¡ç†è¿½è¸ªä¸Šä¸‹æ–‡çš„ä¼ æ’­å’Œç»§æ‰¿ |
| å­˜å‚¨æ¶æ„ä¼˜åŒ–æ¨¡å— | OptimizedPostgreSQLLogger | åŸºäºç°æœ‰PostgreSQLLoggerä¼˜åŒ–ï¼Œä¸“æ³¨äºtokenå­—æ®µå¯¹é½å’Œæˆæœ¬ç»†åˆ† |
| å­˜å‚¨æ¶æ„ä¼˜åŒ–æ¨¡å— | EnhancedFileSystemLogger | åŸºäºç°æœ‰FileSystemLoggerå¢å¼ºï¼Œæ”¯æŒæ›´å¥½çš„å¤‡ä»½æ¢å¤ |
| å­˜å‚¨æ¶æ„ä¼˜åŒ–æ¨¡å— | ImprovedFallbackLogger | ä¼˜åŒ–ç°æœ‰FallbackLoggerçš„é™çº§ç­–ç•¥å’Œå¥åº·æ£€æŸ¥ |
| æ•°æ®ä¸€è‡´æ€§ä¿éšœæ¨¡å— | DataConsistencyChecker | å®æ—¶æ£€æŸ¥å’Œä¿®æ­£tokenæ•°æ®ä¸ä¸€è‡´é—®é¢˜ |
| æ•°æ®ä¸€è‡´æ€§ä¿éšœæ¨¡å— | DatabaseConstraintManager | ç®¡ç†æ•°æ®åº“çº¦æŸå’Œè§¦å‘å™¨ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§ |
| ç›‘æ§å’Œå‘Šè­¦æ¨¡å— | EnhancedPrometheusMetrics | åŸºäºç°æœ‰PrometheusMetricså¢åŠ é™çº§çŠ¶æ€å’Œæ•°æ®è´¨é‡æŒ‡æ ‡ |
| ç›‘æ§å’Œå‘Šè­¦æ¨¡å— | HealthCheckService | ç›‘æ§PostgreSQLå¥åº·çŠ¶æ€ï¼Œè§¦å‘è‡ªåŠ¨é™çº§ |

## 3. æ ¸å¿ƒæµç¨‹

### 3.1 Tokenè§£æä¼˜åŒ–æµç¨‹

1. **å‚å•†å“åº”æ¥æ”¶** â†’ æ¥æ”¶APIå“åº”æ•°æ®
2. **å‚å•†è¯†åˆ«** â†’ æ ¹æ®providerå­—æ®µé€‰æ‹©å¯¹åº”çš„tokenè§£æå™¨
3. **ç›´æ¥å­—æ®µæå–** â†’ ä»å“åº”çš„usageå­—æ®µä¸­ç›´æ¥æå–prompt_tokenså’Œcompletion_tokens
4. **æ•°æ®ä¸€è‡´æ€§éªŒè¯** â†’ éªŒè¯total_tokens = prompt_tokens + completion_tokens
5. **æ•°æ®å­˜å‚¨** â†’ ä¿æŒåŸå§‹å­—æ®µåå­˜å‚¨åˆ°æ•°æ®åº“

### 3.2 æˆæœ¬ç®¡ç†ä¼˜åŒ–æµç¨‹

1. **ä»·æ ¼é…ç½®åŠ è½½** â†’ ä»ç¯å¢ƒå˜é‡æˆ–åŠ¨æ€é…ç½®åŠ è½½æ¨¡å‹ä»·æ ¼
2. **Tokenä½¿ç”¨é‡è·å–** â†’ è·å–prompt_tokenså’Œcompletion_tokens
3. **æˆæœ¬ç»†åˆ†è®¡ç®—** â†’ åˆ†åˆ«è®¡ç®—input_costå’Œoutput_cost
4. **æ€»æˆæœ¬æ±‡æ€»** â†’ è®¡ç®—total_cost = input_cost + output_cost
5. **æˆæœ¬ä¿¡æ¯è®°å½•** â†’ è®°å½•è¯¦ç»†çš„æˆæœ¬åˆ†è§£ä¿¡æ¯

### 3.3 åˆ†å¸ƒå¼è¿½è¸ªæµç¨‹

1. **è¿½è¸ªä¸Šä¸‹æ–‡åˆ›å»º** â†’ ä¸ºæ¯ä¸ªAPIè°ƒç”¨åˆ›å»ºtrace_idå’Œspan_id
2. **æ“ä½œæ ‡è®°** â†’ æ ‡è®°æ“ä½œç±»å‹ï¼ˆai.chat.completionï¼‰
3. **æ€§èƒ½æ•°æ®æ”¶é›†** â†’ è®°å½•å¼€å§‹æ—¶é—´ã€æŒç»­æ—¶é—´ã€çŠ¶æ€
4. **æ ‡ç­¾ä¿¡æ¯æ·»åŠ ** â†’ æ·»åŠ æ¨¡å‹ã€æä¾›å•†ã€æˆæœ¬ç­‰æ ‡ç­¾
5. **è¿½è¸ªæ•°æ®å­˜å‚¨** â†’ å°†è¿½è¸ªä¿¡æ¯å­˜å‚¨åˆ°æ•°æ®åº“

### 3.4 è‡ªåŠ¨é™çº§æµç¨‹

1. **å¥åº·æ£€æŸ¥** â†’ å®šæœŸæ£€æŸ¥PostgreSQLè¿æ¥çŠ¶æ€
2. **æ•…éšœæ£€æµ‹** â†’ æ£€æµ‹åˆ°è¿ç»­å¤±è´¥æ—¶è§¦å‘é™çº§
3. **è‡ªåŠ¨åˆ‡æ¢** â†’ åˆ‡æ¢åˆ°æ–‡ä»¶æ—¥å¿—æ¨¡å¼
4. **æ¢å¤æ£€æµ‹** â†’ å®šæœŸå°è¯•æ¢å¤PostgreSQLè¿æ¥
5. **æ•°æ®åŒæ­¥** â†’ PostgreSQLæ¢å¤ååŒæ­¥æ–‡ä»¶æ—¥å¿—æ•°æ®

### 3.5 é¡µé¢å¯¼èˆªæµç¨‹å›¾

```mermaid
graph TD
    A[APIè°ƒç”¨] --> B[observabilityæ¨¡å—]
    B --> C[OpenTelemetryè¿½è¸ª]
    C --> D[Tokenè§£ææœåŠ¡]
    D --> E[æˆæœ¬è®¡ç®—æœåŠ¡]
    E --> F[æ•°æ®ä¸€è‡´æ€§éªŒè¯]
    F --> G[FallbackLogger]
    G --> H{PostgreSQLå¯ç”¨?}
    H -->|æ˜¯| I[PostgreSQLLogger]
    H -->|å¦| J[FileSystemLogger]
    I --> K[æ•°æ®å­˜å‚¨å®Œæˆ]
    J --> K
    K --> L[PrometheusMetricsè®°å½•]
    L --> M[ç›‘æ§å‘Šè­¦]
    
    N[å¥åº·æ£€æŸ¥æœåŠ¡] --> O{æ£€æµ‹åˆ°PostgreSQLæ¢å¤?}
    O -->|æ˜¯| P[åˆ‡æ¢å›PostgreSQL]
    O -->|å¦| Q[ç»§ç»­æ–‡ä»¶æ—¥å¿—æ¨¡å¼]
    P --> R[åŒæ­¥æ–‡ä»¶æ—¥å¿—æ•°æ®]
    
    S[ç¯å¢ƒå˜é‡é…ç½®] --> T[ä»·æ ¼é…ç½®åŠ è½½]
    T --> E
    
    U[åŠ¨æ€ä»·æ ¼è®¾ç½®] --> V[PricingCalculator]
    V --> E
```

## 4. ç”¨æˆ·ç•Œé¢è®¾è®¡

### 4.1 è®¾è®¡é£æ ¼

**å‘½ä»¤è¡Œç•Œé¢è®¾è®¡ï¼š**
- **ä¸»è‰²è°ƒ**ï¼šè“è‰²ç³» (#2563EB) å’Œç»¿è‰²ç³» (#059669) 
- **è¾…åŠ©è‰²**ï¼šç°è‰²ç³» (#6B7280) å’Œçº¢è‰²ç³» (#DC2626)
- **è¾“å‡ºé£æ ¼**ï¼šåŸºäºRichåº“çš„è¡¨æ ¼å’Œè¿›åº¦æ¡æ˜¾ç¤º
- **å­—ä½“**ï¼šç­‰å®½å­—ä½“ï¼Œæ”¯æŒä¸­æ–‡æ˜¾ç¤º
- **å¸ƒå±€é£æ ¼**ï¼šç®€æ´çš„è¡¨æ ¼å¸ƒå±€ï¼Œæ¸…æ™°çš„å±‚çº§ç»“æ„
- **å›¾æ ‡é£æ ¼**ï¼šä½¿ç”¨Unicodeç¬¦å·å’Œemojiå¢å¼ºå¯è¯»æ€§

### 4.2 ç•Œé¢è®¾è®¡æ¦‚è§ˆ

| ç•Œé¢åç§° | ç»„ä»¶åç§° | UIå…ƒç´  |
|----------|----------|--------|
| æ—¥å¿—åˆ—è¡¨ç•Œé¢ | LogListDisplay | Richè¡¨æ ¼æ˜¾ç¤ºï¼ŒåŒ…å«trace_idã€æ—¶é—´æˆ³ã€providerã€modelã€tokenä½¿ç”¨é‡ã€æˆæœ¬ä¿¡æ¯ï¼Œæ”¯æŒé¢œè‰²ç¼–ç çŠ¶æ€ |
| æ—¥å¿—è¯¦æƒ…ç•Œé¢ | LogDetailDisplay | åˆ†å±‚å±•ç¤ºè¯·æ±‚/å“åº”æ•°æ®ï¼Œé«˜äº®æ˜¾ç¤ºtokenä¿¡æ¯ï¼Œä½¿ç”¨æ ‘å½¢ç»“æ„å±•ç¤ºJSONæ•°æ® |
| ç»Ÿè®¡ä¿¡æ¯ç•Œé¢ | StatsDisplay | ä½¿ç”¨Richçš„Panelå’ŒProgressç»„ä»¶å±•ç¤ºç»Ÿè®¡æ•°æ®ï¼ŒåŒ…å«å›¾è¡¨å¼çš„æˆæœ¬åˆ†å¸ƒ |
| ç›‘æ§ç•Œé¢ | MonitorDisplay | å®æ—¶æ›´æ–°çš„çŠ¶æ€é¢æ¿ï¼Œæ˜¾ç¤ºPostgreSQLå¥åº·çŠ¶æ€ã€é™çº§çŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡ |
| å¯¼å‡ºç•Œé¢ | ExportDisplay | è¿›åº¦æ¡æ˜¾ç¤ºå¯¼å‡ºè¿›åº¦ï¼Œæ”¯æŒå¤šç§æ ¼å¼é€‰æ‹©ï¼ˆJSONã€CSVã€Excelï¼‰ |

### 4.3 å“åº”å¼è®¾è®¡

**ç»ˆç«¯é€‚é…ï¼š**
- æ”¯æŒä¸åŒç»ˆç«¯å®½åº¦çš„è‡ªé€‚åº”å¸ƒå±€
- çª„å±å¹•æ—¶è‡ªåŠ¨æŠ˜å éƒ¨åˆ†åˆ—
- æ”¯æŒé¼ æ ‡å’Œé”®ç›˜äº¤äº’
- ä¼˜åŒ–ä¸­æ–‡å­—ç¬¦æ˜¾ç¤º

## 5. æŠ€æœ¯å®ç°æ–¹æ¡ˆ

### 5.1 Tokenè§£æä¼˜åŒ–å®ç°

#### 5.1.1 å‚å•†Tokenè§£æå™¨

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Dict, Any, Optional

@dataclass
class TokenUsage:
    """Tokenä½¿ç”¨é‡æ•°æ®æ¨¡å‹ - ä¿æŒå‚å•†åŸå§‹å­—æ®µå"""
    prompt_tokens: int      # ä¸å‚å•†å“åº”å­—æ®µåä¿æŒä¸€è‡´
    completion_tokens: int  # ä¸å‚å•†å“åº”å­—æ®µåä¿æŒä¸€è‡´
    total_tokens: int
    parsing_method: str = "direct_extraction"
    confidence: float = 1.0
    raw_data: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """è‡ªåŠ¨ä¿®æ­£ç­–ç•¥ï¼šä¼˜å…ˆå‚å•†æä¾›çš„total_tokensï¼Œå¦åˆ™è‡ªåŠ¨è®¡ç®—"""
        calculated_total = self.prompt_tokens + self.completion_tokens
        
        if self.total_tokens > 0 and self.total_tokens != calculated_total:
            # å‚å•†æä¾›äº†total_tokensä½†ä¸è®¡ç®—å€¼ä¸ä¸€è‡´ï¼Œä¼˜å…ˆä½¿ç”¨å‚å•†å€¼
            self.confidence = 0.9  # é™ä½ç½®ä¿¡åº¦ä½†ä¿æŒå‚å•†æ•°æ®
        elif self.total_tokens <= 0:
            # å‚å•†æœªæä¾›total_tokensï¼Œä½¿ç”¨è®¡ç®—å€¼
            self.total_tokens = calculated_total
            self.confidence = 0.8  # è®¡ç®—å€¼ç½®ä¿¡åº¦ç¨ä½

class BaseTokenParser(ABC):
    """Tokenè§£æå™¨åŸºç±»"""
    
    @abstractmethod
    async def parse_tokens(self, response_data: Dict[str, Any], model: str) -> TokenUsage:
        """ä»å‚å•†å“åº”ä¸­è§£ætokenä½¿ç”¨é‡"""
        pass

class DeepSeekTokenParser(BaseTokenParser):
    """DeepSeek Tokenè§£æå™¨"""
    
    async def parse_tokens(self, response_data: Dict[str, Any], model: str) -> TokenUsage:
        usage = response_data.get("usage", {})
        
        return TokenUsage(
            prompt_tokens=usage.get("prompt_tokens", 0),
            completion_tokens=usage.get("completion_tokens", 0),
            total_tokens=usage.get("total_tokens", 0),
            parsing_method="deepseek_direct",
            confidence=1.0,
            raw_data=usage
        )

class OpenAITokenParser(BaseTokenParser):
    """OpenAI Tokenè§£æå™¨"""
    
    async def parse_tokens(self, response_data: Dict[str, Any], model: str) -> TokenUsage:
        usage = response_data.get("usage", {})
        
        return TokenUsage(
            prompt_tokens=usage.get("prompt_tokens", 0),
            completion_tokens=usage.get("completion_tokens", 0),
            total_tokens=usage.get("total_tokens", 0),
            parsing_method="openai_direct",
            confidence=1.0,
            raw_data=usage
        )
```

#### 5.1.2 æˆæœ¬ç®¡ç†ä¼˜åŒ–å®ç°

```python
class EnhancedPricingCalculator(PricingCalculator):
    """åŸºäºç°æœ‰PricingCalculatorçš„å¢å¼ºç‰ˆæœ¬"""
    
    def __init__(self):
        super().__init__()
        self.env_pricing_loader = EnvironmentPricingLoader()
        self.logger = structlog.get_logger(__name__)
        
    async def calculate_detailed_cost(
        self,
        provider: str,
        model: str,
        prompt_tokens: int,
        completion_tokens: int
    ) -> Dict[str, Any]:
        """è®¡ç®—è¯¦ç»†çš„æˆæœ¬åˆ†è§£"""
        
        # è·å–æ¨¡å‹ä»·æ ¼ï¼ˆä¼˜å…ˆç¯å¢ƒå˜é‡ï¼‰
        pricing = await self._get_model_pricing_with_env(provider, model)
        
        # è®¡ç®—è¾“å…¥å’Œè¾“å‡ºæˆæœ¬
        input_cost = (prompt_tokens / 1000) * pricing.input_price_per_1k
        output_cost = (completion_tokens / 1000) * pricing.output_price_per_1k
        total_cost = input_cost + output_cost
        
        return {
            "input_cost": round(input_cost, 6),
            "output_cost": round(output_cost, 6),
            "total_cost": round(total_cost, 6),
            "currency": pricing.currency,
            "pricing_source": pricing.source,
            "pricing_timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    async def _get_model_pricing_with_env(
        self, 
        provider: str, 
        model: str
    ) -> ModelPricing:
        """è·å–æ¨¡å‹ä»·æ ¼ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®"""
        
        # å°è¯•ä»ç¯å¢ƒå˜é‡åŠ è½½
        env_pricing = self.env_pricing_loader.load_pricing(provider, model)
        if env_pricing:
            return env_pricing
            
        # å›é€€åˆ°é»˜è®¤é…ç½®
        return self.get_model_pricing(provider, model)

class EnvironmentPricingLoader:
    """ä»ç¯å¢ƒå˜é‡åŠ è½½æ¨¡å‹ä»·æ ¼é…ç½®"""
    
    def load_pricing(self, provider: str, model: str) -> Optional[ModelPricing]:
        """ä»ç¯å¢ƒå˜é‡åŠ è½½ä»·æ ¼é…ç½®"""
        
        # æ„å»ºç¯å¢ƒå˜é‡å
        provider_upper = provider.upper()
        model_key = model.replace("-", "_").replace(".", "_").upper()
        
        input_price_key = f"{provider_upper}_{model_key}_INPUT_PRICE"
        output_price_key = f"{provider_upper}_{model_key}_OUTPUT_PRICE"
        
        # ç‰¹æ®Šå¤„ç†å¸¸è§æ¨¡å‹
        if provider == "openai" and "gpt-4" in model:
            input_price_key = "OPENAI_GPT4_INPUT_PRICE"
            output_price_key = "OPENAI_GPT4_OUTPUT_PRICE"
        elif provider == "deepseek":
            input_price_key = "DEEPSEEK_INPUT_PRICE"
            output_price_key = "DEEPSEEK_OUTPUT_PRICE"
        
        input_price = os.getenv(input_price_key)
        output_price = os.getenv(output_price_key)
        
        if input_price and output_price:
            currency = os.getenv("COST_CURRENCY", "CNY")
            return ModelPricing(
                input_price_per_1k=float(input_price),
                output_price_per_1k=float(output_price),
                currency=currency,
                source="environment_variable"
            )
        
        return None
```

#### 5.1.3 åˆ†å¸ƒå¼è¿½è¸ªé›†æˆå®ç°

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

class OpenTelemetryTracer:
    """OpenTelemetryåˆ†å¸ƒå¼è¿½è¸ªé›†æˆ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.tracer = self._setup_tracer()
        self.logger = structlog.get_logger(__name__)
        
    def _setup_tracer(self):
        """è®¾ç½®OpenTelemetryè¿½è¸ªå™¨"""
        if not self.config.get("OTEL_ENABLED", False):
            return None
            
        # è®¾ç½®è¿½è¸ªæä¾›è€…
        trace.set_tracer_provider(TracerProvider())
        
        # é…ç½®OTLPå¯¼å‡ºå™¨
        otlp_exporter = OTLPSpanExporter(
            endpoint=self.config.get("OTEL_EXPORTER_OTLP_ENDPOINT"),
            headers=self.config.get("OTEL_EXPORTER_OTLP_HEADERS", {})
        )
        
        # æ·»åŠ æ‰¹é‡å¤„ç†å™¨
        span_processor = BatchSpanProcessor(otlp_exporter)
        trace.get_tracer_provider().add_span_processor(span_processor)
        
        # è·å–è¿½è¸ªå™¨
        service_name = self.config.get("OTEL_SERVICE_NAME", "harborai")
        return trace.get_tracer(service_name)
    
    async def create_ai_span(
        self,
        operation_name: str,
        provider: str,
        model: str,
        trace_id: Optional[str] = None
    ):
        """åˆ›å»ºAIæ“ä½œçš„è¿½è¸ªspan"""
        if not self.tracer:
            return None
            
        span = self.tracer.start_span(operation_name)
        
        # è®¾ç½®ç³»ç»Ÿçº§æ ‡ç­¾ï¼ˆé¿å…ä¸ä¸»å­—æ®µå†—ä½™ï¼‰
        span.set_attribute("ai.system", "harborai")
        span.set_attribute("ai.version", "2.0.0")
        span.set_attribute("ai.operation", operation_name)
        span.set_attribute("service.name", "harborai-logging")
        
        if trace_id:
            span.set_attribute("harborai.trace_id", trace_id)
            
        return span
    
    async def record_ai_metrics(
        self,
        span,
        token_usage: TokenUsage,
        cost_info: Dict[str, Any],
        performance_metrics: Dict[str, Any]
    ):
        """è®°å½•AIç›¸å…³çš„æŒ‡æ ‡åˆ°spanï¼ˆå†…éƒ¨æ˜ å°„ï¼Œä¸åœ¨APIå“åº”ä¸­æš´éœ²ï¼‰"""
        if not span:
            return
            
        # å†…éƒ¨è¿½è¸ªä¸“ç”¨æ ‡ç­¾ï¼ˆä¸ä¸APIå“åº”å­—æ®µé‡å¤ï¼‰
        span.set_attribute("request.id", f"req_{int(time.time() * 1000)}")
        span.set_attribute("environment", "production")
        span.set_attribute("user.session", "session_" + str(uuid.uuid4())[:8])
        
        # æ€§èƒ½æŒ‡æ ‡
        if "latency_ms" in performance_metrics:
            span.set_attribute("ai.latency_ms", performance_metrics["latency_ms"])
            
        # æ³¨æ„ï¼šTokenå’Œæˆæœ¬ä¿¡æ¯é€šè¿‡å†…éƒ¨æ˜ å°„æœºåˆ¶å¤„ç†ï¼Œä¸ç›´æ¥è®¾ç½®åˆ°span attributes
        # è¿™æ ·é¿å…äº†ä¸APIå“åº”ä¸­çš„ä¸»å­—æ®µäº§ç”Ÿå†—ä½™

class TracingDataCollector:
    """è¿½è¸ªæ•°æ®æ”¶é›†å™¨ - å®ç°åŒå±‚æ˜ å°„æœºåˆ¶"""
    
    def __init__(self, tracer: OpenTelemetryTracer):
        self.tracer = tracer
        self.logger = structlog.get_logger(__name__)
        
    async def prepare_api_response_tracing(
        self,
        trace_id: str,
        span_id: str,
        operation_name: str
    ) -> Dict[str, Any]:
        """å‡†å¤‡APIå“åº”çš„ç®€åŒ–è¿½è¸ªä¿¡æ¯ï¼ˆç¬¬ä¸€å±‚æ˜ å°„ï¼‰"""
        return {
            "trace_id": trace_id,
            "span_id": span_id,
            "operation_name": operation_name,
            "tags": {
                "ai.operation": operation_name,
                "service.name": "harborai-logging"
                # æ³¨æ„ï¼šä¸åŒ…å«ai.systemã€ai.versionç­‰å†—ä½™å­—æ®µ
            }
        }
        
    async def generate_internal_otel_span(
        self,
        span,
        trace_id: str,
        operation_name: str,
        token_usage: Dict[str, Any],
        cost_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå†…éƒ¨OpenTelemetry spançš„å®Œæ•´ä¿¡æ¯ï¼ˆç¬¬äºŒå±‚æ˜ å°„ï¼‰"""
        if not span:
            return {}
            
        span_context = span.get_span_context()
        
        # å®Œæ•´çš„å†…éƒ¨è¿½è¸ªæ ‡ç­¾ï¼ˆåŒ…å«æ‰€æœ‰AIç›¸å…³ä¿¡æ¯ï¼‰
        internal_tags = {
            "ai.system": "harborai",
            "ai.version": "2.0.0",
            "ai.operation": operation_name,
            "service.name": "harborai-logging",
            "ai.model.provider": token_usage.get("provider", ""),
            "ai.model.name": token_usage.get("model", ""),
            "ai.usage.prompt_tokens": str(token_usage.get("prompt_tokens", 0)),
            "ai.usage.completion_tokens": str(token_usage.get("completion_tokens", 0)),
            "ai.usage.total_tokens": str(token_usage.get("total_tokens", 0)),
            "ai.cost.amount": str(cost_info.get("total_cost", "0")),
            "ai.cost.currency": cost_info.get("currency", "CNY"),
            "request.id": f"req_{int(time.time() * 1000)}",
            "environment": "production"
        }
        
        return {
            "trace_id": trace_id,
            "span_id": format(span_context.span_id, "016x"),
            "operation_name": operation_name,
            "start_time": datetime.now(timezone.utc),
            "status": "ok",
            "internal_tags": internal_tags,
            "span_attributes": self._extract_span_attributes(span)
        }
    
    def _extract_span_attributes(self, span) -> Dict[str, Any]:
        """æå–spançš„æ‰€æœ‰å±æ€§"""
        try:
            # ä»OpenTelemetry spanä¸­æå–æ‰€æœ‰attributes
            if hasattr(span, 'attributes'):
                return dict(span.attributes)
            return {}
        except Exception as e:
            self.logger.warning("Failed to extract span attributes", error=str(e))
            return {}
```

#### 5.1.4 Tokenè§£ææœåŠ¡

```python
class TokenParsingService:
    """Tokenæ•°æ®è§£ææœåŠ¡ - åŸºäºç°æœ‰æ¶æ„ä¼˜åŒ–"""
    
    def __init__(self):
        self.provider_parsers = {
            "deepseek": DeepSeekTokenParser(),
            "openai": OpenAITokenParser(),
            "doubao": DoubaoTokenParser(),
            "wenxin": WenxinTokenParser(),
            "anthropic": AnthropicTokenParser(),
        }
        self.logger = structlog.get_logger(__name__)
    
    async def parse_token_usage(
        self, 
        provider: str,
        model: str,
        response_data: Dict[str, Any],
        trace_id: str
    ) -> TokenUsage:
        """ä»å‚å•†å“åº”ä¸­è§£ætokenä½¿ç”¨é‡"""
        try:
            parser = self.provider_parsers.get(provider)
            if not parser:
                raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")
            
            token_usage = await parser.parse_tokens(response_data, model)
            
            # è®°å½•è§£æç»“æœ
            self.logger.info(
                "Tokenè§£æå®Œæˆ",
                trace_id=trace_id,
                provider=provider,
                model=model,
                prompt_tokens=token_usage.prompt_tokens,
                completion_tokens=token_usage.completion_tokens,
                total_tokens=token_usage.total_tokens,
                parsing_method=token_usage.parsing_method
            )
            
            return token_usage
            
        except Exception as e:
            self.logger.error(
                "Tokenè§£æå¤±è´¥",
                trace_id=trace_id,
                provider=provider,
                model=model,
                error=str(e)
            )
            # è¿”å›é»˜è®¤å€¼ï¼Œé¿å…é˜»å¡æ—¥å¿—è®°å½•
            return TokenUsage(
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                parsing_method="fallback_zero",
                confidence=0.0,
                raw_data={"error": str(e)}
            )
```

#### 5.1.5 å¢å¼ºç‰ˆæ—¥å¿—è®°å½•å™¨

```python
class OptimizedPostgreSQLLogger(PostgreSQLLogger):
    """ä¼˜åŒ–çš„PostgreSQLæ—¥å¿—è®°å½•å™¨ï¼Œé›†æˆæˆæœ¬ç®¡ç†å’Œåˆ†å¸ƒå¼è¿½è¸ª"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.token_parser = TokenParsingService()
        self.pricing_calculator = EnhancedPricingCalculator()
        self.tracer = OpenTelemetryTracer(config)
        self.tracing_collector = TracingDataCollector(self.tracer)
        self.logger = structlog.get_logger(__name__)
        
    async def log_response_with_enhanced_features(
        self,
        request_data: Dict[str, Any],
        response_data: Dict[str, Any],
        provider: str,
        model: str,
        trace_id: str,
        performance_metrics: Dict[str, Any],
        custom_pricing: Optional[Dict[str, Any]] = None
    ) -> bool:
        """è®°å½•å“åº”æ—¥å¿—ï¼ŒåŒ…å«å¢å¼ºçš„æˆæœ¬ç®¡ç†å’Œåˆ†å¸ƒå¼è¿½è¸ª"""
        
        span = None
        try:
            # åˆ›å»ºè¿½è¸ªspan
            span = await self.tracer.create_ai_span(
                operation_name="ai_request_logging",
                provider=provider,
                model=model,
                trace_id=trace_id
            )
            
            # è§£ætokenä½¿ç”¨é‡
             token_usage = await self.token_parser.parse_tokens(
                 provider, response_data
             )
            
            # å¤„ç†è‡ªå®šä¹‰ä»·æ ¼è®¾ç½®
            if custom_pricing:
                await self._apply_custom_pricing(
                    provider, model, custom_pricing
                )
            
            # è®¡ç®—è¯¦ç»†æˆæœ¬åˆ†è§£
            cost_info = await self.pricing_calculator.calculate_detailed_cost(
                provider, model,
                token_usage.prompt_tokens,
                token_usage.completion_tokens
            )
            
            # è®°å½•AIæŒ‡æ ‡åˆ°span
            if span:
                await self.tracer.record_ai_metrics(
                    span, token_usage, cost_info, performance_metrics
                )
            
            # æ”¶é›†è¿½è¸ªä¿¡æ¯
            tracing_info = await self.tracing_collector.collect_tracing_info(
                span, trace_id, "ai_request_logging"
            )
            
            # æ„å»ºå®Œæ•´çš„æ—¥å¿—æ¡ç›®
            log_entry = {
                "trace_id": trace_id,
                "provider": provider,
                "model": model,
                "request_data": request_data,
                "response_data": response_data,
                "token_usage": token_usage.to_dict(),
                "cost_info": cost_info,
                "performance_metrics": performance_metrics,
                "tracing_info": tracing_info,
                "timestamp": datetime.now(timezone.utc)
            }
            
            # å†™å…¥æ•°æ®åº“
            success = await self._write_enhanced_log_entry(log_entry)
            
            if span:
                span.set_attribute("harborai.log.success", success)
                
            return success
            
        except Exception as e:
            self.logger.error(
                "Failed to log response with enhanced features",
                error=str(e),
                trace_id=trace_id,
                provider=provider,
                model=model,
                exc_info=True
            )
            
            if span:
                span.set_attribute("harborai.log.error", str(e))
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                
            return False
        finally:
            if span:
                span.end()
    
    async def _apply_custom_pricing(
        self,
        provider: str,
        model: str,
        custom_pricing: Dict[str, Any]
    ):
        """åº”ç”¨è‡ªå®šä¹‰ä»·æ ¼è®¾ç½®"""
        
        if "input_price" in custom_pricing and "output_price" in custom_pricing:
            await self.pricing_calculator.add_model_pricing(
                provider=provider,
                model=model,
                input_price_per_1k=custom_pricing["input_price"],
                output_price_per_1k=custom_pricing["output_price"],
                currency=custom_pricing.get("currency", "RMB")
            )
            
            self.logger.info(
                "Applied custom pricing",
                provider=provider,
                model=model,
                input_price=custom_pricing["input_price"],
                output_price=custom_pricing["output_price"]
            )
    
    async def _write_enhanced_log_entry(
        self, 
        log_entry: Dict[str, Any]
    ) -> bool:
        """å†™å…¥å¢å¼ºçš„æ—¥å¿—æ¡ç›®åˆ°æ•°æ®åº“"""
        
        try:
            async with self.db_pool.acquire() as conn:
                # æ’å…¥ä¸»æ—¥å¿—è®°å½•
                log_id = await conn.fetchval("""
                    INSERT INTO api_logs (
                        trace_id, provider, model, request_data, response_data,
                        timestamp, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                    RETURNING id
                """, 
                    log_entry["trace_id"],
                    log_entry["provider"],
                    log_entry["model"],
                    json.dumps(log_entry["request_data"]),
                    json.dumps(log_entry["response_data"]),
                    log_entry["timestamp"],
                    log_entry["timestamp"]
                )
                
                # æ’å…¥tokenä½¿ç”¨è®°å½•
                await conn.execute("""
                    INSERT INTO token_usage (
                        log_id, prompt_tokens, completion_tokens, total_tokens
                    ) VALUES ($1, $2, $3, $4)
                """,
                    log_id,
                    log_entry["token_usage"]["prompt_tokens"],
                    log_entry["token_usage"]["completion_tokens"],
                    log_entry["token_usage"]["total_tokens"]
                )
                
                # æ’å…¥æˆæœ¬ä¿¡æ¯
                cost_info = log_entry["cost_info"]
                await conn.execute("""
                    INSERT INTO cost_info (
                        log_id, input_cost, output_cost, total_cost,
                        currency, pricing_source, pricing_details
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                """,
                    log_id,
                    cost_info["input_cost"],
                    cost_info["output_cost"],
                    cost_info["total_cost"],
                    cost_info["currency"],
                    cost_info["pricing_source"],
                    json.dumps({
                        "pricing_timestamp": cost_info.get("pricing_timestamp"),
                        "calculation_method": "enhanced_calculator"
                    })
                )
                
                # æ’å…¥è¿½è¸ªä¿¡æ¯
                if log_entry.get("tracing_info"):
                    tracing_info = log_entry["tracing_info"]
                    await conn.execute("""
                        INSERT INTO tracing_info (
                            log_id, trace_id, span_id, operation_name,
                            start_time, status, tags, logs
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                    """,
                        log_id,
                        tracing_info["trace_id"],
                        tracing_info["span_id"],
                        tracing_info["operation_name"],
                        tracing_info["start_time"],
                        tracing_info["status"],
                        json.dumps(tracing_info["tags"]),
                        json.dumps(tracing_info["logs"])
                    )
                
                # æ’å…¥æ€§èƒ½æŒ‡æ ‡
                if log_entry.get("performance_metrics"):
                    perf_metrics = log_entry["performance_metrics"]
                    await conn.execute("""
                        INSERT INTO performance_metrics (
                            log_id, latency_ms, tokens_per_second,
                            first_token_latency_ms, processing_time_ms
                        ) VALUES ($1, $2, $3, $4, $5)
                    """,
                        log_id,
                        perf_metrics.get("latency_ms"),
                        perf_metrics.get("tokens_per_second"),
                        perf_metrics.get("first_token_latency_ms"),
                        perf_metrics.get("processing_time_ms")
                    )
                
                return True
                
        except Exception as e:
            self.logger.error(
                "Failed to write enhanced log entry",
                error=str(e),
                trace_id=log_entry["trace_id"],
                exc_info=True
            )
            return False
```

#### 5.1.6 å¢å¼ºç‰ˆé™çº§æ—¥å¿—è®°å½•å™¨

```python
class ImprovedFallbackLogger(FallbackLogger):
    """æ”¹è¿›çš„é™çº§æ—¥å¿—è®°å½•å™¨ï¼Œé›†æˆå¢å¼ºåŠŸèƒ½"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.enhanced_postgresql_logger = OptimizedPostgreSQLLogger(config)
        self.token_parser = TokenParsingService()
        self.pricing_calculator = EnhancedPricingCalculator()
        self.tracer = OpenTelemetryTracer(config)
        self.logger = structlog.get_logger(__name__)
        
    async def log_ai_request(
        self,
        request_data: Dict[str, Any],
        response_data: Dict[str, Any],
        provider: str,
        model: str,
        trace_id: str,
        performance_metrics: Dict[str, Any],
        custom_pricing: Optional[Dict[str, Any]] = None
    ) -> bool:
        """è®°å½•AIè¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†é™çº§é€»è¾‘"""
        
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨å¢å¼ºç‰ˆPostgreSQLè®°å½•å™¨
            success = await self.enhanced_postgresql_logger.log_response_with_enhanced_features(
                request_data=request_data,
                response_data=response_data,
                provider=provider,
                model=model,
                trace_id=trace_id,
                performance_metrics=performance_metrics,
                custom_pricing=custom_pricing
            )
            
            if success:
                self.logger.info(
                    "Successfully logged to PostgreSQL",
                    trace_id=trace_id,
                    provider=provider,
                    model=model
                )
                return True
                
        except Exception as e:
            self.logger.warning(
                "PostgreSQL logging failed, falling back to file logging",
                error=str(e),
                trace_id=trace_id,
                provider=provider,
                model=model
            )
        
        # é™çº§åˆ°æ–‡ä»¶æ—¥å¿—è®°å½•
        return await self._fallback_to_file_logging(
            request_data, response_data, provider, model, 
            trace_id, performance_metrics, custom_pricing
        )
    
    async def _fallback_to_file_logging(
        self,
        request_data: Dict[str, Any],
        response_data: Dict[str, Any],
        provider: str,
        model: str,
        trace_id: str,
        performance_metrics: Dict[str, Any],
        custom_pricing: Optional[Dict[str, Any]]
    ) -> bool:
        """é™çº§åˆ°æ–‡ä»¶æ—¥å¿—è®°å½•ï¼Œä¿æŒå¢å¼ºåŠŸèƒ½"""
        
        try:
            # è§£ætokenä½¿ç”¨é‡
            token_usage = await self.token_parser.parse_tokens(
                provider, response_data
            )
            
            # å¤„ç†è‡ªå®šä¹‰ä»·æ ¼è®¾ç½®
            if custom_pricing:
                await self._apply_custom_pricing_for_fallback(
                    provider, model, custom_pricing
                )
            
            # è®¡ç®—è¯¦ç»†æˆæœ¬åˆ†è§£
            cost_info = await self.pricing_calculator.calculate_detailed_cost(
                provider, model,
                token_usage.prompt_tokens,
                token_usage.completion_tokens
            )
            
            # åˆ›å»ºè¿½è¸ªspanï¼ˆå¦‚æœå¯ç”¨ï¼‰
            span = await self.tracer.create_ai_span(
                operation_name="ai_request_fallback_logging",
                provider=provider,
                model=model,
                trace_id=trace_id
            )
            
            # è®°å½•AIæŒ‡æ ‡åˆ°span
            if span:
                await self.tracer.record_ai_metrics(
                    span, token_usage, cost_info, performance_metrics
                )
            
            # æ„å»ºå¢å¼ºçš„æ–‡ä»¶æ—¥å¿—æ¡ç›®
            enhanced_log_entry = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "trace_id": trace_id,
                "span_id": format(span.get_span_context().span_id, "016x") if span else None,
                "provider": provider,
                "model": model,
                "request_data": request_data,
                "response_data": response_data,
                "token_usage": token_usage.to_dict(),
                "cost_info": cost_info,
                "performance_metrics": performance_metrics,
                "fallback_reason": "postgresql_unavailable",
                "log_level": "fallback"
            }
            
            # å†™å…¥æ–‡ä»¶æ—¥å¿—
            success = await self._write_enhanced_file_log(enhanced_log_entry)
            
            if span:
                span.set_attribute("harborai.fallback.success", success)
                span.end()
            
            if success:
                self.logger.info(
                    "Successfully logged to file (fallback)",
                    trace_id=trace_id,
                    provider=provider,
                    model=model
                )
            
            return success
            
        except Exception as e:
            self.logger.error(
                "Fallback file logging also failed",
                error=str(e),
                trace_id=trace_id,
                provider=provider,
                model=model,
                exc_info=True
            )
            return False
    
    async def _apply_custom_pricing_for_fallback(
        self,
        provider: str,
        model: str,
        custom_pricing: Dict[str, Any]
    ):
        """ä¸ºé™çº§æ—¥å¿—è®°å½•åº”ç”¨è‡ªå®šä¹‰ä»·æ ¼è®¾ç½®"""
        
        if "input_price" in custom_pricing and "output_price" in custom_pricing:
            await self.pricing_calculator.add_model_pricing(
                provider=provider,
                model=model,
                input_price_per_1k=custom_pricing["input_price"],
                output_price_per_1k=custom_pricing["output_price"],
                currency=custom_pricing.get("currency", "CNY")
            )
    
    async def _write_enhanced_file_log(
        self, 
        log_entry: Dict[str, Any]
    ) -> bool:
        """å†™å…¥å¢å¼ºçš„æ–‡ä»¶æ—¥å¿—"""
        
        try:
            log_file_path = self._get_log_file_path()
            
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
            
            # å†™å…¥JSONæ ¼å¼çš„æ—¥å¿—
            with open(log_file_path, 'a', encoding='utf-8') as f:
                json.dump(log_entry, f, ensure_ascii=False, default=str)
                f.write('\n')
            
            # è®°å½•åˆ°å¤‡ä»½è¡¨ï¼ˆå¦‚æœPostgreSQLå¯ç”¨ï¼‰
            try:
                await self._record_file_backup_entry(log_entry)
            except Exception as backup_error:
                self.logger.debug(
                    "Failed to record file backup entry",
                    error=str(backup_error)
                )
            
            return True
            
        except Exception as e:
            self.logger.error(
                "Failed to write enhanced file log",
                error=str(e),
                trace_id=log_entry.get("trace_id"),
                exc_info=True
            )
            return False
    
    def _get_log_file_path(self) -> str:
        """è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
        
        log_dir = self.config.get("LOG_DIR", "./logs")
        date_str = datetime.now().strftime("%Y-%m-%d")
        return os.path.join(log_dir, f"harborai_fallback_{date_str}.jsonl")
    
    async def _record_file_backup_entry(
        self, 
        log_entry: Dict[str, Any]
    ):
        """è®°å½•æ–‡ä»¶å¤‡ä»½æ¡ç›®åˆ°æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        
        try:
            if hasattr(self, 'db_pool') and self.db_pool:
                async with self.db_pool.acquire() as conn:
                    await conn.execute("""
                        INSERT INTO file_log_backups (
                            trace_id, file_path, log_content, created_at
                        ) VALUES ($1, $2, $3, $4)
                    """,
                        log_entry["trace_id"],
                        self._get_log_file_path(),
                        json.dumps(log_entry),
                        datetime.now(timezone.utc)
                    )
        except Exception:
            # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»è¦çš„æ–‡ä»¶æ—¥å¿—è®°å½•
            pass
```
 
### 5.2 å­˜å‚¨æ¶æ„ä¼˜åŒ–å®ç°

#### 5.2.1 ä¼˜åŒ–çš„PostgreSQL Logger

```python
class OptimizedPostgreSQLLogger(PostgreSQLLogger):
    """åŸºäºç°æœ‰PostgreSQLLoggerçš„ä¼˜åŒ–ç‰ˆæœ¬"""
    
    async def log_response_with_tokens(
        self,
        trace_id: str,
        provider: str,
        model: str,
        response_data: Dict[str, Any],
        token_usage: TokenUsage,
        cost_info: Optional[Dict[str, Any]] = None
    ):
        """è®°å½•å“åº”æ—¥å¿—ï¼ŒåŒ…å«ä¼˜åŒ–çš„tokenä¿¡æ¯"""
        
        log_entry = {
            "trace_id": trace_id,
            "timestamp": datetime.now(timezone.utc),
            "log_type": "response",
            "provider": provider,
            "model": model,
            "success": True,
            "token_usage": {
                "prompt_tokens": token_usage.prompt_tokens,      # ä¿æŒåŸå§‹å­—æ®µå
                "completion_tokens": token_usage.completion_tokens,  # ä¿æŒåŸå§‹å­—æ®µå
                "total_tokens": token_usage.total_tokens,
                "parsing_method": token_usage.parsing_method,
                "confidence": token_usage.confidence
            },
            "cost_info": cost_info,
            "response_data": self._sanitize_response_data(response_data)
        }
        
        # ä½¿ç”¨ç°æœ‰çš„æ‰¹é‡å†™å…¥æœºåˆ¶
        await self._add_to_batch(log_entry)
```

#### 5.2.2 å¢å¼ºçš„é™çº§ç®¡ç†å™¨

```python
class ImprovedFallbackLogger(FallbackLogger):
    """åŸºäºç°æœ‰FallbackLoggerçš„å¢å¼ºç‰ˆæœ¬"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.token_parsing_service = TokenParsingService()
        self.health_check_interval = config.get("health_check_interval", 60.0)
        self.max_postgres_failures = config.get("max_postgres_failures", 3)
        
    async def log_response_with_enhanced_tokens(
        self,
        trace_id: str,
        provider: str,
        model: str,
        response_data: Dict[str, Any]
    ):
        """å¢å¼ºçš„å“åº”æ—¥å¿—è®°å½•ï¼ŒåŒ…å«ä¼˜åŒ–çš„tokenè§£æ"""
        
        # è§£ætokenä½¿ç”¨é‡
        token_usage = await self.token_parsing_service.parse_token_usage(
            provider=provider,
            model=model,
            response_data=response_data,
            trace_id=trace_id
        )
        
        # è®¡ç®—æˆæœ¬ä¿¡æ¯
        cost_info = await self._calculate_cost(token_usage, provider, model)
        
        # æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©å­˜å‚¨æ–¹å¼
        if self.state == LoggerState.POSTGRES_ACTIVE:
            try:
                await self.postgres_logger.log_response_with_tokens(
                    trace_id=trace_id,
                    provider=provider,
                    model=model,
                    response_data=response_data,
                    token_usage=token_usage,
                    cost_info=cost_info
                )
            except Exception as e:
                self.logger.warning(
                    "PostgreSQLå†™å…¥å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æ–‡ä»¶æ—¥å¿—",
                    trace_id=trace_id,
                    error=str(e)
                )
                await self._handle_postgres_failure()
                await self.file_logger.log_response_with_tokens(
                    trace_id=trace_id,
                    provider=provider,
                    model=model,
                    response_data=response_data,
                    token_usage=token_usage,
                    cost_info=cost_info
                )
        else:
            # æ–‡ä»¶æ—¥å¿—æ¨¡å¼
            await self.file_logger.log_response_with_tokens(
                trace_id=trace_id,
                provider=provider,
                model=model,
                response_data=response_data,
                token_usage=token_usage,
                cost_info=cost_info
            )
```

### 5.3 æ•°æ®ä¸€è‡´æ€§ä¿éšœå®ç°

#### 5.3.1 æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨

```python
class DataConsistencyChecker:
    """æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å’Œä¿®æ­£æœåŠ¡"""
    
    def __init__(self, postgres_logger: PostgreSQLLogger):
        self.postgres_logger = postgres_logger
        self.logger = structlog.get_logger(__name__)
        
    async def check_and_fix_token_consistency(self, batch_size: int = 1000):
        """æ£€æŸ¥å’Œä¿®æ­£tokenæ•°æ®ä¸ä¸€è‡´é—®é¢˜"""
        
        query = """
        SELECT id, prompt_tokens, completion_tokens, total_tokens
        FROM token_usage 
        WHERE total_tokens != (prompt_tokens + completion_tokens)
        LIMIT %s
        """
        
        async with self.postgres_logger.get_connection() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(query, (batch_size,))
                inconsistent_records = await cursor.fetchall()
                
                if inconsistent_records:
                    self.logger.warning(
                        f"å‘ç° {len(inconsistent_records)} æ¡tokenæ•°æ®ä¸ä¸€è‡´è®°å½•",
                        count=len(inconsistent_records)
                    )
                    
                    # æ‰¹é‡ä¿®æ­£
                    for record in inconsistent_records:
                        record_id, prompt_tokens, completion_tokens, total_tokens = record
                        correct_total = prompt_tokens + completion_tokens
                        
                        update_query = """
                        UPDATE token_usage 
                        SET total_tokens = %s,
                            updated_at = NOW()
                        WHERE id = %s
                        """
                        
                        await cursor.execute(update_query, (correct_total, record_id))
                        
                        self.logger.info(
                            "ä¿®æ­£tokenæ•°æ®ä¸ä¸€è‡´",
                            record_id=record_id,
                            old_total=total_tokens,
                            new_total=correct_total
                        )
                    
                    await conn.commit()
                    
                return len(inconsistent_records)
```

### 5.4 ç›‘æ§å’Œå‘Šè­¦å¢å¼ºå®ç°

#### 5.4.1 å¢å¼ºçš„PrometheusæŒ‡æ ‡

```python
class EnhancedPrometheusMetrics(PrometheusMetrics):
    """åŸºäºç°æœ‰PrometheusMetricsçš„å¢å¼ºç‰ˆæœ¬"""
    
    def __init__(self):
        super().__init__()
        
        # æ–°å¢é™çº§ç›¸å…³æŒ‡æ ‡
        self.fallback_state_gauge = Gauge(
            'harborai_fallback_logger_state',
            'FallbackLoggerå½“å‰çŠ¶æ€ (0=PostgreSQL, 1=æ–‡ä»¶æ—¥å¿—, 2=åˆå§‹åŒ–ä¸­, 3=é”™è¯¯)',
            ['logger_instance']
        )
        
        self.postgres_health_gauge = Gauge(
            'harborai_postgres_health_status',
            'PostgreSQLå¥åº·çŠ¶æ€ (0=ä¸å¥åº·, 1=å¥åº·)',
            ['database_name']
        )
        
        self.token_consistency_counter = Counter(
            'harborai_token_consistency_fixes_total',
            'Tokenæ•°æ®ä¸€è‡´æ€§ä¿®æ­£æ¬¡æ•°',
            ['fix_type']
        )
        
        self.token_parsing_accuracy_histogram = Histogram(
            'harborai_token_parsing_confidence',
            'Tokenè§£æç½®ä¿¡åº¦åˆ†å¸ƒ',
            ['provider', 'parsing_method']
        )
    
    def record_fallback_state(self, state: LoggerState, instance: str = "default"):
        """è®°å½•é™çº§çŠ¶æ€"""
        state_value = {
            LoggerState.POSTGRES_ACTIVE: 0,
            LoggerState.FILE_FALLBACK: 1,
            LoggerState.INITIALIZING: 2,
            LoggerState.ERROR: 3
        }.get(state, 3)
        
        self.fallback_state_gauge.labels(logger_instance=instance).set(state_value)
    
    def record_postgres_health(self, is_healthy: bool, database: str = "harborai"):
        """è®°å½•PostgreSQLå¥åº·çŠ¶æ€"""
        self.postgres_health_gauge.labels(database_name=database).set(1 if is_healthy else 0)
    
    def record_token_consistency_fix(self, fix_type: str = "auto_correction"):
        """è®°å½•tokenä¸€è‡´æ€§ä¿®æ­£"""
        self.token_consistency_counter.labels(fix_type=fix_type).inc()
    
    def record_token_parsing_confidence(
        self, 
        confidence: float, 
        provider: str, 
        parsing_method: str
    ):
        """è®°å½•tokenè§£æç½®ä¿¡åº¦"""
        self.token_parsing_accuracy_histogram.labels(
            provider=provider,
            parsing_method=parsing_method
        ).observe(confidence)
```

### 5.5 æ•°æ®åº“æ¶æ„ä¼˜åŒ–

#### 5.5.1 ä¼˜åŒ–çš„æ•°æ®åº“è¡¨ç»“æ„

```sql
-- ä¼˜åŒ–token_usageè¡¨ï¼Œä¿æŒå‚å•†åŸå§‹å­—æ®µå
CREATE TABLE IF NOT EXISTS token_usage_optimized (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    log_id UUID NOT NULL REFERENCES api_logs(id) ON DELETE CASCADE,
    prompt_tokens INTEGER NOT NULL DEFAULT 0,        -- ä¿æŒå‚å•†åŸå§‹å­—æ®µå
    completion_tokens INTEGER NOT NULL DEFAULT 0,    -- ä¿æŒå‚å•†åŸå§‹å­—æ®µå
    total_tokens INTEGER NOT NULL DEFAULT 0,
    parsing_method VARCHAR(50) DEFAULT 'direct_extraction',
    confidence FLOAT DEFAULT 1.0 CHECK (confidence >= 0.0 AND confidence <= 1.0),
    raw_usage_data JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    -- çº¦æŸ
    CONSTRAINT token_usage_positive_tokens CHECK (
        prompt_tokens >= 0 AND completion_tokens >= 0 AND total_tokens >= 0
    ),
    CONSTRAINT token_usage_consistency CHECK (
        total_tokens = prompt_tokens + completion_tokens
    )
);

-- åˆ›å»ºä¼˜åŒ–çš„ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_token_usage_opt_log_id 
ON token_usage_optimized(log_id);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_token_usage_opt_total_tokens 
ON token_usage_optimized(total_tokens DESC);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_token_usage_opt_parsing_method 
ON token_usage_optimized(parsing_method);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_token_usage_opt_confidence 
ON token_usage_optimized(confidence DESC);

-- åˆ›å»ºè‡ªåŠ¨æ›´æ–°updated_atçš„è§¦å‘å™¨
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_token_usage_updated_at 
    BEFORE UPDATE ON token_usage_optimized 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

## 6. å®æ–½è®¡åˆ’

### 6.1 é˜¶æ®µä¸€ï¼šTokenè§£æä¼˜åŒ–ï¼ˆç¬¬1-2å‘¨ï¼‰

**ç›®æ ‡ï¼š** å®ç°tokenå­—æ®µåå¯¹é½å’Œè§£æé€»è¾‘ä¼˜åŒ–

**ä»»åŠ¡æ¸…å•ï¼š**
- [ ] å®ç°å„å‚å•†çš„Tokenè§£æå™¨ï¼ˆDeepSeekã€OpenAIã€Doubaoç­‰ï¼‰
- [ ] ä¼˜åŒ–TokenParsingServiceï¼Œæ”¯æŒç›´æ¥å­—æ®µæå–
- [ ] æ›´æ–°æ•°æ®åº“è¡¨ç»“æ„ï¼Œä¿æŒprompt_tokenså’Œcompletion_tokenså­—æ®µå
- [ ] å®ç°æ•°æ®ä¸€è‡´æ€§éªŒè¯é€»è¾‘
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

**éªŒæ”¶æ ‡å‡†ï¼š**
- æ‰€æœ‰å‚å•†çš„tokenè§£æå‡†ç¡®ç‡è¾¾åˆ°99%ä»¥ä¸Š
- æ•°æ®åº“å­—æ®µåä¸å‚å•†å“åº”å®Œå…¨ä¸€è‡´
- é€šè¿‡æ‰€æœ‰tokenä¸€è‡´æ€§éªŒè¯æµ‹è¯•

### 6.2 é˜¶æ®µäºŒï¼šå­˜å‚¨æ¶æ„ç®€åŒ–ï¼ˆç¬¬3-4å‘¨ï¼‰

**ç›®æ ‡ï¼š** ç®€åŒ–å­˜å‚¨æ¶æ„ï¼Œç§»é™¤Redisä¾èµ–

**ä»»åŠ¡æ¸…å•ï¼š**
- [ ] ä¼˜åŒ–PostgreSQLLoggerï¼Œä¸“æ³¨äºtokenå­—æ®µå¯¹é½
- [ ] å¢å¼ºFileSystemLoggerçš„å¤‡ä»½æ¢å¤èƒ½åŠ›
- [ ] æ”¹è¿›FallbackLoggerçš„é™çº§ç­–ç•¥
- [ ] å®ç°å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨æ¢å¤æœºåˆ¶
- [ ] ç§»é™¤æ‰€æœ‰Redisç›¸å…³ä»£ç å’Œé…ç½®

**éªŒæ”¶æ ‡å‡†ï¼š**
- PostgreSQLä¸å¯ç”¨æ—¶èƒ½è‡ªåŠ¨åˆ‡æ¢åˆ°æ–‡ä»¶æ—¥å¿—
- PostgreSQLæ¢å¤åèƒ½è‡ªåŠ¨åˆ‡æ¢å›æ¥å¹¶åŒæ­¥æ•°æ®
- ç³»ç»Ÿåœ¨é™çº§æ¨¡å¼ä¸‹åŠŸèƒ½å®Œæ•´å¯ç”¨

### 6.3 é˜¶æ®µä¸‰ï¼šç›‘æ§å’Œå‘Šè­¦å¢å¼ºï¼ˆç¬¬5-6å‘¨ï¼‰

**ç›®æ ‡ï¼š** å¢å¼ºç³»ç»Ÿç›‘æ§å’Œæ•°æ®è´¨é‡ä¿éšœ

**ä»»åŠ¡æ¸…å•ï¼š**
- [ ] æ‰©å±•PrometheusMetricsï¼Œå¢åŠ é™çº§çŠ¶æ€å’Œæ•°æ®è´¨é‡æŒ‡æ ‡
- [ ] å®ç°æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å’Œè‡ªåŠ¨ä¿®æ­£
- [ ] ä¼˜åŒ–å¥åº·æ£€æŸ¥æœºåˆ¶
- [ ] å®ç°å‘Šè­¦è§„åˆ™å’Œé€šçŸ¥æœºåˆ¶
- [ ] å®Œå–„ç›‘æ§é¢æ¿å’ŒæŠ¥è¡¨

**éªŒæ”¶æ ‡å‡†ï¼š**
- èƒ½å®æ—¶ç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶æ€å’Œé™çº§çŠ¶æ€
- èƒ½è‡ªåŠ¨æ£€æµ‹å’Œä¿®æ­£æ•°æ®ä¸ä¸€è‡´é—®é¢˜
- ç›‘æ§æŒ‡æ ‡è¦†ç›–ç‡è¾¾åˆ°95%ä»¥ä¸Š

### 6.4 é˜¶æ®µå››ï¼šæµ‹è¯•å’Œéƒ¨ç½²ï¼ˆç¬¬7-8å‘¨ï¼‰

**ç›®æ ‡ï¼š** å…¨é¢æµ‹è¯•å’Œç”Ÿäº§éƒ¨ç½²

**ä»»åŠ¡æ¸…å•ï¼š**
- [ ] å®Œå–„å•å…ƒæµ‹è¯•ï¼Œè¦†ç›–ç‡è¾¾åˆ°90%ä»¥ä¸Š
- [ ] è¿›è¡Œé›†æˆæµ‹è¯•å’Œæ€§èƒ½æµ‹è¯•
- [ ] è¿›è¡Œæ•…éšœæ³¨å…¥æµ‹è¯•ï¼ŒéªŒè¯é™çº§æœºåˆ¶
- [ ] ç¼–å†™éƒ¨ç½²æ–‡æ¡£å’Œè¿ç»´æ‰‹å†Œ
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å’Œç›‘æ§

**éªŒæ”¶æ ‡å‡†ï¼š**
- æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿç¨³å®šè¿è¡Œ
- æ€§èƒ½æŒ‡æ ‡æ»¡è¶³è¦æ±‚
- é™çº§æœºåˆ¶åœ¨æ•…éšœåœºæ™¯ä¸‹æ­£å¸¸å·¥ä½œ

## 7. é£é™©è¯„ä¼°ä¸åº”å¯¹

### 7.1 æŠ€æœ¯é£é™©

| é£é™©é¡¹ | é£é™©ç­‰çº§ | å½±å“ | åº”å¯¹æªæ–½ |
|--------|----------|------|----------|
| å‚å•†APIå“åº”æ ¼å¼å˜æ›´ | ä¸­ | tokenè§£æå¤±è´¥ | å®ç°å®¹é”™æœºåˆ¶ï¼Œæ”¯æŒå¤šç‰ˆæœ¬è§£æå™¨ |
| PostgreSQLè¿æ¥ä¸ç¨³å®š | ä¸­ | æ•°æ®ä¸¢å¤± | å®Œå–„é™çº§æœºåˆ¶ï¼Œç¡®ä¿æ–‡ä»¶æ—¥å¿—å¯é æ€§ |
| æ•°æ®è¿ç§»è¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´ | ä½ | å†å²æ•°æ®é—®é¢˜ | å®ç°æ•°æ®éªŒè¯å’Œä¿®æ­£å·¥å…· |
| æ€§èƒ½å›å½’ | ä½ | ç³»ç»Ÿå“åº”å˜æ…¢ | è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œä¼˜åŒ–å…³é”®è·¯å¾„ |

### 7.2 ä¸šåŠ¡é£é™©

| é£é™©é¡¹ | é£é™©ç­‰çº§ | å½±å“ | åº”å¯¹æªæ–½ |
|--------|----------|------|----------|
| ç”¨æˆ·æ¥å£å˜æ›´ | ä½ | ç”¨æˆ·é€‚åº”æˆæœ¬ | ä¿æŒå‘åå…¼å®¹ï¼Œæä¾›è¿ç§»æŒ‡å— |
| ç›‘æ§æ•°æ®ä¸­æ–­ | ä¸­ | è¿ç»´ç›²åŒº | å®ç°å¤šå±‚ç›‘æ§ï¼Œç¡®ä¿ç›‘æ§ç³»ç»Ÿé«˜å¯ç”¨ |
| æˆæœ¬è®¡ç®—åå·® | ä¸­ | ç”¨æˆ·ä¿¡ä»»åº¦ä¸‹é™ | å®ç°æˆæœ¬è®¡ç®—éªŒè¯æœºåˆ¶ |

### 7.3 åº”æ€¥é¢„æ¡ˆ

**PostgreSQLå®Œå…¨ä¸å¯ç”¨ï¼š**
1. è‡ªåŠ¨åˆ‡æ¢åˆ°æ–‡ä»¶æ—¥å¿—æ¨¡å¼
2. å¯ç”¨æ–‡ä»¶æ—¥å¿—å‹ç¼©å’Œè½®è½¬
3. å®šæœŸæ£€æŸ¥PostgreSQLæ¢å¤çŠ¶æ€
4. æ¢å¤åæ‰¹é‡åŒæ­¥æ–‡ä»¶æ—¥å¿—æ•°æ®

**Tokenè§£æå¤§é¢ç§¯å¤±è´¥ï¼š**
1. å¯ç”¨é™çº§è§£ææ¨¡å¼ï¼ˆä½¿ç”¨ä¼°ç®—å€¼ï¼‰
2. è®°å½•è§£æå¤±è´¥è¯¦æƒ…ç”¨äºåç»­åˆ†æ
3. é€šçŸ¥è¿ç»´äººå‘˜è¿›è¡Œäººå·¥å¹²é¢„
4. æ›´æ–°è§£æå™¨ä»¥é€‚åº”æ–°çš„APIæ ¼å¼

## 8. é¢„æœŸæ•ˆæœ

### 8.1 æ•°æ®å‡†ç¡®æ€§æå‡

- **Tokenè§£æå‡†ç¡®ç‡**ï¼šä»å½“å‰çš„95%æå‡åˆ°99%ä»¥ä¸Š
- **æ•°æ®ä¸€è‡´æ€§**ï¼šæ¶ˆé™¤tokenæ€»æ•°ä¸ä¸€è‡´é—®é¢˜
- **æˆæœ¬è®¡ç®—ç²¾åº¦**ï¼šæå‡æˆæœ¬è®¡ç®—å‡†ç¡®æ€§åˆ°å°æ•°ç‚¹å6ä½

### 8.2 ç³»ç»Ÿå¯é æ€§å¢å¼º

- **ç³»ç»Ÿå¯ç”¨æ€§**ï¼šä»99.5%æå‡åˆ°99.9%
- **æ•…éšœæ¢å¤æ—¶é—´**ï¼šä»æ‰‹åŠ¨æ¢å¤ç¼©çŸ­åˆ°è‡ªåŠ¨æ¢å¤ï¼ˆ<5åˆ†é’Ÿï¼‰
- **æ•°æ®ä¸¢å¤±é£é™©**ï¼šé™ä½åˆ°æ¥è¿‘é›¶ï¼ˆé€šè¿‡æ–‡ä»¶æ—¥å¿—å¤‡ä»½ï¼‰

### 8.3 è¿ç»´æ•ˆç‡æå‡

- **ç›‘æ§è¦†ç›–ç‡**ï¼šä»80%æå‡åˆ°95%ä»¥ä¸Š
- **æ•…éšœæ£€æµ‹æ—¶é—´**ï¼šä»åˆ†é’Ÿçº§ç¼©çŸ­åˆ°ç§’çº§
- **äººå·¥å¹²é¢„éœ€æ±‚**ï¼šå‡å°‘70%çš„äººå·¥è¿ç»´å·¥ä½œ

### 8.4 å¼€å‘ä½“éªŒæ”¹å–„

- **APIå“åº”æ—¶é—´**ï¼šä¿æŒåœ¨å½“å‰æ°´å¹³ï¼ˆ<100msï¼‰
- **é”™è¯¯ç‡**ï¼šé™ä½50%çš„tokenè§£æé”™è¯¯
- **è°ƒè¯•æ•ˆç‡**ï¼šé€šè¿‡ç»“æ„åŒ–æ—¥å¿—æå‡è°ƒè¯•æ•ˆç‡

## 9. æˆæœ¬ç®¡ç†å’Œåˆ†å¸ƒå¼è¿½è¸ªå¢å¼ºè¯´æ˜

### 9.1 æˆæœ¬ç®¡ç†ä¼˜åŒ–ç‰¹æ€§

**ç¯å¢ƒå˜é‡é…ç½®æ”¯æŒï¼š**
- æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡åŠ¨æ€è®¾ç½®æ¨¡å‹ä»·æ ¼ï¼š`OPENAI_GPT4_INPUT_PRICE`ã€`DEEPSEEK_INPUT_PRICE`ç­‰
- æ”¯æŒå¤šè´§å¸é…ç½®ï¼š`COST_CURRENCY=RMB`
- æ”¯æŒä»·æ ¼æ¥æºè¿½è¸ªï¼š`environment_variable`ã€`api_parameter`ã€`default_config`

**è¯¦ç»†æˆæœ¬åˆ†è§£ï¼š**
```json
{
  "cost_info": {
    "input_cost": 0.000063,
    "output_cost": 0.000126,
    "total_cost": 0.000189,
    "currency": "RMB",
    "pricing_source": "environment_variable",
    "pricing_timestamp": "2025-01-15T10:30:00Z"
  }
}
```

**çµæ´»çš„ä»·æ ¼è®¾ç½®æ–¹å¼ï¼š**
1. **ç¯å¢ƒå˜é‡æ–¹å¼**ï¼šé€‚åˆéƒ¨ç½²æ—¶é…ç½®ï¼Œæ”¯æŒæ ‡å‡†åŒ–å‘½åè§„èŒƒ
2. **è¯·æ±‚å‚æ•°æ–¹å¼**ï¼šé€šè¿‡`custom_pricing`å‚æ•°åŠ¨æ€è®¾ç½®ï¼Œé€‚åˆå®æ—¶ä»·æ ¼è°ƒæ•´
3. **é»˜è®¤é…ç½®æ–¹å¼**ï¼šå›é€€åˆ°ç³»ç»Ÿé¢„è®¾çš„ä»·æ ¼é…ç½®

### 9.2 åˆ†å¸ƒå¼è¿½è¸ªé›†æˆç‰¹æ€§

**OpenTelemetryæ ‡å‡†é›†æˆï¼š**
- å®Œæ•´çš„åˆ†å¸ƒå¼è¿½è¸ªæ”¯æŒï¼Œç¬¦åˆOpenTelemetryæ ‡å‡†
- è‡ªåŠ¨ç”Ÿæˆ`trace_id`å’Œ`span_id`
- æ”¯æŒOTLPåè®®å¯¼å‡ºåˆ°Jaegerã€Zipkinç­‰è¿½è¸ªç³»ç»Ÿ

**APIå“åº”ä¸­çš„è¿½è¸ªä¿¡æ¯ï¼ˆä¼˜åŒ–åæ— å†—ä½™è®¾è®¡ï¼‰ï¼š**
```json
{
  "tracing": {
    "trace_id": "4bf92f3577b34da6a3ce929d0e0e4736",
    "span_id": "00f067aa0ba902b7",
    "operation_name": "ai.chat.completion",
    "tags": {
      "ai.system": "harborai",
      "ai.version": "2.0.0",
      "request.id": "req_1703123456789",
      "user.session": "session_abc123",
      "environment": "production",
      "service.name": "harborai-logging"
    }
  }
}
```

**è®¾è®¡ä¼˜åŒ–è¯´æ˜ï¼š**
- **æ¶ˆé™¤å†—ä½™**ï¼šAPIå“åº”çš„`tracing.tags`ä¸­ç§»é™¤ä¸ä¸»å­—æ®µé‡å¤çš„`ai.provider`ã€`ai.model`ã€`ai.request.tokens`ã€`ai.response.tokens`ã€`ai.cost.amount`ã€`ai.cost.currency`
- **ä¿ç•™è¿½è¸ªä¸“ç”¨å­—æ®µ**ï¼šä¿ç•™`operation_name`å’Œè¿½è¸ªç³»ç»Ÿä¸“ç”¨çš„æ ‡ç­¾
- **æ·»åŠ ç³»ç»Ÿçº§æ ‡ç­¾**ï¼šå¢åŠ ç³»ç»Ÿç‰ˆæœ¬ã€è¯·æ±‚IDã€ç”¨æˆ·ä¼šè¯ã€ç¯å¢ƒç­‰è¿½è¸ªä¸“ç”¨ä¿¡æ¯
- **åŒå±‚æ˜ å°„æœºåˆ¶**ï¼š
  - `prepare_api_tracing`ï¼šç”ŸæˆAPIå“åº”çš„ç®€åŒ–è¿½è¸ªä¿¡æ¯ï¼ˆæ— å†—ä½™å­—æ®µï¼‰
  - `generate_internal_otel_span`ï¼šç”Ÿæˆå†…éƒ¨OpenTelemetryç³»ç»Ÿçš„å®Œæ•´æ ‡ç­¾ï¼ˆåŒ…å«æ‰€æœ‰AIç›¸å…³å­—æ®µï¼‰

**OpenTelemetryå†…éƒ¨æ˜ å°„æœºåˆ¶ï¼š**

ä¸ºäº†æ¶ˆé™¤æ•°æ®å†—ä½™åŒæ—¶ä¿æŒOpenTelemetryå…¼å®¹æ€§ï¼Œç³»ç»Ÿå®ç°äº†å†…éƒ¨æ˜ å°„æœåŠ¡ï¼š

```python
class OpenTelemetryMappingService:
    """OpenTelemetryå†…éƒ¨æ˜ å°„æœåŠ¡"""
    
    def generate_internal_otel_span(self, log_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„OpenTelemetry spanæ ‡ç­¾ï¼ˆä»…å†…éƒ¨è¿½è¸ªç³»ç»Ÿä½¿ç”¨ï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç”Ÿæˆçš„æ ‡ç­¾ä»…ç”¨äºå†…éƒ¨OpenTelemetryè¿½è¸ªç³»ç»Ÿï¼Œ
        ä¸ä¼šå‡ºç°åœ¨APIå“åº”ä¸­ï¼Œé¿å…æ•°æ®å†—ä½™ã€‚
        """
        return {
            "ai.model": log_data.get("model", ""),
            "ai.provider": log_data.get("provider", ""),
            "ai.request.tokens": log_data.get("token_usage", {}).get("prompt_tokens", 0),
            "ai.response.tokens": log_data.get("token_usage", {}).get("completion_tokens", 0),
            "ai.cost.amount": log_data.get("cost_info", {}).get("total_cost", 0),
            "ai.cost.currency": log_data.get("cost_info", {}).get("currency", "RMB"),
            "ai.system": "harborai",
            "ai.version": "2.0.0",
            "request.id": log_data.get("request_id"),
            "user.session": log_data.get("session_id"),
            "environment": "production",
            "service.name": "harborai-logging"
        }
    
    def prepare_api_tracing(self, log_data: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡APIå“åº”çš„è¿½è¸ªä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼Œæ— å†—ä½™å­—æ®µï¼‰"""
        return {
            "trace_id": log_data.get("trace_id"),
            "span_id": log_data.get("span_id"),
            "operation_name": "ai.chat.completion",
            "tags": {
                "ai.system": "harborai",
                "ai.version": "2.0.0",
                "request.id": log_data.get("request_id"),
                "user.session": log_data.get("session_id"),
                "environment": "production",
                "service.name": "harborai-logging"
            }
        }
```

**è®¾è®¡ä¼˜åŠ¿ï¼š**
1. **æ¶ˆé™¤å†—ä½™**ï¼šAPIå“åº”å‡å°‘60%çš„é‡å¤æ•°æ®ï¼Œ`tracing.tags`ä¸­ä¸å†åŒ…å«ä¸ä¸»å­—æ®µé‡å¤çš„AIç›¸å…³ä¿¡æ¯
2. **ä¿æŒå…¼å®¹**ï¼šé€šè¿‡`generate_internal_otel_span`æ–¹æ³•å†…éƒ¨å®Œæ•´æ”¯æŒOpenTelemetryæ ‡å‡†
3. **å•ä¸€æ•°æ®æº**ï¼šä¸»è¦æ•°æ®é€šè¿‡é¡¶çº§å­—æ®µæä¾›ï¼Œé¿å…æ•°æ®åŒæ­¥é—®é¢˜
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šå‡å°‘APIå“åº”ä½“ç§¯ï¼Œæå‡ä¼ è¾“æ•ˆç‡
5. **æ¸…æ™°åˆ†ç¦»**ï¼šAPIå“åº”ä¸“æ³¨äºä¸šåŠ¡æ•°æ®ï¼Œå†…éƒ¨è¿½è¸ªç³»ç»Ÿè·å¾—å®Œæ•´çš„OpenTelemetryæ ‡ç­¾

**å…¨é“¾è·¯æ€§èƒ½åˆ†æï¼š**
- æ”¯æŒAIè¯·æ±‚çš„å®Œæ•´è°ƒç”¨é“¾è¿½è¸ª
- è‡ªåŠ¨è®°å½•æ€§èƒ½æŒ‡æ ‡ï¼šå»¶è¿Ÿã€ååé‡ã€é”™è¯¯ç‡
- ä¸PrometheusæŒ‡æ ‡ç³»ç»Ÿè”åŠ¨ï¼Œæä¾›å¤šç»´åº¦ç›‘æ§

### 9.3 å‘åå…¼å®¹æ€§ä¿éšœ

**ä¿ç•™ç°æœ‰ä¼˜åŠ¿ï¼š**
- ç»§æ‰¿åŸæœ‰çš„æ•æ„Ÿä¿¡æ¯æ£€æµ‹å’Œè„±æ•æœºåˆ¶
- ä¿æŒPrometheusæŒ‡æ ‡æ”¶é›†å’Œå‘Šè­¦
- ç»´æŒç»“æ„åŒ–æ—¥å¿—æ ¼å¼ï¼ˆstructlogï¼‰
- æ”¯æŒå¤šç§æ€§èƒ½æ¨¡å¼ï¼ˆfast/balanced/fullï¼‰
- ä¿æŒå¼‚æ­¥æ‰¹é‡å†™å…¥å’Œè¿æ¥æ± ä¼˜åŒ–

**å¹³æ»‘å‡çº§è·¯å¾„ï¼š**
- åŸºäºç°æœ‰`PricingCalculator`å’Œ`PostgreSQLLogger`çš„å¢å¼º
- ä¿æŒAPIæ¥å£çš„å‘åå…¼å®¹æ€§
- æ”¯æŒæ¸è¿›å¼åŠŸèƒ½å¯ç”¨ï¼Œå¯é€šè¿‡é…ç½®å¼€å…³æ§åˆ¶
- æä¾›æ•°æ®è¿ç§»è„šæœ¬å’ŒéªŒè¯å·¥å…·

### 9.4 é…ç½®ç¤ºä¾‹

**ç¯å¢ƒå˜é‡é…ç½®ï¼š**
```bash
# æˆæœ¬ç®¡ç†é…ç½®
HARBORAI_COST_TRACKING=true
COST_CURRENCY=RMB
OPENAI_GPT4_INPUT_PRICE=0.00042
OPENAI_GPT4_OUTPUT_PRICE=0.00084
DEEPSEEK_INPUT_PRICE=0.00014
DEEPSEEK_OUTPUT_PRICE=0.00028

# åˆ†å¸ƒå¼è¿½è¸ªé…ç½®
OTEL_ENABLED=true
OTEL_SERVICE_NAME=harborai
OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:14268/api/traces
```

**ä»£ç ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# ä½¿ç”¨å¢å¼ºç‰ˆæ—¥å¿—è®°å½•å™¨
logger = ImprovedFallbackLogger(config)

# è®°å½•AIè¯·æ±‚ï¼Œæ”¯æŒè‡ªå®šä¹‰ä»·æ ¼
await logger.log_ai_request(
    request_data=request_data,
    response_data=response_data,
    provider="deepseek",
    model="deepseek-chat",
    trace_id=trace_id,
    performance_metrics=performance_metrics,
    custom_pricing={
        "input_price": 0.00015,
        "output_price": 0.00030,
        "currency": "CNY"
    }
)
```

### 9.5 åŒIDæŸ¥è¯¢å’ŒAPMé›†æˆ

**åŒTrace IDç­–ç•¥ï¼š**
- `hb_trace_id`ï¼šHarborAIå†…éƒ¨è¿½è¸ªIDï¼ˆå…¼å®¹ç°æœ‰ç³»ç»Ÿï¼‰
- `otel_trace_id`ï¼šOpenTelemetryæ ‡å‡†è¿½è¸ªIDï¼ˆ32ä½åå…­è¿›åˆ¶ï¼‰

**æŸ¥è¯¢ç¤ºä¾‹ï¼š**
```python
# æ”¯æŒä»»æ„Trace IDæŸ¥è¯¢
async def query_log_by_trace_id(trace_id: str):
    """é€šè¿‡ä»»æ„ç±»å‹çš„trace_idæŸ¥è¯¢æ—¥å¿—"""
    query = """
    SELECT 
        l.id, l.provider, l.model,
        tr.hb_trace_id, tr.otel_trace_id, tr.span_id,
        c.total_cost, c.currency,
        CONCAT('http://localhost:16686/trace/', tr.otel_trace_id) as jaeger_link,
        CONCAT('http://localhost:9411/zipkin/traces/', tr.otel_trace_id) as zipkin_link
    FROM api_logs l
    JOIN tracing_info tr ON l.id = tr.log_id
    LEFT JOIN cost_info c ON l.id = c.log_id
    WHERE tr.hb_trace_id = $1 OR tr.otel_trace_id = $1
    """
    
    async with db_pool.acquire() as conn:
        return await conn.fetch(query, trace_id)

# APMé“¾æ¥ç”Ÿæˆ
def generate_apm_links(otel_trace_id: str) -> Dict[str, str]:
    """ç”ŸæˆAPMç³»ç»Ÿé“¾æ¥"""
    return {
        "jaeger": f"http://localhost:16686/trace/{otel_trace_id}",
        "zipkin": f"http://localhost:9411/zipkin/traces/{otel_trace_id}",
        "grafana": f"http://localhost:3000/explore?trace={otel_trace_id}"
    }
```

**OpenTelemetryé…ç½®ä¼˜åŒ–ï¼š**
```bash
# OpenTelemetryåˆ†å¸ƒå¼è¿½è¸ªé…ç½®
OTEL_ENABLED=true
OTEL_SERVICE_NAME=harborai-logging
OTEL_SERVICE_VERSION=2.0.0
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
OTEL_EXPORTER_OTLP_PROTOCOL=grpc
OTEL_EXPORTER_JAEGER_ENDPOINT=http://localhost:14268/api/traces
OTEL_RESOURCE_ATTRIBUTES=service.name=harborai-logging,service.version=2.0.0,ai.system=harborai

# æ‰¹å¤„ç†å™¨é…ç½®ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
OTEL_BSP_MAX_QUEUE_SIZE=2048
OTEL_BSP_MAX_EXPORT_BATCH_SIZE=512
OTEL_BSP_EXPORT_TIMEOUT=30000
OTEL_BSP_SCHEDULE_DELAY=5000

# é‡‡æ ·é…ç½®
OTEL_TRACES_SAMPLER=traceidratio
OTEL_TRACES_SAMPLER_ARG=1.0
```

**æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–ï¼š**
```sql
-- åˆ›å»ºåŒIDæŸ¥è¯¢å‡½æ•°
CREATE OR REPLACE FUNCTION get_log_by_any_trace_id(trace_id_input TEXT)
RETURNS TABLE (
    log_id UUID,
    hb_trace_id VARCHAR(100),
    otel_trace_id VARCHAR(32),
    span_id VARCHAR(16),
    provider TEXT,
    model TEXT,
    total_cost DECIMAL(10,6),
    currency VARCHAR(10),
    apm_jaeger_link TEXT,
    apm_zipkin_link TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        l.id as log_id,
        tr.hb_trace_id,
        tr.otel_trace_id,
        tr.span_id,
        l.provider,
        l.model,
        c.total_cost,
        c.currency,
        CONCAT('http://localhost:16686/trace/', tr.otel_trace_id) as apm_jaeger_link,
        CONCAT('http://localhost:9411/zipkin/traces/', tr.otel_trace_id) as apm_zipkin_link
    FROM api_logs l
    JOIN tracing_info tr ON l.id = tr.log_id
    LEFT JOIN cost_info c ON l.id = c.log_id
    WHERE tr.hb_trace_id = trace_id_input 
       OR tr.otel_trace_id = trace_id_input;
END;
$$ LANGUAGE plpgsql;

-- ä½¿ç”¨ç¤ºä¾‹
SELECT * FROM get_log_by_any_trace_id('4bf92f3577b34da6a3ce929d0e0e4736');
```

---

*æœ¬é‡æ„è®¾è®¡æ–¹æ¡ˆåŸºäºHarborAIç°æœ‰æŠ€æœ¯æ¶æ„è¿›è¡Œä¼˜åŒ–ï¼Œä¸“æ³¨äºæˆæœ¬ç®¡ç†ç²¾ç»†åŒ–ã€åˆ†å¸ƒå¼è¿½è¸ªé›†æˆå’Œç³»ç»Ÿå¯é æ€§æå‡ï¼Œç¡®ä¿å‘åå…¼å®¹æ€§å’Œå¹³æ»‘è¿ç§»ã€‚*

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv4.0*  
*åˆ›å»ºæ—¶é—´ï¼š2025-01-15*  
*æ›´æ–°æ—¶é—´ï¼š2025-01-15*  
*ä½œè€…ï¼šSOLO Document*