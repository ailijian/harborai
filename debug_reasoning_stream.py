#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨ç†æ¨¡å‹æµå¼è¾“å‡ºè°ƒè¯•è„šæœ¬
ç”¨äºæ£€æŸ¥æ¨ç†æ¨¡å‹çš„å®é™…APIå“åº”æ ¼å¼
"""

import os
import json
import httpx
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def test_deepseek_reasoning_stream():
    """æµ‹è¯•DeepSeekæ¨ç†æ¨¡å‹çš„æµå¼è¾“å‡º"""
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("âŒ DEEPSEEK_API_KEY æœªè®¾ç½®")
        return
    
    print("ğŸ” æµ‹è¯•DeepSeekæ¨ç†æ¨¡å‹æµå¼è¾“å‡º...")
    
    # å‡†å¤‡è¯·æ±‚
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": "deepseek-reasoner",
        "messages": [
            {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
        ],
        "stream": True,
        "max_tokens": 100
    }
    
    try:
        with httpx.stream(
            "POST",
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30.0
        ) as response:
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å¤´: {dict(response.headers)}")
            
            if response.status_code != 200:
                print(f"é”™è¯¯å“åº”: {response.text}")
                return
            
            chunk_count = 0
            reasoning_chunks = []
            content_chunks = []
            
            for line in response.iter_lines():
                if line.startswith("data: "):
                    data_str = line[6:].strip()
                    if data_str == "[DONE]":
                        print("\nâœ… æµå¼å“åº”ç»“æŸ")
                        break
                    
                    try:
                        chunk_data = json.loads(data_str)
                        chunk_count += 1
                        
                        print(f"\n--- Chunk {chunk_count} ---")
                        print(f"å®Œæ•´chunkæ•°æ®: {json.dumps(chunk_data, ensure_ascii=False, indent=2)}")
                        
                        # æ£€æŸ¥choicesç»“æ„
                        if "choices" in chunk_data:
                            for i, choice in enumerate(chunk_data["choices"]):
                                print(f"Choice {i}:")
                                if "delta" in choice:
                                    delta = choice["delta"]
                                    print(f"  Delta: {json.dumps(delta, ensure_ascii=False, indent=4)}")
                                    
                                    # æ£€æŸ¥reasoning_content
                                    if "reasoning_content" in delta and delta["reasoning_content"]:
                                        reasoning_content = delta["reasoning_content"]
                                        reasoning_chunks.append(reasoning_content)
                                        print(f"  ğŸ§  æ¨ç†å†…å®¹: {reasoning_content[:100]}...")
                                    
                                    # æ£€æŸ¥content
                                    if "content" in delta and delta["content"]:
                                        content = delta["content"]
                                        content_chunks.append(content)
                                        print(f"  ğŸ’¬ å›ç­”å†…å®¹: {content[:100]}...")
                        
                        # é™åˆ¶è¾“å‡ºæ•°é‡
                        if chunk_count >= 10:
                            print("\nâš ï¸ å·²è¾“å‡ºå‰10ä¸ªchunkï¼Œåœæ­¢è°ƒè¯•")
                            break
                            
                    except json.JSONDecodeError as e:
                        print(f"JSONè§£æé”™è¯¯: {e}, åŸå§‹æ•°æ®: {data_str}")
                        continue
            
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"æ€»chunkæ•°é‡: {chunk_count}")
            print(f"æ¨ç†å†…å®¹chunks: {len(reasoning_chunks)}")
            print(f"å›ç­”å†…å®¹chunks: {len(content_chunks)}")
            
            if reasoning_chunks:
                print(f"\nğŸ§  å®Œæ•´æ¨ç†è¿‡ç¨‹:")
                print(''.join(reasoning_chunks))
            
            if content_chunks:
                print(f"\nğŸ’¬ å®Œæ•´å›ç­”å†…å®¹:")
                print(''.join(content_chunks))
                
    except Exception as e:
        print(f"è¯·æ±‚é”™è¯¯: {e}")

def test_wenxin_reasoning_stream():
    """æµ‹è¯•æ–‡å¿ƒä¸€è¨€æ¨ç†æ¨¡å‹çš„æµå¼è¾“å‡º"""
    api_key = os.getenv("WENXIN_API_KEY")
    if not api_key:
        print("âŒ WENXIN_API_KEY æœªè®¾ç½®")
        return
    
    print("\nğŸ” æµ‹è¯•æ–‡å¿ƒä¸€è¨€æ¨ç†æ¨¡å‹æµå¼è¾“å‡º...")
    
    # å‡†å¤‡è¯·æ±‚
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": "ernie-x1-turbo-32k",
        "messages": [
            {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
        ],
        "stream": True,
        "max_tokens": 100
    }
    
    try:
        with httpx.stream(
            "POST",
            "https://qianfan.baidubce.com/v2/chat/completions",
            headers=headers,
            json=data,
            timeout=30.0
        ) as response:
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å¤´: {dict(response.headers)}")
            
            if response.status_code != 200:
                print(f"é”™è¯¯å“åº”: {response.text}")
                return
            
            chunk_count = 0
            reasoning_chunks = []
            content_chunks = []
            
            for line in response.iter_lines():
                if line.startswith("data: "):
                    data_str = line[6:].strip()
                    if data_str == "[DONE]":
                        print("\nâœ… æµå¼å“åº”ç»“æŸ")
                        break
                    
                    try:
                        chunk_data = json.loads(data_str)
                        chunk_count += 1
                        
                        print(f"\n--- Chunk {chunk_count} ---")
                        print(f"å®Œæ•´chunkæ•°æ®: {json.dumps(chunk_data, ensure_ascii=False, indent=2)}")
                        
                        # æ£€æŸ¥choicesç»“æ„
                        if "choices" in chunk_data:
                            for i, choice in enumerate(chunk_data["choices"]):
                                print(f"Choice {i}:")
                                if "delta" in choice:
                                    delta = choice["delta"]
                                    print(f"  Delta: {json.dumps(delta, ensure_ascii=False, indent=4)}")
                                    
                                    # æ£€æŸ¥reasoning_content
                                    if "reasoning_content" in delta and delta["reasoning_content"]:
                                        reasoning_content = delta["reasoning_content"]
                                        reasoning_chunks.append(reasoning_content)
                                        print(f"  ğŸ§  æ¨ç†å†…å®¹: {reasoning_content[:100]}...")
                                    
                                    # æ£€æŸ¥content
                                    if "content" in delta and delta["content"]:
                                        content = delta["content"]
                                        content_chunks.append(content)
                                        print(f"  ğŸ’¬ å›ç­”å†…å®¹: {content[:100]}...")
                        
                        # é™åˆ¶è¾“å‡ºæ•°é‡
                        if chunk_count >= 10:
                            print("\nâš ï¸ å·²è¾“å‡ºå‰10ä¸ªchunkï¼Œåœæ­¢è°ƒè¯•")
                            break
                            
                    except json.JSONDecodeError as e:
                        print(f"JSONè§£æé”™è¯¯: {e}, åŸå§‹æ•°æ®: {data_str}")
                        continue
            
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"æ€»chunkæ•°é‡: {chunk_count}")
            print(f"æ¨ç†å†…å®¹chunks: {len(reasoning_chunks)}")
            print(f"å›ç­”å†…å®¹chunks: {len(content_chunks)}")
            
            if reasoning_chunks:
                print(f"\nğŸ§  å®Œæ•´æ¨ç†è¿‡ç¨‹:")
                print(''.join(reasoning_chunks))
            
            if content_chunks:
                print(f"\nğŸ’¬ å®Œæ•´å›ç­”å†…å®¹:")
                print(''.join(content_chunks))
                
    except Exception as e:
        print(f"è¯·æ±‚é”™è¯¯: {e}")

if __name__ == "__main__":
    print("ğŸ” è°ƒè¯•æ¨ç†æ¨¡å‹æµå¼è¾“å‡ºé—®é¢˜")
    print("æ£€æŸ¥ç¯å¢ƒå˜é‡...")
    
    deepseek_key = os.getenv("DEEPSEEK_API_KEY")
    wenxin_key = os.getenv("WENXIN_API_KEY")
    
    print(f"DEEPSEEK_API_KEY: {'âœ… å·²è®¾ç½®' if deepseek_key else 'âŒ æœªè®¾ç½®'}")
    print(f"WENXIN_API_KEY: {'âœ… å·²è®¾ç½®' if wenxin_key else 'âŒ æœªè®¾ç½®'}")
    
    if deepseek_key:
        test_deepseek_reasoning_stream()
    
    if wenxin_key:
        test_wenxin_reasoning_stream()
    
    print("\nğŸ” è°ƒè¯•å®Œæˆ")