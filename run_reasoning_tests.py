#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨ç†æ¨¡å‹åŠŸèƒ½å®Œæ•´æµ‹è¯•è„šæœ¬
é¿å…pytestç¯å¢ƒé—®é¢˜ï¼Œç›´æ¥è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
"""

import os
import sys
import traceback
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def setup_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    os.environ["DEEPSEEK_API_KEY"] = "test-key"
    os.environ["WENXIN_API_KEY"] = "test-key"
    os.environ["DOUBAO_API_KEY"] = "test-key"

def create_mock_client():
    """åˆ›å»ºæ¨¡æ‹Ÿå®¢æˆ·ç«¯"""
    from harborai import HarborAI
    
    client = HarborAI()
    
    # Mockåº•å±‚çš„client_managerè°ƒç”¨
    with patch.object(client.client_manager, 'chat_completion_sync_with_fallback') as mock_completion:
        # åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„Mockå“åº”å¯¹è±¡
        mock_response = Mock()
        mock_response.choices = [Mock(
            message=Mock(
                content="è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„æ¨ç†é—®é¢˜ï¼Œéœ€è¦ä»”ç»†åˆ†æ...",
                role="assistant"
            ),
            finish_reason="stop"
        )]
        mock_response.usage = Mock(
            prompt_tokens=100,
            completion_tokens=200,
            total_tokens=300
        )
        # æ·»åŠ model_dumpæ–¹æ³•
        mock_response.model_dump.return_value = {
            "choices": [{
                "message": {
                    "content": "è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„æ¨ç†é—®é¢˜ï¼Œéœ€è¦ä»”ç»†åˆ†æ...",
                    "role": "assistant"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 100,
                "completion_tokens": 200,
                "total_tokens": 300
            }
        }
        mock_completion.return_value = mock_response
        
        return client, mock_completion

def test_deepseek_reasoning_model_detection():
    """æµ‹è¯•DeepSeekæ¨ç†æ¨¡å‹æ£€æµ‹"""
    print("\n=== æµ‹è¯•DeepSeekæ¨ç†æ¨¡å‹æ£€æµ‹ ===")
    
    try:
        client, mock_completion = create_mock_client()
        
        reasoning_models = [
            "deepseek-r1",
            "deepseek-r1-lite",
            "deepseek-reasoner"
        ]
        
        for model_name in reasoning_models:
            print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
            
            # æ‰§è¡Œæ¨ç†æ¨¡å‹è¯·æ±‚
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "user", "content": "è¯·è§£å†³è¿™ä¸ªå¤æ‚çš„æ•°å­¦é—®é¢˜ï¼šè¯æ˜è´¹é©¬å¤§å®šç†"}
                ]
            )
            
            # éªŒè¯å“åº”
            assert response is not None
            assert response.choices[0].message.content is not None
            assert len(response.choices[0].message.content) > 0
            
            # éªŒè¯è°ƒç”¨å‚æ•°ï¼ˆæ¨ç†æ¨¡å‹åº”è¯¥ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°ï¼‰
            call_args = mock_completion.call_args
            call_kwargs = call_args[1] if len(call_args) > 1 else call_args[0]
            
            assert call_kwargs.get('model') == model_name
            
            # æ¨ç†æ¨¡å‹ä¸åº”è¯¥åŒ…å«è¿™äº›å‚æ•°
            unsupported_params = ['temperature', 'top_p', 'frequency_penalty', 'presence_penalty', 'stream']
            for param in unsupported_params:
                assert param not in call_kwargs, f"æ¨ç†æ¨¡å‹ {model_name} ä¸åº”è¯¥åŒ…å«å‚æ•° {param}"
            
            print(f"âœ“ {model_name} æ£€æµ‹æ­£å¸¸")
        
        print("âœ“ DeepSeekæ¨ç†æ¨¡å‹æ£€æµ‹æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— DeepSeekæ¨ç†æ¨¡å‹æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_non_reasoning_model_detection():
    """æµ‹è¯•éæ¨ç†æ¨¡å‹æ£€æµ‹"""
    print("\n=== æµ‹è¯•éæ¨ç†æ¨¡å‹æ£€æµ‹ ===")
    
    try:
        client, mock_completion = create_mock_client()
        
        regular_models = [
            "deepseek-chat",
            "ernie-3.5-8k",
            "ernie-4.0-turbo-8k",
            "doubao-pro-4k",
            "doubao-pro-32k"
        ]
        
        for model_name in regular_models:
            print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
            
            # æ‰§è¡Œå¸¸è§„æ¨¡å‹è¯·æ±‚ï¼ˆåŒ…å«æ¨ç†æ¨¡å‹ä¸æ”¯æŒçš„å‚æ•°ï¼‰
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "user", "content": "ä½ å¥½"}
                ],
                temperature=0.7,
                top_p=0.9,
                frequency_penalty=0.1,
                presence_penalty=0.1
            )
            
            # éªŒè¯å“åº”
            assert response is not None
            
            # éªŒè¯è°ƒç”¨å‚æ•°ï¼ˆå¸¸è§„æ¨¡å‹åº”è¯¥ä¿ç•™æ‰€æœ‰å‚æ•°ï¼‰
            call_args = mock_completion.call_args
            call_kwargs = call_args[1] if len(call_args) > 1 else call_args[0]
            
            assert call_kwargs.get('model') == model_name
            assert call_kwargs.get('temperature') == 0.7
            assert call_kwargs.get('top_p') == 0.9
            assert call_kwargs.get('frequency_penalty') == 0.1
            assert call_kwargs.get('presence_penalty') == 0.1
            
            print(f"âœ“ {model_name} æ£€æµ‹æ­£å¸¸")
        
        print("âœ“ éæ¨ç†æ¨¡å‹æ£€æµ‹æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— éæ¨ç†æ¨¡å‹æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_parameter_filtering_for_reasoning_models():
    """æµ‹è¯•æ¨ç†æ¨¡å‹çš„å‚æ•°è¿‡æ»¤"""
    print("\n=== æµ‹è¯•æ¨ç†æ¨¡å‹å‚æ•°è¿‡æ»¤ ===")
    
    try:
        client, mock_completion = create_mock_client()
        
        # å°è¯•ä½¿ç”¨æ¨ç†æ¨¡å‹ä¸æ”¯æŒçš„å‚æ•°
        response = client.chat.completions.create(
            model="deepseek-r1",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"},  # æ¨ç†æ¨¡å‹ä¸æ”¯æŒsystemæ¶ˆæ¯
                {"role": "user", "content": "è§£å†³è¿™ä¸ªé—®é¢˜"}
            ],
            temperature=0.7,        # æ¨ç†æ¨¡å‹ä¸æ”¯æŒ
            top_p=0.9,             # æ¨ç†æ¨¡å‹ä¸æ”¯æŒ
            frequency_penalty=0.1,  # æ¨ç†æ¨¡å‹ä¸æ”¯æŒ
            presence_penalty=0.1,   # æ¨ç†æ¨¡å‹ä¸æ”¯æŒ
            stream=True,           # æ¨ç†æ¨¡å‹ä¸æ”¯æŒ
            max_tokens=1000
        )
        
        # éªŒè¯å“åº”
        assert response is not None
        
        # æ£€æŸ¥ä¼ é€’ç»™client_managerçš„å‚æ•°
        call_args = mock_completion.call_args
        call_kwargs = call_args[1] if len(call_args) > 1 else call_args[0]
        
        # åº”è¯¥ä¿ç•™çš„å‚æ•°
        assert call_kwargs.get('model') == "deepseek-r1"
        assert call_kwargs.get('max_tokens') == 1000
        
        # åº”è¯¥è¢«è¿‡æ»¤çš„å‚æ•°
        filtered_params = ['temperature', 'top_p', 'frequency_penalty', 'presence_penalty', 'stream']
        for param in filtered_params:
            assert param not in call_kwargs, f"å‚æ•° {param} åº”è¯¥è¢«è¿‡æ»¤"
        
        # éªŒè¯systemæ¶ˆæ¯å¤„ç†
        messages = call_kwargs.get('messages', [])
        system_messages = []
        for msg in messages:
            if hasattr(msg, 'role'):
                if msg.role == 'system':
                    system_messages.append(msg)
            elif isinstance(msg, dict) and msg.get('role') == 'system':
                system_messages.append(msg)
        assert len(system_messages) == 0, "æ¨ç†æ¨¡å‹ä¸åº”è¯¥åŒ…å«systemæ¶ˆæ¯"
        
        # éªŒè¯systemæ¶ˆæ¯å†…å®¹è¢«åˆå¹¶åˆ°useræ¶ˆæ¯ä¸­
        user_messages = []
        for msg in messages:
            if hasattr(msg, 'role'):
                if msg.role == 'user':
                    user_messages.append(msg)
            elif isinstance(msg, dict) and msg.get('role') == 'user':
                user_messages.append(msg)
        
        assert len(user_messages) > 0, "åº”è¯¥è‡³å°‘æœ‰ä¸€ä¸ªuseræ¶ˆæ¯"
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªuseræ¶ˆæ¯æ˜¯å¦åŒ…å«åŸsystemæ¶ˆæ¯çš„å†…å®¹
        first_user_msg = user_messages[0]
        if hasattr(first_user_msg, 'content'):
            content = first_user_msg.content
        else:
            content = first_user_msg.get('content', '')
        
        assert "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹" in content, "systemæ¶ˆæ¯å†…å®¹åº”è¯¥è¢«åˆå¹¶åˆ°useræ¶ˆæ¯ä¸­"
        
        print("âœ“ æ¨ç†æ¨¡å‹å‚æ•°è¿‡æ»¤æµ‹è¯•é€šè¿‡")
        print(f"âœ“ ä¼ é€’ç»™client_managerçš„å‚æ•°: {list(call_kwargs.keys())}")
        print(f"âœ“ å¤„ç†åçš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ¨ç†æ¨¡å‹å‚æ•°è¿‡æ»¤æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_model_capabilities_detection():
    """æµ‹è¯•æ¨¡å‹èƒ½åŠ›æ£€æµ‹"""
    print("\n=== æµ‹è¯•æ¨¡å‹èƒ½åŠ›æ£€æµ‹ ===")
    
    try:
        from harborai.core.models import is_reasoning_model, filter_parameters_for_model
        
        # æµ‹è¯•æ¨ç†æ¨¡å‹æ£€æµ‹
        reasoning_models = ["deepseek-r1", "deepseek-r1-lite", "deepseek-reasoner"]
        for model in reasoning_models:
            assert is_reasoning_model(model) == True, f"{model} åº”è¯¥è¢«è¯†åˆ«ä¸ºæ¨ç†æ¨¡å‹"
            print(f"âœ“ {model} æ­£ç¡®è¯†åˆ«ä¸ºæ¨ç†æ¨¡å‹")
        
        # æµ‹è¯•éæ¨ç†æ¨¡å‹æ£€æµ‹
        regular_models = ["deepseek-chat", "ernie-3.5-8k", "doubao-pro-4k"]
        for model in regular_models:
            assert is_reasoning_model(model) == False, f"{model} ä¸åº”è¯¥è¢«è¯†åˆ«ä¸ºæ¨ç†æ¨¡å‹"
            print(f"âœ“ {model} æ­£ç¡®è¯†åˆ«ä¸ºå¸¸è§„æ¨¡å‹")
        
        # æµ‹è¯•å‚æ•°è¿‡æ»¤åŠŸèƒ½
        test_params = {
            "model": "deepseek-r1",
            "messages": [{"role": "user", "content": "test"}],
            "temperature": 0.7,
            "top_p": 0.9,
            "frequency_penalty": 0.1,
            "presence_penalty": 0.1,
            "stream": True,
            "max_tokens": 1000
        }
        
        filtered_params = filter_parameters_for_model("deepseek-r1", test_params)
        
        # æ£€æŸ¥è¿‡æ»¤ç»“æœ
        assert "model" in filtered_params
        assert "messages" in filtered_params
        assert "max_tokens" in filtered_params
        
        # è¿™äº›å‚æ•°åº”è¯¥è¢«è¿‡æ»¤æ‰
        filtered_out = ["temperature", "top_p", "frequency_penalty", "presence_penalty", "stream"]
        for param in filtered_out:
            assert param not in filtered_params, f"å‚æ•° {param} åº”è¯¥è¢«è¿‡æ»¤"
        
        print("âœ“ æ¨¡å‹èƒ½åŠ›æ£€æµ‹æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹èƒ½åŠ›æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹è¿è¡ŒHarborAIæ¨ç†æ¨¡å‹åŠŸèƒ½æµ‹è¯•...")
    print("=" * 60)
    
    setup_environment()
    
    tests = [
        ("æ¨¡å‹èƒ½åŠ›æ£€æµ‹", test_model_capabilities_detection),
        ("DeepSeekæ¨ç†æ¨¡å‹æ£€æµ‹", test_deepseek_reasoning_model_detection),
        ("éæ¨ç†æ¨¡å‹æ£€æµ‹", test_non_reasoning_model_detection),
        ("æ¨ç†æ¨¡å‹å‚æ•°è¿‡æ»¤", test_parameter_filtering_for_reasoning_models),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} - é€šè¿‡")
            else:
                failed += 1
                print(f"âŒ {test_name} - å¤±è´¥")
        except Exception as e:
            failed += 1
            print(f"âŒ {test_name} - å¼‚å¸¸: {e}")
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•æ€»ç»“: é€šè¿‡ {passed}/{len(tests)}, å¤±è´¥ {failed}/{len(tests)}")
    
    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print(f"âŒ æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥")
        return False

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)